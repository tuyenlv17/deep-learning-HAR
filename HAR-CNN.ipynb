{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAR CNN training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_score, \\\n",
    "    recall_score, confusion_matrix, classification_report, \\\n",
    "    accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label_vn = [u\"Cổ tay\",u\"Cổ chân\",u\"Bả vai\",u\"Xoay người\",u\"Xoay đầu gối\",u\"Đi bộ\",u\"Chạy\",u\"Đạp xe\",u\"Đánh răng\",u\"Rửa tay\",u\"Lau bàn\",u\"Nạo\",u\"Thái\",u\"Trộn\",u\"Lên cầu thang\",u\"Xuống cầu thang\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_sensor_acc\n",
      "x_watch_acc\n",
      "x_watch_gyr\n",
      "y_sensor_acc\n",
      "y_watch_acc\n",
      "y_watch_gyr\n",
      "z_sensor_acc\n",
      "z_watch_acc\n",
      "z_watch_gyr\n",
      "x_sensor_acc\n",
      "x_watch_acc\n",
      "x_watch_gyr\n",
      "y_sensor_acc\n",
      "y_watch_acc\n",
      "y_watch_gyr\n",
      "z_sensor_acc\n",
      "z_watch_acc\n",
      "z_watch_gyr\n"
     ]
    }
   ],
   "source": [
    "rootDatasetDir = \"./datasets/PTIT/normalized\"\n",
    "X_train, labels_train, list_ch_train = read_data(data_path=\"./datasets/PTIT/normalized\", split=\"train\") # train\n",
    "X_test, labels_test, list_ch_test = read_data(data_path=rootDatasetDir, split=\"test\") # test\n",
    "assert list_ch_train == list_ch_test, \"Mistmatch in channels!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize?\n",
    "X_train, X_test = standardize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_vld, lab_tr, lab_vld = train_test_split(X_train, labels_train, \n",
    "                                                stratify = labels_train, random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = one_hot(lab_tr)\n",
    "y_vld = one_hot(lab_vld)\n",
    "y_test = one_hot(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_channels 9\n"
     ]
    }
   ],
   "source": [
    "batch_size = 600       # Batch size\n",
    "seq_len = WINDOWN_SIZE          # Number of steps or window size\n",
    "learning_rate = 0.0001\n",
    "epochs = 2000\n",
    "\n",
    "n_classes = NUM_CLASS\n",
    "n_channels = NUM_CHANNEL\n",
    "print \"n_channels %d\" % n_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the graph\n",
    "Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "# Construct placeholders\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.float32, [None, seq_len, n_channels], name = 'inputs')\n",
    "    labels_ = tf.placeholder(tf.float32, [None, n_classes], name = 'labels')\n",
    "    keep_prob_ = tf.placeholder(tf.float32, name = 'keep')\n",
    "    learning_rate_ = tf.placeholder(tf.float32, name = 'learning_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Convolutional Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    # (batch, 128, 9) --> (batch, 64, 18)\n",
    "    conv1 = tf.layers.conv1d(inputs=inputs_, filters=18, kernel_size=2, strides=1, \n",
    "                             padding='same', activation = tf.nn.relu)\n",
    "    max_pool_1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=2, strides=2, padding='same')\n",
    "    \n",
    "    # (batch, 64, 18) --> (batch, 32, 36)\n",
    "    conv2 = tf.layers.conv1d(inputs=max_pool_1, filters=36, kernel_size=2, strides=1, \n",
    "                             padding='same', activation = tf.nn.relu)\n",
    "    max_pool_2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=2, strides=2, padding='same')\n",
    "    \n",
    "    # (batch, 32, 36) --> (batch, 16, 72)\n",
    "    conv3 = tf.layers.conv1d(inputs=max_pool_2, filters=72, kernel_size=2, strides=1, \n",
    "                             padding='same', activation = tf.nn.relu)\n",
    "    max_pool_3 = tf.layers.max_pooling1d(inputs=conv3, pool_size=2, strides=2, padding='same')\n",
    "    \n",
    "    # (batch, 16, 72) --> (batch, 8, 144)\n",
    "    conv4 = tf.layers.conv1d(inputs=max_pool_3, filters=144, kernel_size=2, strides=1, \n",
    "                             padding='same', activation = tf.nn.relu)\n",
    "    max_pool_4 = tf.layers.max_pooling1d(inputs=conv4, pool_size=2, strides=2, padding='same')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, flatten and pass to the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    # Flatten and add dropout\n",
    "    flat = tf.reshape(max_pool_4, (-1, 8*144))\n",
    "    flat = tf.nn.dropout(flat, keep_prob=keep_prob_)\n",
    "    \n",
    "    # Predictions\n",
    "    logits = tf.layers.dense(flat, n_classes)\n",
    "    \n",
    "    # Cost function and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate_).minimize(cost)\n",
    "    \n",
    "    # Accuracy\n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.path.exists('checkpoints-cnn') == False):\n",
    "    !mkdir checkpoints-cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 0/2000', 'Iteration: 5', 'Train loss: 2.858363', 'Train acc: 0.071667')\n",
      "('Epoch: 1/2000', 'Iteration: 10', 'Train loss: 2.820016', 'Train acc: 0.068333')\n",
      "('Epoch: 1/2000', 'Iteration: 10', 'Validation loss: 2.760466', 'Validation acc: 0.071667')\n",
      "('Epoch: 2/2000', 'Iteration: 15', 'Train loss: 2.767467', 'Train acc: 0.076667')\n",
      "('Epoch: 3/2000', 'Iteration: 20', 'Train loss: 2.712113', 'Train acc: 0.098333')\n",
      "('Epoch: 3/2000', 'Iteration: 20', 'Validation loss: 2.689046', 'Validation acc: 0.098333')\n",
      "('Epoch: 4/2000', 'Iteration: 25', 'Train loss: 2.713872', 'Train acc: 0.100000')\n",
      "('Epoch: 5/2000', 'Iteration: 30', 'Train loss: 2.670680', 'Train acc: 0.101667')\n",
      "('Epoch: 5/2000', 'Iteration: 30', 'Validation loss: 2.642451', 'Validation acc: 0.095000')\n",
      "('Epoch: 6/2000', 'Iteration: 35', 'Train loss: 2.668613', 'Train acc: 0.106667')\n",
      "('Epoch: 7/2000', 'Iteration: 40', 'Train loss: 2.640273', 'Train acc: 0.111667')\n",
      "('Epoch: 7/2000', 'Iteration: 40', 'Validation loss: 2.610223', 'Validation acc: 0.095000')\n",
      "('Epoch: 8/2000', 'Iteration: 45', 'Train loss: 2.610568', 'Train acc: 0.113333')\n",
      "('Epoch: 9/2000', 'Iteration: 50', 'Train loss: 2.623593', 'Train acc: 0.111667')\n",
      "('Epoch: 9/2000', 'Iteration: 50', 'Validation loss: 2.583467', 'Validation acc: 0.105000')\n",
      "('Epoch: 10/2000', 'Iteration: 55', 'Train loss: 2.589733', 'Train acc: 0.118333')\n",
      "('Epoch: 11/2000', 'Iteration: 60', 'Train loss: 2.580440', 'Train acc: 0.126667')\n",
      "('Epoch: 11/2000', 'Iteration: 60', 'Validation loss: 2.557479', 'Validation acc: 0.118333')\n",
      "('Epoch: 12/2000', 'Iteration: 65', 'Train loss: 2.564332', 'Train acc: 0.151667')\n",
      "('Epoch: 13/2000', 'Iteration: 70', 'Train loss: 2.562181', 'Train acc: 0.161667')\n",
      "('Epoch: 13/2000', 'Iteration: 70', 'Validation loss: 2.530174', 'Validation acc: 0.153333')\n",
      "('Epoch: 14/2000', 'Iteration: 75', 'Train loss: 2.518806', 'Train acc: 0.171667')\n",
      "('Epoch: 15/2000', 'Iteration: 80', 'Train loss: 2.517138', 'Train acc: 0.173333')\n",
      "('Epoch: 15/2000', 'Iteration: 80', 'Validation loss: 2.500309', 'Validation acc: 0.190000')\n",
      "('Epoch: 16/2000', 'Iteration: 85', 'Train loss: 2.485800', 'Train acc: 0.215000')\n",
      "('Epoch: 17/2000', 'Iteration: 90', 'Train loss: 2.487088', 'Train acc: 0.198333')\n",
      "('Epoch: 17/2000', 'Iteration: 90', 'Validation loss: 2.467384', 'Validation acc: 0.225000')\n",
      "('Epoch: 18/2000', 'Iteration: 95', 'Train loss: 2.450408', 'Train acc: 0.226667')\n",
      "('Epoch: 19/2000', 'Iteration: 100', 'Train loss: 2.453974', 'Train acc: 0.223333')\n",
      "('Epoch: 19/2000', 'Iteration: 100', 'Validation loss: 2.431080', 'Validation acc: 0.253333')\n",
      "('Epoch: 20/2000', 'Iteration: 105', 'Train loss: 2.407133', 'Train acc: 0.235000')\n",
      "('Epoch: 21/2000', 'Iteration: 110', 'Train loss: 2.375324', 'Train acc: 0.276667')\n",
      "('Epoch: 21/2000', 'Iteration: 110', 'Validation loss: 2.391289', 'Validation acc: 0.265000')\n",
      "('Epoch: 22/2000', 'Iteration: 115', 'Train loss: 2.371279', 'Train acc: 0.240000')\n",
      "('Epoch: 23/2000', 'Iteration: 120', 'Train loss: 2.340541', 'Train acc: 0.308333')\n",
      "('Epoch: 23/2000', 'Iteration: 120', 'Validation loss: 2.348190', 'Validation acc: 0.278333')\n",
      "('Epoch: 24/2000', 'Iteration: 125', 'Train loss: 2.318761', 'Train acc: 0.280000')\n",
      "('Epoch: 25/2000', 'Iteration: 130', 'Train loss: 2.281254', 'Train acc: 0.296667')\n",
      "('Epoch: 25/2000', 'Iteration: 130', 'Validation loss: 2.301947', 'Validation acc: 0.286667')\n",
      "('Epoch: 26/2000', 'Iteration: 135', 'Train loss: 2.270857', 'Train acc: 0.293333')\n",
      "('Epoch: 27/2000', 'Iteration: 140', 'Train loss: 2.226188', 'Train acc: 0.343333')\n",
      "('Epoch: 27/2000', 'Iteration: 140', 'Validation loss: 2.252011', 'Validation acc: 0.305000')\n",
      "('Epoch: 28/2000', 'Iteration: 145', 'Train loss: 2.213523', 'Train acc: 0.320000')\n",
      "('Epoch: 29/2000', 'Iteration: 150', 'Train loss: 2.187047', 'Train acc: 0.320000')\n",
      "('Epoch: 29/2000', 'Iteration: 150', 'Validation loss: 2.199172', 'Validation acc: 0.320000')\n",
      "('Epoch: 30/2000', 'Iteration: 155', 'Train loss: 2.175672', 'Train acc: 0.330000')\n",
      "('Epoch: 31/2000', 'Iteration: 160', 'Train loss: 2.156789', 'Train acc: 0.301667')\n",
      "('Epoch: 31/2000', 'Iteration: 160', 'Validation loss: 2.143947', 'Validation acc: 0.345000')\n",
      "('Epoch: 32/2000', 'Iteration: 165', 'Train loss: 2.120957', 'Train acc: 0.331667')\n",
      "('Epoch: 33/2000', 'Iteration: 170', 'Train loss: 2.093995', 'Train acc: 0.350000')\n",
      "('Epoch: 33/2000', 'Iteration: 170', 'Validation loss: 2.087519', 'Validation acc: 0.361667')\n",
      "('Epoch: 34/2000', 'Iteration: 175', 'Train loss: 2.064713', 'Train acc: 0.376667')\n",
      "('Epoch: 35/2000', 'Iteration: 180', 'Train loss: 2.027138', 'Train acc: 0.375000')\n",
      "('Epoch: 35/2000', 'Iteration: 180', 'Validation loss: 2.029678', 'Validation acc: 0.381667')\n",
      "('Epoch: 36/2000', 'Iteration: 185', 'Train loss: 2.005159', 'Train acc: 0.348333')\n",
      "('Epoch: 37/2000', 'Iteration: 190', 'Train loss: 1.973828', 'Train acc: 0.376667')\n",
      "('Epoch: 37/2000', 'Iteration: 190', 'Validation loss: 1.972764', 'Validation acc: 0.390000')\n",
      "('Epoch: 38/2000', 'Iteration: 195', 'Train loss: 1.951012', 'Train acc: 0.371667')\n",
      "('Epoch: 39/2000', 'Iteration: 200', 'Train loss: 1.921353', 'Train acc: 0.398333')\n",
      "('Epoch: 39/2000', 'Iteration: 200', 'Validation loss: 1.916668', 'Validation acc: 0.406667')\n",
      "('Epoch: 40/2000', 'Iteration: 205', 'Train loss: 1.882559', 'Train acc: 0.398333')\n",
      "('Epoch: 41/2000', 'Iteration: 210', 'Train loss: 1.856379', 'Train acc: 0.418333')\n",
      "('Epoch: 41/2000', 'Iteration: 210', 'Validation loss: 1.861795', 'Validation acc: 0.418333')\n",
      "('Epoch: 42/2000', 'Iteration: 215', 'Train loss: 1.836015', 'Train acc: 0.400000')\n",
      "('Epoch: 43/2000', 'Iteration: 220', 'Train loss: 1.808614', 'Train acc: 0.436667')\n",
      "('Epoch: 43/2000', 'Iteration: 220', 'Validation loss: 1.808231', 'Validation acc: 0.435000')\n",
      "('Epoch: 44/2000', 'Iteration: 225', 'Train loss: 1.775139', 'Train acc: 0.425000')\n",
      "('Epoch: 45/2000', 'Iteration: 230', 'Train loss: 1.736068', 'Train acc: 0.453333')\n",
      "('Epoch: 45/2000', 'Iteration: 230', 'Validation loss: 1.755417', 'Validation acc: 0.453333')\n",
      "('Epoch: 46/2000', 'Iteration: 235', 'Train loss: 1.762808', 'Train acc: 0.446667')\n",
      "('Epoch: 47/2000', 'Iteration: 240', 'Train loss: 1.731486', 'Train acc: 0.423333')\n",
      "('Epoch: 47/2000', 'Iteration: 240', 'Validation loss: 1.705512', 'Validation acc: 0.463333')\n",
      "('Epoch: 48/2000', 'Iteration: 245', 'Train loss: 1.690438', 'Train acc: 0.420000')\n",
      "('Epoch: 49/2000', 'Iteration: 250', 'Train loss: 1.688112', 'Train acc: 0.448333')\n",
      "('Epoch: 49/2000', 'Iteration: 250', 'Validation loss: 1.656894', 'Validation acc: 0.490000')\n",
      "('Epoch: 50/2000', 'Iteration: 255', 'Train loss: 1.649083', 'Train acc: 0.461667')\n",
      "('Epoch: 51/2000', 'Iteration: 260', 'Train loss: 1.641236', 'Train acc: 0.478333')\n",
      "('Epoch: 51/2000', 'Iteration: 260', 'Validation loss: 1.609603', 'Validation acc: 0.503333')\n",
      "('Epoch: 52/2000', 'Iteration: 265', 'Train loss: 1.613325', 'Train acc: 0.471667')\n",
      "('Epoch: 53/2000', 'Iteration: 270', 'Train loss: 1.606370', 'Train acc: 0.480000')\n",
      "('Epoch: 53/2000', 'Iteration: 270', 'Validation loss: 1.564778', 'Validation acc: 0.511667')\n",
      "('Epoch: 54/2000', 'Iteration: 275', 'Train loss: 1.568924', 'Train acc: 0.495000')\n",
      "('Epoch: 55/2000', 'Iteration: 280', 'Train loss: 1.537369', 'Train acc: 0.500000')\n",
      "('Epoch: 55/2000', 'Iteration: 280', 'Validation loss: 1.521446', 'Validation acc: 0.520000')\n",
      "('Epoch: 56/2000', 'Iteration: 285', 'Train loss: 1.551405', 'Train acc: 0.480000')\n",
      "('Epoch: 57/2000', 'Iteration: 290', 'Train loss: 1.508482', 'Train acc: 0.515000')\n",
      "('Epoch: 57/2000', 'Iteration: 290', 'Validation loss: 1.479376', 'Validation acc: 0.543333')\n",
      "('Epoch: 58/2000', 'Iteration: 295', 'Train loss: 1.503682', 'Train acc: 0.490000')\n",
      "('Epoch: 59/2000', 'Iteration: 300', 'Train loss: 1.504228', 'Train acc: 0.481667')\n",
      "('Epoch: 59/2000', 'Iteration: 300', 'Validation loss: 1.439627', 'Validation acc: 0.556667')\n",
      "('Epoch: 60/2000', 'Iteration: 305', 'Train loss: 1.452239', 'Train acc: 0.515000')\n",
      "('Epoch: 61/2000', 'Iteration: 310', 'Train loss: 1.433031', 'Train acc: 0.516667')\n",
      "('Epoch: 61/2000', 'Iteration: 310', 'Validation loss: 1.401608', 'Validation acc: 0.565000')\n",
      "('Epoch: 62/2000', 'Iteration: 315', 'Train loss: 1.455708', 'Train acc: 0.513333')\n",
      "('Epoch: 63/2000', 'Iteration: 320', 'Train loss: 1.387640', 'Train acc: 0.535000')\n",
      "('Epoch: 63/2000', 'Iteration: 320', 'Validation loss: 1.365070', 'Validation acc: 0.583333')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 64/2000', 'Iteration: 325', 'Train loss: 1.407504', 'Train acc: 0.553333')\n",
      "('Epoch: 65/2000', 'Iteration: 330', 'Train loss: 1.373470', 'Train acc: 0.541667')\n",
      "('Epoch: 65/2000', 'Iteration: 330', 'Validation loss: 1.328496', 'Validation acc: 0.600000')\n",
      "('Epoch: 66/2000', 'Iteration: 335', 'Train loss: 1.323758', 'Train acc: 0.566667')\n",
      "('Epoch: 67/2000', 'Iteration: 340', 'Train loss: 1.347815', 'Train acc: 0.555000')\n",
      "('Epoch: 67/2000', 'Iteration: 340', 'Validation loss: 1.293897', 'Validation acc: 0.605000')\n",
      "('Epoch: 68/2000', 'Iteration: 345', 'Train loss: 1.294908', 'Train acc: 0.571667')\n",
      "('Epoch: 69/2000', 'Iteration: 350', 'Train loss: 1.302216', 'Train acc: 0.568333')\n",
      "('Epoch: 69/2000', 'Iteration: 350', 'Validation loss: 1.260232', 'Validation acc: 0.620000')\n",
      "('Epoch: 70/2000', 'Iteration: 355', 'Train loss: 1.283090', 'Train acc: 0.571667')\n",
      "('Epoch: 71/2000', 'Iteration: 360', 'Train loss: 1.260313', 'Train acc: 0.583333')\n",
      "('Epoch: 71/2000', 'Iteration: 360', 'Validation loss: 1.227165', 'Validation acc: 0.635000')\n",
      "('Epoch: 72/2000', 'Iteration: 365', 'Train loss: 1.227781', 'Train acc: 0.588333')\n",
      "('Epoch: 73/2000', 'Iteration: 370', 'Train loss: 1.208473', 'Train acc: 0.610000')\n",
      "('Epoch: 73/2000', 'Iteration: 370', 'Validation loss: 1.198202', 'Validation acc: 0.653333')\n",
      "('Epoch: 74/2000', 'Iteration: 375', 'Train loss: 1.237982', 'Train acc: 0.581667')\n",
      "('Epoch: 75/2000', 'Iteration: 380', 'Train loss: 1.218383', 'Train acc: 0.578333')\n",
      "('Epoch: 75/2000', 'Iteration: 380', 'Validation loss: 1.166706', 'Validation acc: 0.663333')\n",
      "('Epoch: 76/2000', 'Iteration: 385', 'Train loss: 1.190801', 'Train acc: 0.595000')\n",
      "('Epoch: 77/2000', 'Iteration: 390', 'Train loss: 1.144310', 'Train acc: 0.638333')\n",
      "('Epoch: 77/2000', 'Iteration: 390', 'Validation loss: 1.137909', 'Validation acc: 0.675000')\n",
      "('Epoch: 78/2000', 'Iteration: 395', 'Train loss: 1.163792', 'Train acc: 0.615000')\n",
      "('Epoch: 79/2000', 'Iteration: 400', 'Train loss: 1.170506', 'Train acc: 0.606667')\n",
      "('Epoch: 79/2000', 'Iteration: 400', 'Validation loss: 1.108711', 'Validation acc: 0.686667')\n",
      "('Epoch: 80/2000', 'Iteration: 405', 'Train loss: 1.126679', 'Train acc: 0.608333')\n",
      "('Epoch: 81/2000', 'Iteration: 410', 'Train loss: 1.082826', 'Train acc: 0.618333')\n",
      "('Epoch: 81/2000', 'Iteration: 410', 'Validation loss: 1.083984', 'Validation acc: 0.691667')\n",
      "('Epoch: 82/2000', 'Iteration: 415', 'Train loss: 1.127516', 'Train acc: 0.636667')\n",
      "('Epoch: 83/2000', 'Iteration: 420', 'Train loss: 1.079234', 'Train acc: 0.646667')\n",
      "('Epoch: 83/2000', 'Iteration: 420', 'Validation loss: 1.057511', 'Validation acc: 0.696667')\n",
      "('Epoch: 84/2000', 'Iteration: 425', 'Train loss: 1.070748', 'Train acc: 0.638333')\n",
      "('Epoch: 85/2000', 'Iteration: 430', 'Train loss: 1.047428', 'Train acc: 0.670000')\n",
      "('Epoch: 85/2000', 'Iteration: 430', 'Validation loss: 1.031719', 'Validation acc: 0.703333')\n",
      "('Epoch: 86/2000', 'Iteration: 435', 'Train loss: 1.038199', 'Train acc: 0.668333')\n",
      "('Epoch: 87/2000', 'Iteration: 440', 'Train loss: 1.060524', 'Train acc: 0.646667')\n",
      "('Epoch: 87/2000', 'Iteration: 440', 'Validation loss: 1.007643', 'Validation acc: 0.711667')\n",
      "('Epoch: 88/2000', 'Iteration: 445', 'Train loss: 1.004233', 'Train acc: 0.681667')\n",
      "('Epoch: 89/2000', 'Iteration: 450', 'Train loss: 1.010163', 'Train acc: 0.658333')\n",
      "('Epoch: 89/2000', 'Iteration: 450', 'Validation loss: 0.983563', 'Validation acc: 0.718333')\n",
      "('Epoch: 90/2000', 'Iteration: 455', 'Train loss: 1.010756', 'Train acc: 0.686667')\n",
      "('Epoch: 91/2000', 'Iteration: 460', 'Train loss: 0.990360', 'Train acc: 0.670000')\n",
      "('Epoch: 91/2000', 'Iteration: 460', 'Validation loss: 0.963765', 'Validation acc: 0.720000')\n",
      "('Epoch: 92/2000', 'Iteration: 465', 'Train loss: 0.973081', 'Train acc: 0.688333')\n",
      "('Epoch: 93/2000', 'Iteration: 470', 'Train loss: 0.979394', 'Train acc: 0.673333')\n",
      "('Epoch: 93/2000', 'Iteration: 470', 'Validation loss: 0.941498', 'Validation acc: 0.721667')\n",
      "('Epoch: 94/2000', 'Iteration: 475', 'Train loss: 0.959476', 'Train acc: 0.690000')\n",
      "('Epoch: 95/2000', 'Iteration: 480', 'Train loss: 0.963360', 'Train acc: 0.691667')\n",
      "('Epoch: 95/2000', 'Iteration: 480', 'Validation loss: 0.923918', 'Validation acc: 0.730000')\n",
      "('Epoch: 96/2000', 'Iteration: 485', 'Train loss: 0.928229', 'Train acc: 0.700000')\n",
      "('Epoch: 97/2000', 'Iteration: 490', 'Train loss: 0.908271', 'Train acc: 0.725000')\n",
      "('Epoch: 97/2000', 'Iteration: 490', 'Validation loss: 0.904645', 'Validation acc: 0.736667')\n",
      "('Epoch: 98/2000', 'Iteration: 495', 'Train loss: 0.934572', 'Train acc: 0.690000')\n",
      "('Epoch: 99/2000', 'Iteration: 500', 'Train loss: 0.898124', 'Train acc: 0.691667')\n",
      "('Epoch: 99/2000', 'Iteration: 500', 'Validation loss: 0.887524', 'Validation acc: 0.738333')\n",
      "('Epoch: 100/2000', 'Iteration: 505', 'Train loss: 0.879661', 'Train acc: 0.728333')\n",
      "('Epoch: 101/2000', 'Iteration: 510', 'Train loss: 0.886833', 'Train acc: 0.705000')\n",
      "('Epoch: 101/2000', 'Iteration: 510', 'Validation loss: 0.871994', 'Validation acc: 0.741667')\n",
      "('Epoch: 102/2000', 'Iteration: 515', 'Train loss: 0.874770', 'Train acc: 0.718333')\n",
      "('Epoch: 103/2000', 'Iteration: 520', 'Train loss: 0.879694', 'Train acc: 0.723333')\n",
      "('Epoch: 103/2000', 'Iteration: 520', 'Validation loss: 0.855272', 'Validation acc: 0.750000')\n",
      "('Epoch: 104/2000', 'Iteration: 525', 'Train loss: 0.839233', 'Train acc: 0.748333')\n",
      "('Epoch: 105/2000', 'Iteration: 530', 'Train loss: 0.844324', 'Train acc: 0.730000')\n",
      "('Epoch: 105/2000', 'Iteration: 530', 'Validation loss: 0.841275', 'Validation acc: 0.753333')\n",
      "('Epoch: 106/2000', 'Iteration: 535', 'Train loss: 0.841866', 'Train acc: 0.715000')\n",
      "('Epoch: 107/2000', 'Iteration: 540', 'Train loss: 0.848228', 'Train acc: 0.726667')\n",
      "('Epoch: 107/2000', 'Iteration: 540', 'Validation loss: 0.825635', 'Validation acc: 0.758333')\n",
      "('Epoch: 108/2000', 'Iteration: 545', 'Train loss: 0.832116', 'Train acc: 0.720000')\n",
      "('Epoch: 109/2000', 'Iteration: 550', 'Train loss: 0.810965', 'Train acc: 0.741667')\n",
      "('Epoch: 109/2000', 'Iteration: 550', 'Validation loss: 0.811970', 'Validation acc: 0.765000')\n",
      "('Epoch: 110/2000', 'Iteration: 555', 'Train loss: 0.809597', 'Train acc: 0.733333')\n",
      "('Epoch: 111/2000', 'Iteration: 560', 'Train loss: 0.783202', 'Train acc: 0.741667')\n",
      "('Epoch: 111/2000', 'Iteration: 560', 'Validation loss: 0.799905', 'Validation acc: 0.771667')\n",
      "('Epoch: 112/2000', 'Iteration: 565', 'Train loss: 0.782750', 'Train acc: 0.763333')\n",
      "('Epoch: 113/2000', 'Iteration: 570', 'Train loss: 0.781926', 'Train acc: 0.758333')\n",
      "('Epoch: 113/2000', 'Iteration: 570', 'Validation loss: 0.786157', 'Validation acc: 0.773333')\n",
      "('Epoch: 114/2000', 'Iteration: 575', 'Train loss: 0.777501', 'Train acc: 0.738333')\n",
      "('Epoch: 115/2000', 'Iteration: 580', 'Train loss: 0.758744', 'Train acc: 0.745000')\n",
      "('Epoch: 115/2000', 'Iteration: 580', 'Validation loss: 0.773726', 'Validation acc: 0.780000')\n",
      "('Epoch: 116/2000', 'Iteration: 585', 'Train loss: 0.760730', 'Train acc: 0.771667')\n",
      "('Epoch: 117/2000', 'Iteration: 590', 'Train loss: 0.754250', 'Train acc: 0.773333')\n",
      "('Epoch: 117/2000', 'Iteration: 590', 'Validation loss: 0.763789', 'Validation acc: 0.780000')\n",
      "('Epoch: 118/2000', 'Iteration: 595', 'Train loss: 0.741020', 'Train acc: 0.760000')\n",
      "('Epoch: 119/2000', 'Iteration: 600', 'Train loss: 0.743969', 'Train acc: 0.766667')\n",
      "('Epoch: 119/2000', 'Iteration: 600', 'Validation loss: 0.752229', 'Validation acc: 0.781667')\n",
      "('Epoch: 120/2000', 'Iteration: 605', 'Train loss: 0.732571', 'Train acc: 0.780000')\n",
      "('Epoch: 121/2000', 'Iteration: 610', 'Train loss: 0.707139', 'Train acc: 0.783333')\n",
      "('Epoch: 121/2000', 'Iteration: 610', 'Validation loss: 0.740834', 'Validation acc: 0.786667')\n",
      "('Epoch: 122/2000', 'Iteration: 615', 'Train loss: 0.722317', 'Train acc: 0.760000')\n",
      "('Epoch: 123/2000', 'Iteration: 620', 'Train loss: 0.743986', 'Train acc: 0.753333')\n",
      "('Epoch: 123/2000', 'Iteration: 620', 'Validation loss: 0.731946', 'Validation acc: 0.785000')\n",
      "('Epoch: 124/2000', 'Iteration: 625', 'Train loss: 0.710112', 'Train acc: 0.781667')\n",
      "('Epoch: 125/2000', 'Iteration: 630', 'Train loss: 0.691260', 'Train acc: 0.776667')\n",
      "('Epoch: 125/2000', 'Iteration: 630', 'Validation loss: 0.721859', 'Validation acc: 0.785000')\n",
      "('Epoch: 126/2000', 'Iteration: 635', 'Train loss: 0.700311', 'Train acc: 0.778333')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 127/2000', 'Iteration: 640', 'Train loss: 0.686708', 'Train acc: 0.771667')\n",
      "('Epoch: 127/2000', 'Iteration: 640', 'Validation loss: 0.712298', 'Validation acc: 0.786667')\n",
      "('Epoch: 128/2000', 'Iteration: 645', 'Train loss: 0.692885', 'Train acc: 0.795000')\n",
      "('Epoch: 129/2000', 'Iteration: 650', 'Train loss: 0.686818', 'Train acc: 0.793333')\n",
      "('Epoch: 129/2000', 'Iteration: 650', 'Validation loss: 0.703948', 'Validation acc: 0.793333')\n",
      "('Epoch: 130/2000', 'Iteration: 655', 'Train loss: 0.658683', 'Train acc: 0.803333')\n",
      "('Epoch: 131/2000', 'Iteration: 660', 'Train loss: 0.683652', 'Train acc: 0.793333')\n",
      "('Epoch: 131/2000', 'Iteration: 660', 'Validation loss: 0.694215', 'Validation acc: 0.795000')\n",
      "('Epoch: 132/2000', 'Iteration: 665', 'Train loss: 0.649913', 'Train acc: 0.801667')\n",
      "('Epoch: 133/2000', 'Iteration: 670', 'Train loss: 0.650563', 'Train acc: 0.806667')\n",
      "('Epoch: 133/2000', 'Iteration: 670', 'Validation loss: 0.686871', 'Validation acc: 0.795000')\n",
      "('Epoch: 134/2000', 'Iteration: 675', 'Train loss: 0.632492', 'Train acc: 0.801667')\n",
      "('Epoch: 135/2000', 'Iteration: 680', 'Train loss: 0.641655', 'Train acc: 0.815000')\n",
      "('Epoch: 135/2000', 'Iteration: 680', 'Validation loss: 0.680491', 'Validation acc: 0.796667')\n",
      "('Epoch: 136/2000', 'Iteration: 685', 'Train loss: 0.649057', 'Train acc: 0.806667')\n",
      "('Epoch: 137/2000', 'Iteration: 690', 'Train loss: 0.634795', 'Train acc: 0.801667')\n",
      "('Epoch: 137/2000', 'Iteration: 690', 'Validation loss: 0.670742', 'Validation acc: 0.793333')\n",
      "('Epoch: 138/2000', 'Iteration: 695', 'Train loss: 0.649652', 'Train acc: 0.801667')\n",
      "('Epoch: 139/2000', 'Iteration: 700', 'Train loss: 0.614724', 'Train acc: 0.818333')\n",
      "('Epoch: 139/2000', 'Iteration: 700', 'Validation loss: 0.663534', 'Validation acc: 0.796667')\n",
      "('Epoch: 140/2000', 'Iteration: 705', 'Train loss: 0.619147', 'Train acc: 0.811667')\n",
      "('Epoch: 141/2000', 'Iteration: 710', 'Train loss: 0.591950', 'Train acc: 0.820000')\n",
      "('Epoch: 141/2000', 'Iteration: 710', 'Validation loss: 0.656505', 'Validation acc: 0.798333')\n",
      "('Epoch: 142/2000', 'Iteration: 715', 'Train loss: 0.596722', 'Train acc: 0.806667')\n",
      "('Epoch: 143/2000', 'Iteration: 720', 'Train loss: 0.597728', 'Train acc: 0.830000')\n",
      "('Epoch: 143/2000', 'Iteration: 720', 'Validation loss: 0.649071', 'Validation acc: 0.800000')\n",
      "('Epoch: 144/2000', 'Iteration: 725', 'Train loss: 0.617664', 'Train acc: 0.818333')\n",
      "('Epoch: 145/2000', 'Iteration: 730', 'Train loss: 0.610355', 'Train acc: 0.808333')\n",
      "('Epoch: 145/2000', 'Iteration: 730', 'Validation loss: 0.642905', 'Validation acc: 0.803333')\n",
      "('Epoch: 146/2000', 'Iteration: 735', 'Train loss: 0.581136', 'Train acc: 0.820000')\n",
      "('Epoch: 147/2000', 'Iteration: 740', 'Train loss: 0.578487', 'Train acc: 0.823333')\n",
      "('Epoch: 147/2000', 'Iteration: 740', 'Validation loss: 0.636051', 'Validation acc: 0.801667')\n",
      "('Epoch: 148/2000', 'Iteration: 745', 'Train loss: 0.596946', 'Train acc: 0.820000')\n",
      "('Epoch: 149/2000', 'Iteration: 750', 'Train loss: 0.580319', 'Train acc: 0.828333')\n",
      "('Epoch: 149/2000', 'Iteration: 750', 'Validation loss: 0.630173', 'Validation acc: 0.805000')\n",
      "('Epoch: 150/2000', 'Iteration: 755', 'Train loss: 0.582384', 'Train acc: 0.831667')\n",
      "('Epoch: 151/2000', 'Iteration: 760', 'Train loss: 0.566427', 'Train acc: 0.830000')\n",
      "('Epoch: 151/2000', 'Iteration: 760', 'Validation loss: 0.624063', 'Validation acc: 0.805000')\n",
      "('Epoch: 152/2000', 'Iteration: 765', 'Train loss: 0.555723', 'Train acc: 0.833333')\n",
      "('Epoch: 153/2000', 'Iteration: 770', 'Train loss: 0.541669', 'Train acc: 0.846667')\n",
      "('Epoch: 153/2000', 'Iteration: 770', 'Validation loss: 0.619568', 'Validation acc: 0.803333')\n",
      "('Epoch: 154/2000', 'Iteration: 775', 'Train loss: 0.559835', 'Train acc: 0.828333')\n",
      "('Epoch: 155/2000', 'Iteration: 780', 'Train loss: 0.563691', 'Train acc: 0.825000')\n",
      "('Epoch: 155/2000', 'Iteration: 780', 'Validation loss: 0.612459', 'Validation acc: 0.808333')\n",
      "('Epoch: 156/2000', 'Iteration: 785', 'Train loss: 0.546519', 'Train acc: 0.838333')\n",
      "('Epoch: 157/2000', 'Iteration: 790', 'Train loss: 0.523543', 'Train acc: 0.845000')\n",
      "('Epoch: 157/2000', 'Iteration: 790', 'Validation loss: 0.607195', 'Validation acc: 0.813333')\n",
      "('Epoch: 158/2000', 'Iteration: 795', 'Train loss: 0.534941', 'Train acc: 0.841667')\n",
      "('Epoch: 159/2000', 'Iteration: 800', 'Train loss: 0.537840', 'Train acc: 0.855000')\n",
      "('Epoch: 159/2000', 'Iteration: 800', 'Validation loss: 0.602510', 'Validation acc: 0.805000')\n",
      "('Epoch: 160/2000', 'Iteration: 805', 'Train loss: 0.541741', 'Train acc: 0.836667')\n",
      "('Epoch: 161/2000', 'Iteration: 810', 'Train loss: 0.548507', 'Train acc: 0.828333')\n",
      "('Epoch: 161/2000', 'Iteration: 810', 'Validation loss: 0.597598', 'Validation acc: 0.813333')\n",
      "('Epoch: 162/2000', 'Iteration: 815', 'Train loss: 0.539053', 'Train acc: 0.836667')\n",
      "('Epoch: 163/2000', 'Iteration: 820', 'Train loss: 0.507261', 'Train acc: 0.875000')\n",
      "('Epoch: 163/2000', 'Iteration: 820', 'Validation loss: 0.592153', 'Validation acc: 0.815000')\n",
      "('Epoch: 164/2000', 'Iteration: 825', 'Train loss: 0.517788', 'Train acc: 0.848333')\n",
      "('Epoch: 165/2000', 'Iteration: 830', 'Train loss: 0.533504', 'Train acc: 0.853333')\n",
      "('Epoch: 165/2000', 'Iteration: 830', 'Validation loss: 0.588354', 'Validation acc: 0.816667')\n",
      "('Epoch: 166/2000', 'Iteration: 835', 'Train loss: 0.517712', 'Train acc: 0.836667')\n",
      "('Epoch: 167/2000', 'Iteration: 840', 'Train loss: 0.500929', 'Train acc: 0.848333')\n",
      "('Epoch: 167/2000', 'Iteration: 840', 'Validation loss: 0.582704', 'Validation acc: 0.815000')\n",
      "('Epoch: 168/2000', 'Iteration: 845', 'Train loss: 0.492065', 'Train acc: 0.853333')\n",
      "('Epoch: 169/2000', 'Iteration: 850', 'Train loss: 0.522300', 'Train acc: 0.830000')\n",
      "('Epoch: 169/2000', 'Iteration: 850', 'Validation loss: 0.578690', 'Validation acc: 0.821667')\n",
      "('Epoch: 170/2000', 'Iteration: 855', 'Train loss: 0.539776', 'Train acc: 0.850000')\n",
      "('Epoch: 171/2000', 'Iteration: 860', 'Train loss: 0.496053', 'Train acc: 0.848333')\n",
      "('Epoch: 171/2000', 'Iteration: 860', 'Validation loss: 0.573807', 'Validation acc: 0.818333')\n",
      "('Epoch: 172/2000', 'Iteration: 865', 'Train loss: 0.486386', 'Train acc: 0.856667')\n",
      "('Epoch: 173/2000', 'Iteration: 870', 'Train loss: 0.491607', 'Train acc: 0.861667')\n",
      "('Epoch: 173/2000', 'Iteration: 870', 'Validation loss: 0.568702', 'Validation acc: 0.825000')\n",
      "('Epoch: 174/2000', 'Iteration: 875', 'Train loss: 0.476248', 'Train acc: 0.865000')\n",
      "('Epoch: 175/2000', 'Iteration: 880', 'Train loss: 0.502410', 'Train acc: 0.851667')\n",
      "('Epoch: 175/2000', 'Iteration: 880', 'Validation loss: 0.564052', 'Validation acc: 0.825000')\n",
      "('Epoch: 176/2000', 'Iteration: 885', 'Train loss: 0.497039', 'Train acc: 0.853333')\n",
      "('Epoch: 177/2000', 'Iteration: 890', 'Train loss: 0.480108', 'Train acc: 0.848333')\n",
      "('Epoch: 177/2000', 'Iteration: 890', 'Validation loss: 0.560969', 'Validation acc: 0.821667')\n",
      "('Epoch: 178/2000', 'Iteration: 895', 'Train loss: 0.473074', 'Train acc: 0.861667')\n",
      "('Epoch: 179/2000', 'Iteration: 900', 'Train loss: 0.473897', 'Train acc: 0.868333')\n",
      "('Epoch: 179/2000', 'Iteration: 900', 'Validation loss: 0.556450', 'Validation acc: 0.826667')\n",
      "('Epoch: 180/2000', 'Iteration: 905', 'Train loss: 0.468077', 'Train acc: 0.881667')\n",
      "('Epoch: 181/2000', 'Iteration: 910', 'Train loss: 0.473861', 'Train acc: 0.858333')\n",
      "('Epoch: 181/2000', 'Iteration: 910', 'Validation loss: 0.552844', 'Validation acc: 0.830000')\n",
      "('Epoch: 182/2000', 'Iteration: 915', 'Train loss: 0.463888', 'Train acc: 0.865000')\n",
      "('Epoch: 183/2000', 'Iteration: 920', 'Train loss: 0.453352', 'Train acc: 0.875000')\n",
      "('Epoch: 183/2000', 'Iteration: 920', 'Validation loss: 0.548361', 'Validation acc: 0.833333')\n",
      "('Epoch: 184/2000', 'Iteration: 925', 'Train loss: 0.448353', 'Train acc: 0.868333')\n",
      "('Epoch: 185/2000', 'Iteration: 930', 'Train loss: 0.475535', 'Train acc: 0.871667')\n",
      "('Epoch: 185/2000', 'Iteration: 930', 'Validation loss: 0.545539', 'Validation acc: 0.831667')\n",
      "('Epoch: 186/2000', 'Iteration: 935', 'Train loss: 0.461164', 'Train acc: 0.875000')\n",
      "('Epoch: 187/2000', 'Iteration: 940', 'Train loss: 0.454159', 'Train acc: 0.863333')\n",
      "('Epoch: 187/2000', 'Iteration: 940', 'Validation loss: 0.543020', 'Validation acc: 0.825000')\n",
      "('Epoch: 188/2000', 'Iteration: 945', 'Train loss: 0.465856', 'Train acc: 0.868333')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 189/2000', 'Iteration: 950', 'Train loss: 0.452441', 'Train acc: 0.870000')\n",
      "('Epoch: 189/2000', 'Iteration: 950', 'Validation loss: 0.540290', 'Validation acc: 0.835000')\n",
      "('Epoch: 190/2000', 'Iteration: 955', 'Train loss: 0.451448', 'Train acc: 0.858333')\n",
      "('Epoch: 191/2000', 'Iteration: 960', 'Train loss: 0.439700', 'Train acc: 0.881667')\n",
      "('Epoch: 191/2000', 'Iteration: 960', 'Validation loss: 0.535441', 'Validation acc: 0.831667')\n",
      "('Epoch: 192/2000', 'Iteration: 965', 'Train loss: 0.417836', 'Train acc: 0.886667')\n",
      "('Epoch: 193/2000', 'Iteration: 970', 'Train loss: 0.426814', 'Train acc: 0.878333')\n",
      "('Epoch: 193/2000', 'Iteration: 970', 'Validation loss: 0.531974', 'Validation acc: 0.836667')\n",
      "('Epoch: 194/2000', 'Iteration: 975', 'Train loss: 0.444752', 'Train acc: 0.866667')\n",
      "('Epoch: 195/2000', 'Iteration: 980', 'Train loss: 0.430450', 'Train acc: 0.863333')\n",
      "('Epoch: 195/2000', 'Iteration: 980', 'Validation loss: 0.531019', 'Validation acc: 0.831667')\n",
      "('Epoch: 196/2000', 'Iteration: 985', 'Train loss: 0.429528', 'Train acc: 0.870000')\n",
      "('Epoch: 197/2000', 'Iteration: 990', 'Train loss: 0.446364', 'Train acc: 0.873333')\n",
      "('Epoch: 197/2000', 'Iteration: 990', 'Validation loss: 0.527453', 'Validation acc: 0.836667')\n",
      "('Epoch: 198/2000', 'Iteration: 995', 'Train loss: 0.426840', 'Train acc: 0.885000')\n",
      "('Epoch: 199/2000', 'Iteration: 1000', 'Train loss: 0.430675', 'Train acc: 0.865000')\n",
      "('Epoch: 199/2000', 'Iteration: 1000', 'Validation loss: 0.522988', 'Validation acc: 0.833333')\n",
      "('Epoch: 200/2000', 'Iteration: 1005', 'Train loss: 0.432323', 'Train acc: 0.863333')\n",
      "('Epoch: 201/2000', 'Iteration: 1010', 'Train loss: 0.418132', 'Train acc: 0.865000')\n",
      "('Epoch: 201/2000', 'Iteration: 1010', 'Validation loss: 0.518959', 'Validation acc: 0.840000')\n",
      "('Epoch: 202/2000', 'Iteration: 1015', 'Train loss: 0.406678', 'Train acc: 0.883333')\n",
      "('Epoch: 203/2000', 'Iteration: 1020', 'Train loss: 0.423390', 'Train acc: 0.888333')\n",
      "('Epoch: 203/2000', 'Iteration: 1020', 'Validation loss: 0.515191', 'Validation acc: 0.833333')\n",
      "('Epoch: 204/2000', 'Iteration: 1025', 'Train loss: 0.403717', 'Train acc: 0.891667')\n",
      "('Epoch: 205/2000', 'Iteration: 1030', 'Train loss: 0.426867', 'Train acc: 0.880000')\n",
      "('Epoch: 205/2000', 'Iteration: 1030', 'Validation loss: 0.512600', 'Validation acc: 0.838333')\n",
      "('Epoch: 206/2000', 'Iteration: 1035', 'Train loss: 0.409325', 'Train acc: 0.878333')\n",
      "('Epoch: 207/2000', 'Iteration: 1040', 'Train loss: 0.399137', 'Train acc: 0.896667')\n",
      "('Epoch: 207/2000', 'Iteration: 1040', 'Validation loss: 0.511898', 'Validation acc: 0.830000')\n",
      "('Epoch: 208/2000', 'Iteration: 1045', 'Train loss: 0.394042', 'Train acc: 0.898333')\n",
      "('Epoch: 209/2000', 'Iteration: 1050', 'Train loss: 0.390047', 'Train acc: 0.886667')\n",
      "('Epoch: 209/2000', 'Iteration: 1050', 'Validation loss: 0.506233', 'Validation acc: 0.838333')\n",
      "('Epoch: 210/2000', 'Iteration: 1055', 'Train loss: 0.385656', 'Train acc: 0.901667')\n",
      "('Epoch: 211/2000', 'Iteration: 1060', 'Train loss: 0.417304', 'Train acc: 0.878333')\n",
      "('Epoch: 211/2000', 'Iteration: 1060', 'Validation loss: 0.504142', 'Validation acc: 0.840000')\n",
      "('Epoch: 212/2000', 'Iteration: 1065', 'Train loss: 0.401935', 'Train acc: 0.873333')\n",
      "('Epoch: 213/2000', 'Iteration: 1070', 'Train loss: 0.390318', 'Train acc: 0.886667')\n",
      "('Epoch: 213/2000', 'Iteration: 1070', 'Validation loss: 0.501426', 'Validation acc: 0.838333')\n",
      "('Epoch: 214/2000', 'Iteration: 1075', 'Train loss: 0.386720', 'Train acc: 0.888333')\n",
      "('Epoch: 215/2000', 'Iteration: 1080', 'Train loss: 0.353803', 'Train acc: 0.900000')\n",
      "('Epoch: 215/2000', 'Iteration: 1080', 'Validation loss: 0.498973', 'Validation acc: 0.838333')\n",
      "('Epoch: 216/2000', 'Iteration: 1085', 'Train loss: 0.401180', 'Train acc: 0.868333')\n",
      "('Epoch: 217/2000', 'Iteration: 1090', 'Train loss: 0.407899', 'Train acc: 0.890000')\n",
      "('Epoch: 217/2000', 'Iteration: 1090', 'Validation loss: 0.496375', 'Validation acc: 0.838333')\n",
      "('Epoch: 218/2000', 'Iteration: 1095', 'Train loss: 0.404042', 'Train acc: 0.883333')\n",
      "('Epoch: 219/2000', 'Iteration: 1100', 'Train loss: 0.378860', 'Train acc: 0.905000')\n",
      "('Epoch: 219/2000', 'Iteration: 1100', 'Validation loss: 0.494212', 'Validation acc: 0.840000')\n",
      "('Epoch: 220/2000', 'Iteration: 1105', 'Train loss: 0.370677', 'Train acc: 0.898333')\n",
      "('Epoch: 221/2000', 'Iteration: 1110', 'Train loss: 0.383968', 'Train acc: 0.888333')\n",
      "('Epoch: 221/2000', 'Iteration: 1110', 'Validation loss: 0.492214', 'Validation acc: 0.850000')\n",
      "('Epoch: 222/2000', 'Iteration: 1115', 'Train loss: 0.399965', 'Train acc: 0.880000')\n",
      "('Epoch: 223/2000', 'Iteration: 1120', 'Train loss: 0.374468', 'Train acc: 0.881667')\n",
      "('Epoch: 223/2000', 'Iteration: 1120', 'Validation loss: 0.491168', 'Validation acc: 0.843333')\n",
      "('Epoch: 224/2000', 'Iteration: 1125', 'Train loss: 0.370263', 'Train acc: 0.895000')\n",
      "('Epoch: 225/2000', 'Iteration: 1130', 'Train loss: 0.347715', 'Train acc: 0.903333')\n",
      "('Epoch: 225/2000', 'Iteration: 1130', 'Validation loss: 0.486853', 'Validation acc: 0.846667')\n",
      "('Epoch: 226/2000', 'Iteration: 1135', 'Train loss: 0.364639', 'Train acc: 0.885000')\n",
      "('Epoch: 227/2000', 'Iteration: 1140', 'Train loss: 0.388094', 'Train acc: 0.885000')\n",
      "('Epoch: 227/2000', 'Iteration: 1140', 'Validation loss: 0.483778', 'Validation acc: 0.848333')\n",
      "('Epoch: 228/2000', 'Iteration: 1145', 'Train loss: 0.351634', 'Train acc: 0.901667')\n",
      "('Epoch: 229/2000', 'Iteration: 1150', 'Train loss: 0.368641', 'Train acc: 0.895000')\n",
      "('Epoch: 229/2000', 'Iteration: 1150', 'Validation loss: 0.483455', 'Validation acc: 0.848333')\n",
      "('Epoch: 230/2000', 'Iteration: 1155', 'Train loss: 0.342971', 'Train acc: 0.901667')\n",
      "('Epoch: 231/2000', 'Iteration: 1160', 'Train loss: 0.355327', 'Train acc: 0.895000')\n",
      "('Epoch: 231/2000', 'Iteration: 1160', 'Validation loss: 0.479964', 'Validation acc: 0.853333')\n",
      "('Epoch: 232/2000', 'Iteration: 1165', 'Train loss: 0.339416', 'Train acc: 0.908333')\n",
      "('Epoch: 233/2000', 'Iteration: 1170', 'Train loss: 0.368644', 'Train acc: 0.898333')\n",
      "('Epoch: 233/2000', 'Iteration: 1170', 'Validation loss: 0.479626', 'Validation acc: 0.846667')\n",
      "('Epoch: 234/2000', 'Iteration: 1175', 'Train loss: 0.361039', 'Train acc: 0.898333')\n",
      "('Epoch: 235/2000', 'Iteration: 1180', 'Train loss: 0.359320', 'Train acc: 0.885000')\n",
      "('Epoch: 235/2000', 'Iteration: 1180', 'Validation loss: 0.476612', 'Validation acc: 0.853333')\n",
      "('Epoch: 236/2000', 'Iteration: 1185', 'Train loss: 0.343580', 'Train acc: 0.915000')\n",
      "('Epoch: 237/2000', 'Iteration: 1190', 'Train loss: 0.335785', 'Train acc: 0.901667')\n",
      "('Epoch: 237/2000', 'Iteration: 1190', 'Validation loss: 0.474509', 'Validation acc: 0.853333')\n",
      "('Epoch: 238/2000', 'Iteration: 1195', 'Train loss: 0.338925', 'Train acc: 0.911667')\n",
      "('Epoch: 239/2000', 'Iteration: 1200', 'Train loss: 0.346164', 'Train acc: 0.910000')\n",
      "('Epoch: 239/2000', 'Iteration: 1200', 'Validation loss: 0.474966', 'Validation acc: 0.846667')\n",
      "('Epoch: 240/2000', 'Iteration: 1205', 'Train loss: 0.349196', 'Train acc: 0.913333')\n",
      "('Epoch: 241/2000', 'Iteration: 1210', 'Train loss: 0.333458', 'Train acc: 0.895000')\n",
      "('Epoch: 241/2000', 'Iteration: 1210', 'Validation loss: 0.471006', 'Validation acc: 0.855000')\n",
      "('Epoch: 242/2000', 'Iteration: 1215', 'Train loss: 0.357150', 'Train acc: 0.890000')\n",
      "('Epoch: 243/2000', 'Iteration: 1220', 'Train loss: 0.358631', 'Train acc: 0.901667')\n",
      "('Epoch: 243/2000', 'Iteration: 1220', 'Validation loss: 0.469237', 'Validation acc: 0.860000')\n",
      "('Epoch: 244/2000', 'Iteration: 1225', 'Train loss: 0.345100', 'Train acc: 0.893333')\n",
      "('Epoch: 245/2000', 'Iteration: 1230', 'Train loss: 0.333855', 'Train acc: 0.893333')\n",
      "('Epoch: 245/2000', 'Iteration: 1230', 'Validation loss: 0.469624', 'Validation acc: 0.853333')\n",
      "('Epoch: 246/2000', 'Iteration: 1235', 'Train loss: 0.325689', 'Train acc: 0.916667')\n",
      "('Epoch: 247/2000', 'Iteration: 1240', 'Train loss: 0.353349', 'Train acc: 0.895000')\n",
      "('Epoch: 247/2000', 'Iteration: 1240', 'Validation loss: 0.465087', 'Validation acc: 0.861667')\n",
      "('Epoch: 248/2000', 'Iteration: 1245', 'Train loss: 0.308040', 'Train acc: 0.920000')\n",
      "('Epoch: 249/2000', 'Iteration: 1250', 'Train loss: 0.328275', 'Train acc: 0.915000')\n",
      "('Epoch: 249/2000', 'Iteration: 1250', 'Validation loss: 0.464477', 'Validation acc: 0.853333')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 250/2000', 'Iteration: 1255', 'Train loss: 0.327007', 'Train acc: 0.908333')\n",
      "('Epoch: 251/2000', 'Iteration: 1260', 'Train loss: 0.343761', 'Train acc: 0.900000')\n",
      "('Epoch: 251/2000', 'Iteration: 1260', 'Validation loss: 0.463211', 'Validation acc: 0.856667')\n",
      "('Epoch: 252/2000', 'Iteration: 1265', 'Train loss: 0.318062', 'Train acc: 0.913333')\n",
      "('Epoch: 253/2000', 'Iteration: 1270', 'Train loss: 0.315262', 'Train acc: 0.906667')\n",
      "('Epoch: 253/2000', 'Iteration: 1270', 'Validation loss: 0.460071', 'Validation acc: 0.860000')\n",
      "('Epoch: 254/2000', 'Iteration: 1275', 'Train loss: 0.308481', 'Train acc: 0.910000')\n",
      "('Epoch: 255/2000', 'Iteration: 1280', 'Train loss: 0.338272', 'Train acc: 0.896667')\n",
      "('Epoch: 255/2000', 'Iteration: 1280', 'Validation loss: 0.458967', 'Validation acc: 0.855000')\n",
      "('Epoch: 256/2000', 'Iteration: 1285', 'Train loss: 0.319261', 'Train acc: 0.901667')\n",
      "('Epoch: 257/2000', 'Iteration: 1290', 'Train loss: 0.332884', 'Train acc: 0.898333')\n",
      "('Epoch: 257/2000', 'Iteration: 1290', 'Validation loss: 0.455232', 'Validation acc: 0.863333')\n",
      "('Epoch: 258/2000', 'Iteration: 1295', 'Train loss: 0.342197', 'Train acc: 0.901667')\n",
      "('Epoch: 259/2000', 'Iteration: 1300', 'Train loss: 0.310332', 'Train acc: 0.910000')\n",
      "('Epoch: 259/2000', 'Iteration: 1300', 'Validation loss: 0.455847', 'Validation acc: 0.860000')\n",
      "('Epoch: 260/2000', 'Iteration: 1305', 'Train loss: 0.293598', 'Train acc: 0.915000')\n",
      "('Epoch: 261/2000', 'Iteration: 1310', 'Train loss: 0.314790', 'Train acc: 0.908333')\n",
      "('Epoch: 261/2000', 'Iteration: 1310', 'Validation loss: 0.453569', 'Validation acc: 0.858333')\n",
      "('Epoch: 262/2000', 'Iteration: 1315', 'Train loss: 0.301250', 'Train acc: 0.920000')\n",
      "('Epoch: 263/2000', 'Iteration: 1320', 'Train loss: 0.312328', 'Train acc: 0.916667')\n",
      "('Epoch: 263/2000', 'Iteration: 1320', 'Validation loss: 0.451519', 'Validation acc: 0.860000')\n",
      "('Epoch: 264/2000', 'Iteration: 1325', 'Train loss: 0.312983', 'Train acc: 0.898333')\n",
      "('Epoch: 265/2000', 'Iteration: 1330', 'Train loss: 0.306126', 'Train acc: 0.910000')\n",
      "('Epoch: 265/2000', 'Iteration: 1330', 'Validation loss: 0.450532', 'Validation acc: 0.860000')\n",
      "('Epoch: 266/2000', 'Iteration: 1335', 'Train loss: 0.291562', 'Train acc: 0.915000')\n",
      "('Epoch: 267/2000', 'Iteration: 1340', 'Train loss: 0.287504', 'Train acc: 0.918333')\n",
      "('Epoch: 267/2000', 'Iteration: 1340', 'Validation loss: 0.447757', 'Validation acc: 0.868333')\n",
      "('Epoch: 268/2000', 'Iteration: 1345', 'Train loss: 0.299100', 'Train acc: 0.915000')\n",
      "('Epoch: 269/2000', 'Iteration: 1350', 'Train loss: 0.311681', 'Train acc: 0.908333')\n",
      "('Epoch: 269/2000', 'Iteration: 1350', 'Validation loss: 0.444404', 'Validation acc: 0.863333')\n",
      "('Epoch: 270/2000', 'Iteration: 1355', 'Train loss: 0.297452', 'Train acc: 0.911667')\n",
      "('Epoch: 271/2000', 'Iteration: 1360', 'Train loss: 0.295933', 'Train acc: 0.908333')\n",
      "('Epoch: 271/2000', 'Iteration: 1360', 'Validation loss: 0.447431', 'Validation acc: 0.860000')\n",
      "('Epoch: 272/2000', 'Iteration: 1365', 'Train loss: 0.308793', 'Train acc: 0.908333')\n",
      "('Epoch: 273/2000', 'Iteration: 1370', 'Train loss: 0.303948', 'Train acc: 0.918333')\n",
      "('Epoch: 273/2000', 'Iteration: 1370', 'Validation loss: 0.443199', 'Validation acc: 0.861667')\n",
      "('Epoch: 274/2000', 'Iteration: 1375', 'Train loss: 0.296290', 'Train acc: 0.916667')\n",
      "('Epoch: 275/2000', 'Iteration: 1380', 'Train loss: 0.298468', 'Train acc: 0.911667')\n",
      "('Epoch: 275/2000', 'Iteration: 1380', 'Validation loss: 0.442644', 'Validation acc: 0.860000')\n",
      "('Epoch: 276/2000', 'Iteration: 1385', 'Train loss: 0.297826', 'Train acc: 0.911667')\n",
      "('Epoch: 277/2000', 'Iteration: 1390', 'Train loss: 0.268959', 'Train acc: 0.921667')\n",
      "('Epoch: 277/2000', 'Iteration: 1390', 'Validation loss: 0.440467', 'Validation acc: 0.868333')\n",
      "('Epoch: 278/2000', 'Iteration: 1395', 'Train loss: 0.308391', 'Train acc: 0.918333')\n",
      "('Epoch: 279/2000', 'Iteration: 1400', 'Train loss: 0.299121', 'Train acc: 0.900000')\n",
      "('Epoch: 279/2000', 'Iteration: 1400', 'Validation loss: 0.439346', 'Validation acc: 0.860000')\n",
      "('Epoch: 280/2000', 'Iteration: 1405', 'Train loss: 0.295935', 'Train acc: 0.913333')\n",
      "('Epoch: 281/2000', 'Iteration: 1410', 'Train loss: 0.263972', 'Train acc: 0.928333')\n",
      "('Epoch: 281/2000', 'Iteration: 1410', 'Validation loss: 0.441352', 'Validation acc: 0.868333')\n",
      "('Epoch: 282/2000', 'Iteration: 1415', 'Train loss: 0.254929', 'Train acc: 0.928333')\n",
      "('Epoch: 283/2000', 'Iteration: 1420', 'Train loss: 0.288491', 'Train acc: 0.923333')\n",
      "('Epoch: 283/2000', 'Iteration: 1420', 'Validation loss: 0.436659', 'Validation acc: 0.868333')\n",
      "('Epoch: 284/2000', 'Iteration: 1425', 'Train loss: 0.268383', 'Train acc: 0.915000')\n",
      "('Epoch: 285/2000', 'Iteration: 1430', 'Train loss: 0.252891', 'Train acc: 0.920000')\n",
      "('Epoch: 285/2000', 'Iteration: 1430', 'Validation loss: 0.434970', 'Validation acc: 0.865000')\n",
      "('Epoch: 286/2000', 'Iteration: 1435', 'Train loss: 0.270695', 'Train acc: 0.908333')\n",
      "('Epoch: 287/2000', 'Iteration: 1440', 'Train loss: 0.283893', 'Train acc: 0.918333')\n",
      "('Epoch: 287/2000', 'Iteration: 1440', 'Validation loss: 0.436040', 'Validation acc: 0.866667')\n",
      "('Epoch: 288/2000', 'Iteration: 1445', 'Train loss: 0.270742', 'Train acc: 0.926667')\n",
      "('Epoch: 289/2000', 'Iteration: 1450', 'Train loss: 0.286880', 'Train acc: 0.920000')\n",
      "('Epoch: 289/2000', 'Iteration: 1450', 'Validation loss: 0.434657', 'Validation acc: 0.870000')\n",
      "('Epoch: 290/2000', 'Iteration: 1455', 'Train loss: 0.261399', 'Train acc: 0.931667')\n",
      "('Epoch: 291/2000', 'Iteration: 1460', 'Train loss: 0.274199', 'Train acc: 0.913333')\n",
      "('Epoch: 291/2000', 'Iteration: 1460', 'Validation loss: 0.434052', 'Validation acc: 0.863333')\n",
      "('Epoch: 292/2000', 'Iteration: 1465', 'Train loss: 0.265855', 'Train acc: 0.926667')\n",
      "('Epoch: 293/2000', 'Iteration: 1470', 'Train loss: 0.277496', 'Train acc: 0.918333')\n",
      "('Epoch: 293/2000', 'Iteration: 1470', 'Validation loss: 0.431307', 'Validation acc: 0.866667')\n",
      "('Epoch: 294/2000', 'Iteration: 1475', 'Train loss: 0.275050', 'Train acc: 0.918333')\n",
      "('Epoch: 295/2000', 'Iteration: 1480', 'Train loss: 0.283822', 'Train acc: 0.921667')\n",
      "('Epoch: 295/2000', 'Iteration: 1480', 'Validation loss: 0.430734', 'Validation acc: 0.866667')\n",
      "('Epoch: 296/2000', 'Iteration: 1485', 'Train loss: 0.254564', 'Train acc: 0.938333')\n",
      "('Epoch: 297/2000', 'Iteration: 1490', 'Train loss: 0.260493', 'Train acc: 0.928333')\n",
      "('Epoch: 297/2000', 'Iteration: 1490', 'Validation loss: 0.428293', 'Validation acc: 0.870000')\n",
      "('Epoch: 298/2000', 'Iteration: 1495', 'Train loss: 0.256266', 'Train acc: 0.923333')\n",
      "('Epoch: 299/2000', 'Iteration: 1500', 'Train loss: 0.260001', 'Train acc: 0.931667')\n",
      "('Epoch: 299/2000', 'Iteration: 1500', 'Validation loss: 0.427458', 'Validation acc: 0.876667')\n",
      "('Epoch: 300/2000', 'Iteration: 1505', 'Train loss: 0.257818', 'Train acc: 0.926667')\n",
      "('Epoch: 301/2000', 'Iteration: 1510', 'Train loss: 0.271337', 'Train acc: 0.925000')\n",
      "('Epoch: 301/2000', 'Iteration: 1510', 'Validation loss: 0.425941', 'Validation acc: 0.873333')\n",
      "('Epoch: 302/2000', 'Iteration: 1515', 'Train loss: 0.264873', 'Train acc: 0.925000')\n",
      "('Epoch: 303/2000', 'Iteration: 1520', 'Train loss: 0.267032', 'Train acc: 0.925000')\n",
      "('Epoch: 303/2000', 'Iteration: 1520', 'Validation loss: 0.426964', 'Validation acc: 0.873333')\n",
      "('Epoch: 304/2000', 'Iteration: 1525', 'Train loss: 0.270059', 'Train acc: 0.923333')\n",
      "('Epoch: 305/2000', 'Iteration: 1530', 'Train loss: 0.255619', 'Train acc: 0.926667')\n",
      "('Epoch: 305/2000', 'Iteration: 1530', 'Validation loss: 0.425466', 'Validation acc: 0.873333')\n",
      "('Epoch: 306/2000', 'Iteration: 1535', 'Train loss: 0.250870', 'Train acc: 0.925000')\n",
      "('Epoch: 307/2000', 'Iteration: 1540', 'Train loss: 0.269031', 'Train acc: 0.926667')\n",
      "('Epoch: 307/2000', 'Iteration: 1540', 'Validation loss: 0.424191', 'Validation acc: 0.873333')\n",
      "('Epoch: 308/2000', 'Iteration: 1545', 'Train loss: 0.258754', 'Train acc: 0.923333')\n",
      "('Epoch: 309/2000', 'Iteration: 1550', 'Train loss: 0.249566', 'Train acc: 0.916667')\n",
      "('Epoch: 309/2000', 'Iteration: 1550', 'Validation loss: 0.423870', 'Validation acc: 0.870000')\n",
      "('Epoch: 310/2000', 'Iteration: 1555', 'Train loss: 0.253737', 'Train acc: 0.920000')\n",
      "('Epoch: 311/2000', 'Iteration: 1560', 'Train loss: 0.242146', 'Train acc: 0.938333')\n",
      "('Epoch: 311/2000', 'Iteration: 1560', 'Validation loss: 0.424025', 'Validation acc: 0.876667')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 312/2000', 'Iteration: 1565', 'Train loss: 0.243644', 'Train acc: 0.936667')\n",
      "('Epoch: 313/2000', 'Iteration: 1570', 'Train loss: 0.249858', 'Train acc: 0.926667')\n",
      "('Epoch: 313/2000', 'Iteration: 1570', 'Validation loss: 0.422063', 'Validation acc: 0.881667')\n",
      "('Epoch: 314/2000', 'Iteration: 1575', 'Train loss: 0.256333', 'Train acc: 0.911667')\n",
      "('Epoch: 315/2000', 'Iteration: 1580', 'Train loss: 0.248576', 'Train acc: 0.928333')\n",
      "('Epoch: 315/2000', 'Iteration: 1580', 'Validation loss: 0.419712', 'Validation acc: 0.876667')\n",
      "('Epoch: 316/2000', 'Iteration: 1585', 'Train loss: 0.268362', 'Train acc: 0.916667')\n",
      "('Epoch: 317/2000', 'Iteration: 1590', 'Train loss: 0.248459', 'Train acc: 0.921667')\n",
      "('Epoch: 317/2000', 'Iteration: 1590', 'Validation loss: 0.419039', 'Validation acc: 0.878333')\n",
      "('Epoch: 318/2000', 'Iteration: 1595', 'Train loss: 0.245047', 'Train acc: 0.926667')\n",
      "('Epoch: 319/2000', 'Iteration: 1600', 'Train loss: 0.247274', 'Train acc: 0.921667')\n",
      "('Epoch: 319/2000', 'Iteration: 1600', 'Validation loss: 0.416720', 'Validation acc: 0.883333')\n",
      "('Epoch: 320/2000', 'Iteration: 1605', 'Train loss: 0.234828', 'Train acc: 0.938333')\n",
      "('Epoch: 321/2000', 'Iteration: 1610', 'Train loss: 0.224615', 'Train acc: 0.945000')\n",
      "('Epoch: 321/2000', 'Iteration: 1610', 'Validation loss: 0.417748', 'Validation acc: 0.875000')\n",
      "('Epoch: 322/2000', 'Iteration: 1615', 'Train loss: 0.258005', 'Train acc: 0.925000')\n",
      "('Epoch: 323/2000', 'Iteration: 1620', 'Train loss: 0.241399', 'Train acc: 0.930000')\n",
      "('Epoch: 323/2000', 'Iteration: 1620', 'Validation loss: 0.415508', 'Validation acc: 0.876667')\n",
      "('Epoch: 324/2000', 'Iteration: 1625', 'Train loss: 0.223517', 'Train acc: 0.936667')\n",
      "('Epoch: 325/2000', 'Iteration: 1630', 'Train loss: 0.241227', 'Train acc: 0.935000')\n",
      "('Epoch: 325/2000', 'Iteration: 1630', 'Validation loss: 0.414052', 'Validation acc: 0.880000')\n",
      "('Epoch: 326/2000', 'Iteration: 1635', 'Train loss: 0.227954', 'Train acc: 0.943333')\n",
      "('Epoch: 327/2000', 'Iteration: 1640', 'Train loss: 0.232936', 'Train acc: 0.936667')\n",
      "('Epoch: 327/2000', 'Iteration: 1640', 'Validation loss: 0.413382', 'Validation acc: 0.880000')\n",
      "('Epoch: 328/2000', 'Iteration: 1645', 'Train loss: 0.231625', 'Train acc: 0.935000')\n",
      "('Epoch: 329/2000', 'Iteration: 1650', 'Train loss: 0.248770', 'Train acc: 0.930000')\n",
      "('Epoch: 329/2000', 'Iteration: 1650', 'Validation loss: 0.412980', 'Validation acc: 0.878333')\n",
      "('Epoch: 330/2000', 'Iteration: 1655', 'Train loss: 0.231960', 'Train acc: 0.933333')\n",
      "('Epoch: 331/2000', 'Iteration: 1660', 'Train loss: 0.231791', 'Train acc: 0.935000')\n",
      "('Epoch: 331/2000', 'Iteration: 1660', 'Validation loss: 0.409360', 'Validation acc: 0.876667')\n",
      "('Epoch: 332/2000', 'Iteration: 1665', 'Train loss: 0.228683', 'Train acc: 0.941667')\n",
      "('Epoch: 333/2000', 'Iteration: 1670', 'Train loss: 0.240420', 'Train acc: 0.921667')\n",
      "('Epoch: 333/2000', 'Iteration: 1670', 'Validation loss: 0.413126', 'Validation acc: 0.876667')\n",
      "('Epoch: 334/2000', 'Iteration: 1675', 'Train loss: 0.232514', 'Train acc: 0.935000')\n",
      "('Epoch: 335/2000', 'Iteration: 1680', 'Train loss: 0.238041', 'Train acc: 0.925000')\n",
      "('Epoch: 335/2000', 'Iteration: 1680', 'Validation loss: 0.414632', 'Validation acc: 0.880000')\n",
      "('Epoch: 336/2000', 'Iteration: 1685', 'Train loss: 0.221395', 'Train acc: 0.938333')\n",
      "('Epoch: 337/2000', 'Iteration: 1690', 'Train loss: 0.236893', 'Train acc: 0.926667')\n",
      "('Epoch: 337/2000', 'Iteration: 1690', 'Validation loss: 0.411475', 'Validation acc: 0.883333')\n",
      "('Epoch: 338/2000', 'Iteration: 1695', 'Train loss: 0.216138', 'Train acc: 0.945000')\n",
      "('Epoch: 339/2000', 'Iteration: 1700', 'Train loss: 0.217128', 'Train acc: 0.943333')\n",
      "('Epoch: 339/2000', 'Iteration: 1700', 'Validation loss: 0.409287', 'Validation acc: 0.883333')\n",
      "('Epoch: 340/2000', 'Iteration: 1705', 'Train loss: 0.238981', 'Train acc: 0.943333')\n",
      "('Epoch: 341/2000', 'Iteration: 1710', 'Train loss: 0.237605', 'Train acc: 0.926667')\n",
      "('Epoch: 341/2000', 'Iteration: 1710', 'Validation loss: 0.407844', 'Validation acc: 0.883333')\n",
      "('Epoch: 342/2000', 'Iteration: 1715', 'Train loss: 0.220225', 'Train acc: 0.946667')\n",
      "('Epoch: 343/2000', 'Iteration: 1720', 'Train loss: 0.227665', 'Train acc: 0.928333')\n",
      "('Epoch: 343/2000', 'Iteration: 1720', 'Validation loss: 0.405698', 'Validation acc: 0.883333')\n",
      "('Epoch: 344/2000', 'Iteration: 1725', 'Train loss: 0.218968', 'Train acc: 0.935000')\n",
      "('Epoch: 345/2000', 'Iteration: 1730', 'Train loss: 0.238685', 'Train acc: 0.913333')\n",
      "('Epoch: 345/2000', 'Iteration: 1730', 'Validation loss: 0.406327', 'Validation acc: 0.883333')\n",
      "('Epoch: 346/2000', 'Iteration: 1735', 'Train loss: 0.213557', 'Train acc: 0.940000')\n",
      "('Epoch: 347/2000', 'Iteration: 1740', 'Train loss: 0.220208', 'Train acc: 0.936667')\n",
      "('Epoch: 347/2000', 'Iteration: 1740', 'Validation loss: 0.406355', 'Validation acc: 0.883333')\n",
      "('Epoch: 348/2000', 'Iteration: 1745', 'Train loss: 0.231386', 'Train acc: 0.941667')\n",
      "('Epoch: 349/2000', 'Iteration: 1750', 'Train loss: 0.213951', 'Train acc: 0.945000')\n",
      "('Epoch: 349/2000', 'Iteration: 1750', 'Validation loss: 0.405054', 'Validation acc: 0.883333')\n",
      "('Epoch: 350/2000', 'Iteration: 1755', 'Train loss: 0.215053', 'Train acc: 0.931667')\n",
      "('Epoch: 351/2000', 'Iteration: 1760', 'Train loss: 0.201356', 'Train acc: 0.950000')\n",
      "('Epoch: 351/2000', 'Iteration: 1760', 'Validation loss: 0.402747', 'Validation acc: 0.888333')\n",
      "('Epoch: 352/2000', 'Iteration: 1765', 'Train loss: 0.216263', 'Train acc: 0.931667')\n",
      "('Epoch: 353/2000', 'Iteration: 1770', 'Train loss: 0.201202', 'Train acc: 0.943333')\n",
      "('Epoch: 353/2000', 'Iteration: 1770', 'Validation loss: 0.406756', 'Validation acc: 0.885000')\n",
      "('Epoch: 354/2000', 'Iteration: 1775', 'Train loss: 0.217729', 'Train acc: 0.943333')\n",
      "('Epoch: 355/2000', 'Iteration: 1780', 'Train loss: 0.197614', 'Train acc: 0.943333')\n",
      "('Epoch: 355/2000', 'Iteration: 1780', 'Validation loss: 0.403175', 'Validation acc: 0.885000')\n",
      "('Epoch: 356/2000', 'Iteration: 1785', 'Train loss: 0.202526', 'Train acc: 0.940000')\n",
      "('Epoch: 357/2000', 'Iteration: 1790', 'Train loss: 0.207155', 'Train acc: 0.946667')\n",
      "('Epoch: 357/2000', 'Iteration: 1790', 'Validation loss: 0.399792', 'Validation acc: 0.883333')\n",
      "('Epoch: 358/2000', 'Iteration: 1795', 'Train loss: 0.216346', 'Train acc: 0.940000')\n",
      "('Epoch: 359/2000', 'Iteration: 1800', 'Train loss: 0.230736', 'Train acc: 0.943333')\n",
      "('Epoch: 359/2000', 'Iteration: 1800', 'Validation loss: 0.399842', 'Validation acc: 0.891667')\n",
      "('Epoch: 360/2000', 'Iteration: 1805', 'Train loss: 0.206497', 'Train acc: 0.936667')\n",
      "('Epoch: 361/2000', 'Iteration: 1810', 'Train loss: 0.194960', 'Train acc: 0.945000')\n",
      "('Epoch: 361/2000', 'Iteration: 1810', 'Validation loss: 0.401337', 'Validation acc: 0.886667')\n",
      "('Epoch: 362/2000', 'Iteration: 1815', 'Train loss: 0.211510', 'Train acc: 0.940000')\n",
      "('Epoch: 363/2000', 'Iteration: 1820', 'Train loss: 0.207349', 'Train acc: 0.945000')\n",
      "('Epoch: 363/2000', 'Iteration: 1820', 'Validation loss: 0.401102', 'Validation acc: 0.891667')\n",
      "('Epoch: 364/2000', 'Iteration: 1825', 'Train loss: 0.212745', 'Train acc: 0.936667')\n",
      "('Epoch: 365/2000', 'Iteration: 1830', 'Train loss: 0.213943', 'Train acc: 0.940000')\n",
      "('Epoch: 365/2000', 'Iteration: 1830', 'Validation loss: 0.404298', 'Validation acc: 0.881667')\n",
      "('Epoch: 366/2000', 'Iteration: 1835', 'Train loss: 0.217494', 'Train acc: 0.938333')\n",
      "('Epoch: 367/2000', 'Iteration: 1840', 'Train loss: 0.215002', 'Train acc: 0.928333')\n",
      "('Epoch: 367/2000', 'Iteration: 1840', 'Validation loss: 0.399317', 'Validation acc: 0.885000')\n",
      "('Epoch: 368/2000', 'Iteration: 1845', 'Train loss: 0.187892', 'Train acc: 0.951667')\n",
      "('Epoch: 369/2000', 'Iteration: 1850', 'Train loss: 0.216329', 'Train acc: 0.938333')\n",
      "('Epoch: 369/2000', 'Iteration: 1850', 'Validation loss: 0.399339', 'Validation acc: 0.881667')\n",
      "('Epoch: 370/2000', 'Iteration: 1855', 'Train loss: 0.197825', 'Train acc: 0.940000')\n",
      "('Epoch: 371/2000', 'Iteration: 1860', 'Train loss: 0.200548', 'Train acc: 0.948333')\n",
      "('Epoch: 371/2000', 'Iteration: 1860', 'Validation loss: 0.401669', 'Validation acc: 0.886667')\n",
      "('Epoch: 372/2000', 'Iteration: 1865', 'Train loss: 0.179333', 'Train acc: 0.950000')\n",
      "('Epoch: 373/2000', 'Iteration: 1870', 'Train loss: 0.184083', 'Train acc: 0.948333')\n",
      "('Epoch: 373/2000', 'Iteration: 1870', 'Validation loss: 0.397834', 'Validation acc: 0.888333')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 374/2000', 'Iteration: 1875', 'Train loss: 0.200249', 'Train acc: 0.943333')\n",
      "('Epoch: 375/2000', 'Iteration: 1880', 'Train loss: 0.184333', 'Train acc: 0.943333')\n",
      "('Epoch: 375/2000', 'Iteration: 1880', 'Validation loss: 0.397358', 'Validation acc: 0.886667')\n",
      "('Epoch: 376/2000', 'Iteration: 1885', 'Train loss: 0.197252', 'Train acc: 0.943333')\n",
      "('Epoch: 377/2000', 'Iteration: 1890', 'Train loss: 0.172310', 'Train acc: 0.956667')\n",
      "('Epoch: 377/2000', 'Iteration: 1890', 'Validation loss: 0.397224', 'Validation acc: 0.888333')\n",
      "('Epoch: 378/2000', 'Iteration: 1895', 'Train loss: 0.194840', 'Train acc: 0.946667')\n",
      "('Epoch: 379/2000', 'Iteration: 1900', 'Train loss: 0.193815', 'Train acc: 0.938333')\n",
      "('Epoch: 379/2000', 'Iteration: 1900', 'Validation loss: 0.398134', 'Validation acc: 0.881667')\n",
      "('Epoch: 380/2000', 'Iteration: 1905', 'Train loss: 0.192335', 'Train acc: 0.936667')\n",
      "('Epoch: 381/2000', 'Iteration: 1910', 'Train loss: 0.175599', 'Train acc: 0.948333')\n",
      "('Epoch: 381/2000', 'Iteration: 1910', 'Validation loss: 0.395288', 'Validation acc: 0.893333')\n",
      "('Epoch: 382/2000', 'Iteration: 1915', 'Train loss: 0.198046', 'Train acc: 0.946667')\n",
      "('Epoch: 383/2000', 'Iteration: 1920', 'Train loss: 0.175559', 'Train acc: 0.953333')\n",
      "('Epoch: 383/2000', 'Iteration: 1920', 'Validation loss: 0.398492', 'Validation acc: 0.891667')\n",
      "('Epoch: 384/2000', 'Iteration: 1925', 'Train loss: 0.181123', 'Train acc: 0.943333')\n",
      "('Epoch: 385/2000', 'Iteration: 1930', 'Train loss: 0.183586', 'Train acc: 0.941667')\n",
      "('Epoch: 385/2000', 'Iteration: 1930', 'Validation loss: 0.394733', 'Validation acc: 0.886667')\n",
      "('Epoch: 386/2000', 'Iteration: 1935', 'Train loss: 0.200994', 'Train acc: 0.935000')\n",
      "('Epoch: 387/2000', 'Iteration: 1940', 'Train loss: 0.201701', 'Train acc: 0.946667')\n",
      "('Epoch: 387/2000', 'Iteration: 1940', 'Validation loss: 0.399179', 'Validation acc: 0.886667')\n",
      "('Epoch: 388/2000', 'Iteration: 1945', 'Train loss: 0.196846', 'Train acc: 0.943333')\n",
      "('Epoch: 389/2000', 'Iteration: 1950', 'Train loss: 0.179393', 'Train acc: 0.958333')\n",
      "('Epoch: 389/2000', 'Iteration: 1950', 'Validation loss: 0.395629', 'Validation acc: 0.891667')\n",
      "('Epoch: 390/2000', 'Iteration: 1955', 'Train loss: 0.194089', 'Train acc: 0.950000')\n",
      "('Epoch: 391/2000', 'Iteration: 1960', 'Train loss: 0.201707', 'Train acc: 0.943333')\n",
      "('Epoch: 391/2000', 'Iteration: 1960', 'Validation loss: 0.395204', 'Validation acc: 0.890000')\n",
      "('Epoch: 392/2000', 'Iteration: 1965', 'Train loss: 0.180972', 'Train acc: 0.950000')\n",
      "('Epoch: 393/2000', 'Iteration: 1970', 'Train loss: 0.193308', 'Train acc: 0.935000')\n",
      "('Epoch: 393/2000', 'Iteration: 1970', 'Validation loss: 0.391598', 'Validation acc: 0.895000')\n",
      "('Epoch: 394/2000', 'Iteration: 1975', 'Train loss: 0.178413', 'Train acc: 0.955000')\n",
      "('Epoch: 395/2000', 'Iteration: 1980', 'Train loss: 0.186466', 'Train acc: 0.943333')\n",
      "('Epoch: 395/2000', 'Iteration: 1980', 'Validation loss: 0.394826', 'Validation acc: 0.895000')\n",
      "('Epoch: 396/2000', 'Iteration: 1985', 'Train loss: 0.192050', 'Train acc: 0.946667')\n",
      "('Epoch: 397/2000', 'Iteration: 1990', 'Train loss: 0.199634', 'Train acc: 0.940000')\n",
      "('Epoch: 397/2000', 'Iteration: 1990', 'Validation loss: 0.393039', 'Validation acc: 0.895000')\n",
      "('Epoch: 398/2000', 'Iteration: 1995', 'Train loss: 0.207038', 'Train acc: 0.946667')\n",
      "('Epoch: 399/2000', 'Iteration: 2000', 'Train loss: 0.208229', 'Train acc: 0.930000')\n",
      "('Epoch: 399/2000', 'Iteration: 2000', 'Validation loss: 0.392673', 'Validation acc: 0.893333')\n",
      "('Epoch: 400/2000', 'Iteration: 2005', 'Train loss: 0.180840', 'Train acc: 0.940000')\n",
      "('Epoch: 401/2000', 'Iteration: 2010', 'Train loss: 0.179229', 'Train acc: 0.953333')\n",
      "('Epoch: 401/2000', 'Iteration: 2010', 'Validation loss: 0.391608', 'Validation acc: 0.898333')\n",
      "('Epoch: 402/2000', 'Iteration: 2015', 'Train loss: 0.153964', 'Train acc: 0.963333')\n",
      "('Epoch: 403/2000', 'Iteration: 2020', 'Train loss: 0.183641', 'Train acc: 0.946667')\n",
      "('Epoch: 403/2000', 'Iteration: 2020', 'Validation loss: 0.391718', 'Validation acc: 0.895000')\n",
      "('Epoch: 404/2000', 'Iteration: 2025', 'Train loss: 0.181987', 'Train acc: 0.941667')\n",
      "('Epoch: 405/2000', 'Iteration: 2030', 'Train loss: 0.184188', 'Train acc: 0.945000')\n",
      "('Epoch: 405/2000', 'Iteration: 2030', 'Validation loss: 0.390091', 'Validation acc: 0.890000')\n",
      "('Epoch: 406/2000', 'Iteration: 2035', 'Train loss: 0.175543', 'Train acc: 0.943333')\n",
      "('Epoch: 407/2000', 'Iteration: 2040', 'Train loss: 0.166556', 'Train acc: 0.953333')\n",
      "('Epoch: 407/2000', 'Iteration: 2040', 'Validation loss: 0.392238', 'Validation acc: 0.896667')\n",
      "('Epoch: 408/2000', 'Iteration: 2045', 'Train loss: 0.163088', 'Train acc: 0.961667')\n",
      "('Epoch: 409/2000', 'Iteration: 2050', 'Train loss: 0.176222', 'Train acc: 0.950000')\n",
      "('Epoch: 409/2000', 'Iteration: 2050', 'Validation loss: 0.393400', 'Validation acc: 0.895000')\n",
      "('Epoch: 410/2000', 'Iteration: 2055', 'Train loss: 0.177510', 'Train acc: 0.953333')\n",
      "('Epoch: 411/2000', 'Iteration: 2060', 'Train loss: 0.187296', 'Train acc: 0.940000')\n",
      "('Epoch: 411/2000', 'Iteration: 2060', 'Validation loss: 0.388182', 'Validation acc: 0.898333')\n",
      "('Epoch: 412/2000', 'Iteration: 2065', 'Train loss: 0.179369', 'Train acc: 0.948333')\n",
      "('Epoch: 413/2000', 'Iteration: 2070', 'Train loss: 0.168232', 'Train acc: 0.953333')\n",
      "('Epoch: 413/2000', 'Iteration: 2070', 'Validation loss: 0.387566', 'Validation acc: 0.896667')\n",
      "('Epoch: 414/2000', 'Iteration: 2075', 'Train loss: 0.157427', 'Train acc: 0.968333')\n",
      "('Epoch: 415/2000', 'Iteration: 2080', 'Train loss: 0.163854', 'Train acc: 0.948333')\n",
      "('Epoch: 415/2000', 'Iteration: 2080', 'Validation loss: 0.386717', 'Validation acc: 0.895000')\n",
      "('Epoch: 416/2000', 'Iteration: 2085', 'Train loss: 0.169307', 'Train acc: 0.950000')\n",
      "('Epoch: 417/2000', 'Iteration: 2090', 'Train loss: 0.177879', 'Train acc: 0.946667')\n",
      "('Epoch: 417/2000', 'Iteration: 2090', 'Validation loss: 0.388127', 'Validation acc: 0.896667')\n",
      "('Epoch: 418/2000', 'Iteration: 2095', 'Train loss: 0.182275', 'Train acc: 0.945000')\n",
      "('Epoch: 419/2000', 'Iteration: 2100', 'Train loss: 0.177254', 'Train acc: 0.943333')\n",
      "('Epoch: 419/2000', 'Iteration: 2100', 'Validation loss: 0.387032', 'Validation acc: 0.895000')\n",
      "('Epoch: 420/2000', 'Iteration: 2105', 'Train loss: 0.161925', 'Train acc: 0.951667')\n",
      "('Epoch: 421/2000', 'Iteration: 2110', 'Train loss: 0.177894', 'Train acc: 0.946667')\n",
      "('Epoch: 421/2000', 'Iteration: 2110', 'Validation loss: 0.390214', 'Validation acc: 0.896667')\n",
      "('Epoch: 422/2000', 'Iteration: 2115', 'Train loss: 0.158753', 'Train acc: 0.963333')\n",
      "('Epoch: 423/2000', 'Iteration: 2120', 'Train loss: 0.191074', 'Train acc: 0.936667')\n",
      "('Epoch: 423/2000', 'Iteration: 2120', 'Validation loss: 0.388417', 'Validation acc: 0.896667')\n",
      "('Epoch: 424/2000', 'Iteration: 2125', 'Train loss: 0.169771', 'Train acc: 0.950000')\n",
      "('Epoch: 425/2000', 'Iteration: 2130', 'Train loss: 0.171229', 'Train acc: 0.941667')\n",
      "('Epoch: 425/2000', 'Iteration: 2130', 'Validation loss: 0.385587', 'Validation acc: 0.898333')\n",
      "('Epoch: 426/2000', 'Iteration: 2135', 'Train loss: 0.166688', 'Train acc: 0.950000')\n",
      "('Epoch: 427/2000', 'Iteration: 2140', 'Train loss: 0.162450', 'Train acc: 0.953333')\n",
      "('Epoch: 427/2000', 'Iteration: 2140', 'Validation loss: 0.385835', 'Validation acc: 0.900000')\n",
      "('Epoch: 428/2000', 'Iteration: 2145', 'Train loss: 0.155253', 'Train acc: 0.950000')\n",
      "('Epoch: 429/2000', 'Iteration: 2150', 'Train loss: 0.151114', 'Train acc: 0.955000')\n",
      "('Epoch: 429/2000', 'Iteration: 2150', 'Validation loss: 0.384914', 'Validation acc: 0.896667')\n",
      "('Epoch: 430/2000', 'Iteration: 2155', 'Train loss: 0.171756', 'Train acc: 0.945000')\n",
      "('Epoch: 431/2000', 'Iteration: 2160', 'Train loss: 0.151977', 'Train acc: 0.960000')\n",
      "('Epoch: 431/2000', 'Iteration: 2160', 'Validation loss: 0.383574', 'Validation acc: 0.900000')\n",
      "('Epoch: 432/2000', 'Iteration: 2165', 'Train loss: 0.149609', 'Train acc: 0.960000')\n",
      "('Epoch: 433/2000', 'Iteration: 2170', 'Train loss: 0.175283', 'Train acc: 0.948333')\n",
      "('Epoch: 433/2000', 'Iteration: 2170', 'Validation loss: 0.382697', 'Validation acc: 0.898333')\n",
      "('Epoch: 434/2000', 'Iteration: 2175', 'Train loss: 0.182654', 'Train acc: 0.940000')\n",
      "('Epoch: 435/2000', 'Iteration: 2180', 'Train loss: 0.152800', 'Train acc: 0.955000')\n",
      "('Epoch: 435/2000', 'Iteration: 2180', 'Validation loss: 0.384236', 'Validation acc: 0.896667')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 436/2000', 'Iteration: 2185', 'Train loss: 0.161646', 'Train acc: 0.945000')\n",
      "('Epoch: 437/2000', 'Iteration: 2190', 'Train loss: 0.144925', 'Train acc: 0.960000')\n",
      "('Epoch: 437/2000', 'Iteration: 2190', 'Validation loss: 0.385629', 'Validation acc: 0.896667')\n",
      "('Epoch: 438/2000', 'Iteration: 2195', 'Train loss: 0.163318', 'Train acc: 0.953333')\n",
      "('Epoch: 439/2000', 'Iteration: 2200', 'Train loss: 0.178707', 'Train acc: 0.950000')\n",
      "('Epoch: 439/2000', 'Iteration: 2200', 'Validation loss: 0.385222', 'Validation acc: 0.898333')\n",
      "('Epoch: 440/2000', 'Iteration: 2205', 'Train loss: 0.139017', 'Train acc: 0.961667')\n",
      "('Epoch: 441/2000', 'Iteration: 2210', 'Train loss: 0.144529', 'Train acc: 0.963333')\n",
      "('Epoch: 441/2000', 'Iteration: 2210', 'Validation loss: 0.381672', 'Validation acc: 0.896667')\n",
      "('Epoch: 442/2000', 'Iteration: 2215', 'Train loss: 0.154640', 'Train acc: 0.958333')\n",
      "('Epoch: 443/2000', 'Iteration: 2220', 'Train loss: 0.163749', 'Train acc: 0.951667')\n",
      "('Epoch: 443/2000', 'Iteration: 2220', 'Validation loss: 0.383549', 'Validation acc: 0.896667')\n",
      "('Epoch: 444/2000', 'Iteration: 2225', 'Train loss: 0.164994', 'Train acc: 0.951667')\n",
      "('Epoch: 445/2000', 'Iteration: 2230', 'Train loss: 0.164265', 'Train acc: 0.948333')\n",
      "('Epoch: 445/2000', 'Iteration: 2230', 'Validation loss: 0.383853', 'Validation acc: 0.896667')\n",
      "('Epoch: 446/2000', 'Iteration: 2235', 'Train loss: 0.152370', 'Train acc: 0.958333')\n",
      "('Epoch: 447/2000', 'Iteration: 2240', 'Train loss: 0.158750', 'Train acc: 0.951667')\n",
      "('Epoch: 447/2000', 'Iteration: 2240', 'Validation loss: 0.383686', 'Validation acc: 0.895000')\n",
      "('Epoch: 448/2000', 'Iteration: 2245', 'Train loss: 0.152607', 'Train acc: 0.963333')\n",
      "('Epoch: 449/2000', 'Iteration: 2250', 'Train loss: 0.165928', 'Train acc: 0.953333')\n",
      "('Epoch: 449/2000', 'Iteration: 2250', 'Validation loss: 0.381945', 'Validation acc: 0.896667')\n",
      "('Epoch: 450/2000', 'Iteration: 2255', 'Train loss: 0.161271', 'Train acc: 0.956667')\n",
      "('Epoch: 451/2000', 'Iteration: 2260', 'Train loss: 0.138955', 'Train acc: 0.960000')\n",
      "('Epoch: 451/2000', 'Iteration: 2260', 'Validation loss: 0.383234', 'Validation acc: 0.900000')\n",
      "('Epoch: 452/2000', 'Iteration: 2265', 'Train loss: 0.147998', 'Train acc: 0.951667')\n",
      "('Epoch: 453/2000', 'Iteration: 2270', 'Train loss: 0.155938', 'Train acc: 0.951667')\n",
      "('Epoch: 453/2000', 'Iteration: 2270', 'Validation loss: 0.379645', 'Validation acc: 0.896667')\n",
      "('Epoch: 454/2000', 'Iteration: 2275', 'Train loss: 0.147839', 'Train acc: 0.961667')\n",
      "('Epoch: 455/2000', 'Iteration: 2280', 'Train loss: 0.147781', 'Train acc: 0.960000')\n",
      "('Epoch: 455/2000', 'Iteration: 2280', 'Validation loss: 0.380965', 'Validation acc: 0.900000')\n",
      "('Epoch: 456/2000', 'Iteration: 2285', 'Train loss: 0.139438', 'Train acc: 0.960000')\n",
      "('Epoch: 457/2000', 'Iteration: 2290', 'Train loss: 0.135096', 'Train acc: 0.963333')\n",
      "('Epoch: 457/2000', 'Iteration: 2290', 'Validation loss: 0.383007', 'Validation acc: 0.898333')\n",
      "('Epoch: 458/2000', 'Iteration: 2295', 'Train loss: 0.173089', 'Train acc: 0.950000')\n",
      "('Epoch: 459/2000', 'Iteration: 2300', 'Train loss: 0.155738', 'Train acc: 0.948333')\n",
      "('Epoch: 459/2000', 'Iteration: 2300', 'Validation loss: 0.378681', 'Validation acc: 0.896667')\n",
      "('Epoch: 460/2000', 'Iteration: 2305', 'Train loss: 0.151827', 'Train acc: 0.956667')\n",
      "('Epoch: 461/2000', 'Iteration: 2310', 'Train loss: 0.150000', 'Train acc: 0.951667')\n",
      "('Epoch: 461/2000', 'Iteration: 2310', 'Validation loss: 0.382537', 'Validation acc: 0.896667')\n",
      "('Epoch: 462/2000', 'Iteration: 2315', 'Train loss: 0.153204', 'Train acc: 0.946667')\n",
      "('Epoch: 463/2000', 'Iteration: 2320', 'Train loss: 0.165716', 'Train acc: 0.951667')\n",
      "('Epoch: 463/2000', 'Iteration: 2320', 'Validation loss: 0.382664', 'Validation acc: 0.900000')\n",
      "('Epoch: 464/2000', 'Iteration: 2325', 'Train loss: 0.129947', 'Train acc: 0.965000')\n",
      "('Epoch: 465/2000', 'Iteration: 2330', 'Train loss: 0.143730', 'Train acc: 0.956667')\n",
      "('Epoch: 465/2000', 'Iteration: 2330', 'Validation loss: 0.378376', 'Validation acc: 0.901667')\n",
      "('Epoch: 466/2000', 'Iteration: 2335', 'Train loss: 0.139054', 'Train acc: 0.963333')\n",
      "('Epoch: 467/2000', 'Iteration: 2340', 'Train loss: 0.155439', 'Train acc: 0.961667')\n",
      "('Epoch: 467/2000', 'Iteration: 2340', 'Validation loss: 0.377663', 'Validation acc: 0.898333')\n",
      "('Epoch: 468/2000', 'Iteration: 2345', 'Train loss: 0.145337', 'Train acc: 0.965000')\n",
      "('Epoch: 469/2000', 'Iteration: 2350', 'Train loss: 0.144496', 'Train acc: 0.958333')\n",
      "('Epoch: 469/2000', 'Iteration: 2350', 'Validation loss: 0.378405', 'Validation acc: 0.898333')\n",
      "('Epoch: 470/2000', 'Iteration: 2355', 'Train loss: 0.145764', 'Train acc: 0.951667')\n",
      "('Epoch: 471/2000', 'Iteration: 2360', 'Train loss: 0.132705', 'Train acc: 0.963333')\n",
      "('Epoch: 471/2000', 'Iteration: 2360', 'Validation loss: 0.377657', 'Validation acc: 0.898333')\n",
      "('Epoch: 472/2000', 'Iteration: 2365', 'Train loss: 0.130737', 'Train acc: 0.961667')\n",
      "('Epoch: 473/2000', 'Iteration: 2370', 'Train loss: 0.149901', 'Train acc: 0.953333')\n",
      "('Epoch: 473/2000', 'Iteration: 2370', 'Validation loss: 0.376043', 'Validation acc: 0.901667')\n",
      "('Epoch: 474/2000', 'Iteration: 2375', 'Train loss: 0.145143', 'Train acc: 0.953333')\n",
      "('Epoch: 475/2000', 'Iteration: 2380', 'Train loss: 0.129166', 'Train acc: 0.966667')\n",
      "('Epoch: 475/2000', 'Iteration: 2380', 'Validation loss: 0.380416', 'Validation acc: 0.896667')\n",
      "('Epoch: 476/2000', 'Iteration: 2385', 'Train loss: 0.148302', 'Train acc: 0.958333')\n",
      "('Epoch: 477/2000', 'Iteration: 2390', 'Train loss: 0.143793', 'Train acc: 0.960000')\n",
      "('Epoch: 477/2000', 'Iteration: 2390', 'Validation loss: 0.377002', 'Validation acc: 0.896667')\n",
      "('Epoch: 478/2000', 'Iteration: 2395', 'Train loss: 0.127554', 'Train acc: 0.966667')\n",
      "('Epoch: 479/2000', 'Iteration: 2400', 'Train loss: 0.135312', 'Train acc: 0.963333')\n",
      "('Epoch: 479/2000', 'Iteration: 2400', 'Validation loss: 0.383729', 'Validation acc: 0.895000')\n",
      "('Epoch: 480/2000', 'Iteration: 2405', 'Train loss: 0.141684', 'Train acc: 0.960000')\n",
      "('Epoch: 481/2000', 'Iteration: 2410', 'Train loss: 0.143311', 'Train acc: 0.956667')\n",
      "('Epoch: 481/2000', 'Iteration: 2410', 'Validation loss: 0.380893', 'Validation acc: 0.900000')\n",
      "('Epoch: 482/2000', 'Iteration: 2415', 'Train loss: 0.141985', 'Train acc: 0.963333')\n",
      "('Epoch: 483/2000', 'Iteration: 2420', 'Train loss: 0.141927', 'Train acc: 0.951667')\n",
      "('Epoch: 483/2000', 'Iteration: 2420', 'Validation loss: 0.376891', 'Validation acc: 0.898333')\n",
      "('Epoch: 484/2000', 'Iteration: 2425', 'Train loss: 0.138780', 'Train acc: 0.956667')\n",
      "('Epoch: 485/2000', 'Iteration: 2430', 'Train loss: 0.144684', 'Train acc: 0.953333')\n",
      "('Epoch: 485/2000', 'Iteration: 2430', 'Validation loss: 0.379050', 'Validation acc: 0.898333')\n",
      "('Epoch: 486/2000', 'Iteration: 2435', 'Train loss: 0.151429', 'Train acc: 0.958333')\n",
      "('Epoch: 487/2000', 'Iteration: 2440', 'Train loss: 0.129717', 'Train acc: 0.966667')\n",
      "('Epoch: 487/2000', 'Iteration: 2440', 'Validation loss: 0.378373', 'Validation acc: 0.901667')\n",
      "('Epoch: 488/2000', 'Iteration: 2445', 'Train loss: 0.122083', 'Train acc: 0.970000')\n",
      "('Epoch: 489/2000', 'Iteration: 2450', 'Train loss: 0.120213', 'Train acc: 0.970000')\n",
      "('Epoch: 489/2000', 'Iteration: 2450', 'Validation loss: 0.377527', 'Validation acc: 0.898333')\n",
      "('Epoch: 490/2000', 'Iteration: 2455', 'Train loss: 0.138071', 'Train acc: 0.963333')\n",
      "('Epoch: 491/2000', 'Iteration: 2460', 'Train loss: 0.148109', 'Train acc: 0.956667')\n",
      "('Epoch: 491/2000', 'Iteration: 2460', 'Validation loss: 0.376151', 'Validation acc: 0.896667')\n",
      "('Epoch: 492/2000', 'Iteration: 2465', 'Train loss: 0.150252', 'Train acc: 0.948333')\n",
      "('Epoch: 493/2000', 'Iteration: 2470', 'Train loss: 0.128567', 'Train acc: 0.960000')\n",
      "('Epoch: 493/2000', 'Iteration: 2470', 'Validation loss: 0.375931', 'Validation acc: 0.900000')\n",
      "('Epoch: 494/2000', 'Iteration: 2475', 'Train loss: 0.133886', 'Train acc: 0.966667')\n",
      "('Epoch: 495/2000', 'Iteration: 2480', 'Train loss: 0.126261', 'Train acc: 0.968333')\n",
      "('Epoch: 495/2000', 'Iteration: 2480', 'Validation loss: 0.380676', 'Validation acc: 0.900000')\n",
      "('Epoch: 496/2000', 'Iteration: 2485', 'Train loss: 0.133814', 'Train acc: 0.968333')\n",
      "('Epoch: 497/2000', 'Iteration: 2490', 'Train loss: 0.142260', 'Train acc: 0.961667')\n",
      "('Epoch: 497/2000', 'Iteration: 2490', 'Validation loss: 0.378623', 'Validation acc: 0.900000')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 498/2000', 'Iteration: 2495', 'Train loss: 0.119681', 'Train acc: 0.973333')\n",
      "('Epoch: 499/2000', 'Iteration: 2500', 'Train loss: 0.128136', 'Train acc: 0.963333')\n",
      "('Epoch: 499/2000', 'Iteration: 2500', 'Validation loss: 0.376033', 'Validation acc: 0.903333')\n",
      "('Epoch: 500/2000', 'Iteration: 2505', 'Train loss: 0.121538', 'Train acc: 0.966667')\n",
      "('Epoch: 501/2000', 'Iteration: 2510', 'Train loss: 0.131640', 'Train acc: 0.965000')\n",
      "('Epoch: 501/2000', 'Iteration: 2510', 'Validation loss: 0.378234', 'Validation acc: 0.903333')\n",
      "('Epoch: 502/2000', 'Iteration: 2515', 'Train loss: 0.132885', 'Train acc: 0.955000')\n",
      "('Epoch: 503/2000', 'Iteration: 2520', 'Train loss: 0.135177', 'Train acc: 0.966667')\n",
      "('Epoch: 503/2000', 'Iteration: 2520', 'Validation loss: 0.378602', 'Validation acc: 0.900000')\n",
      "('Epoch: 504/2000', 'Iteration: 2525', 'Train loss: 0.130187', 'Train acc: 0.966667')\n",
      "('Epoch: 505/2000', 'Iteration: 2530', 'Train loss: 0.124278', 'Train acc: 0.960000')\n",
      "('Epoch: 505/2000', 'Iteration: 2530', 'Validation loss: 0.378747', 'Validation acc: 0.900000')\n",
      "('Epoch: 506/2000', 'Iteration: 2535', 'Train loss: 0.129286', 'Train acc: 0.966667')\n",
      "('Epoch: 507/2000', 'Iteration: 2540', 'Train loss: 0.120549', 'Train acc: 0.971667')\n",
      "('Epoch: 507/2000', 'Iteration: 2540', 'Validation loss: 0.375558', 'Validation acc: 0.903333')\n",
      "('Epoch: 508/2000', 'Iteration: 2545', 'Train loss: 0.137104', 'Train acc: 0.958333')\n",
      "('Epoch: 509/2000', 'Iteration: 2550', 'Train loss: 0.134526', 'Train acc: 0.966667')\n",
      "('Epoch: 509/2000', 'Iteration: 2550', 'Validation loss: 0.373716', 'Validation acc: 0.903333')\n",
      "('Epoch: 510/2000', 'Iteration: 2555', 'Train loss: 0.142629', 'Train acc: 0.960000')\n",
      "('Epoch: 511/2000', 'Iteration: 2560', 'Train loss: 0.123630', 'Train acc: 0.960000')\n",
      "('Epoch: 511/2000', 'Iteration: 2560', 'Validation loss: 0.377930', 'Validation acc: 0.900000')\n",
      "('Epoch: 512/2000', 'Iteration: 2565', 'Train loss: 0.128481', 'Train acc: 0.958333')\n",
      "('Epoch: 513/2000', 'Iteration: 2570', 'Train loss: 0.127413', 'Train acc: 0.968333')\n",
      "('Epoch: 513/2000', 'Iteration: 2570', 'Validation loss: 0.375240', 'Validation acc: 0.901667')\n",
      "('Epoch: 514/2000', 'Iteration: 2575', 'Train loss: 0.135821', 'Train acc: 0.960000')\n",
      "('Epoch: 515/2000', 'Iteration: 2580', 'Train loss: 0.115899', 'Train acc: 0.968333')\n",
      "('Epoch: 515/2000', 'Iteration: 2580', 'Validation loss: 0.374356', 'Validation acc: 0.903333')\n",
      "('Epoch: 516/2000', 'Iteration: 2585', 'Train loss: 0.124828', 'Train acc: 0.965000')\n",
      "('Epoch: 517/2000', 'Iteration: 2590', 'Train loss: 0.140446', 'Train acc: 0.955000')\n",
      "('Epoch: 517/2000', 'Iteration: 2590', 'Validation loss: 0.377259', 'Validation acc: 0.901667')\n",
      "('Epoch: 518/2000', 'Iteration: 2595', 'Train loss: 0.128390', 'Train acc: 0.960000')\n",
      "('Epoch: 519/2000', 'Iteration: 2600', 'Train loss: 0.134604', 'Train acc: 0.965000')\n",
      "('Epoch: 519/2000', 'Iteration: 2600', 'Validation loss: 0.371753', 'Validation acc: 0.905000')\n",
      "('Epoch: 520/2000', 'Iteration: 2605', 'Train loss: 0.122030', 'Train acc: 0.970000')\n",
      "('Epoch: 521/2000', 'Iteration: 2610', 'Train loss: 0.119851', 'Train acc: 0.965000')\n",
      "('Epoch: 521/2000', 'Iteration: 2610', 'Validation loss: 0.374647', 'Validation acc: 0.901667')\n",
      "('Epoch: 522/2000', 'Iteration: 2615', 'Train loss: 0.128610', 'Train acc: 0.961667')\n",
      "('Epoch: 523/2000', 'Iteration: 2620', 'Train loss: 0.127694', 'Train acc: 0.968333')\n",
      "('Epoch: 523/2000', 'Iteration: 2620', 'Validation loss: 0.372539', 'Validation acc: 0.901667')\n",
      "('Epoch: 524/2000', 'Iteration: 2625', 'Train loss: 0.138711', 'Train acc: 0.955000')\n",
      "('Epoch: 525/2000', 'Iteration: 2630', 'Train loss: 0.125224', 'Train acc: 0.963333')\n",
      "('Epoch: 525/2000', 'Iteration: 2630', 'Validation loss: 0.370837', 'Validation acc: 0.901667')\n",
      "('Epoch: 526/2000', 'Iteration: 2635', 'Train loss: 0.140055', 'Train acc: 0.953333')\n",
      "('Epoch: 527/2000', 'Iteration: 2640', 'Train loss: 0.123346', 'Train acc: 0.961667')\n",
      "('Epoch: 527/2000', 'Iteration: 2640', 'Validation loss: 0.371636', 'Validation acc: 0.901667')\n",
      "('Epoch: 528/2000', 'Iteration: 2645', 'Train loss: 0.125412', 'Train acc: 0.973333')\n",
      "('Epoch: 529/2000', 'Iteration: 2650', 'Train loss: 0.118438', 'Train acc: 0.956667')\n",
      "('Epoch: 529/2000', 'Iteration: 2650', 'Validation loss: 0.369175', 'Validation acc: 0.901667')\n",
      "('Epoch: 530/2000', 'Iteration: 2655', 'Train loss: 0.127251', 'Train acc: 0.955000')\n",
      "('Epoch: 531/2000', 'Iteration: 2660', 'Train loss: 0.108600', 'Train acc: 0.973333')\n",
      "('Epoch: 531/2000', 'Iteration: 2660', 'Validation loss: 0.371249', 'Validation acc: 0.903333')\n",
      "('Epoch: 532/2000', 'Iteration: 2665', 'Train loss: 0.123041', 'Train acc: 0.958333')\n",
      "('Epoch: 533/2000', 'Iteration: 2670', 'Train loss: 0.123856', 'Train acc: 0.956667')\n",
      "('Epoch: 533/2000', 'Iteration: 2670', 'Validation loss: 0.375854', 'Validation acc: 0.900000')\n",
      "('Epoch: 534/2000', 'Iteration: 2675', 'Train loss: 0.145258', 'Train acc: 0.956667')\n",
      "('Epoch: 535/2000', 'Iteration: 2680', 'Train loss: 0.115596', 'Train acc: 0.968333')\n",
      "('Epoch: 535/2000', 'Iteration: 2680', 'Validation loss: 0.370045', 'Validation acc: 0.903333')\n",
      "('Epoch: 536/2000', 'Iteration: 2685', 'Train loss: 0.120182', 'Train acc: 0.968333')\n",
      "('Epoch: 537/2000', 'Iteration: 2690', 'Train loss: 0.118114', 'Train acc: 0.968333')\n",
      "('Epoch: 537/2000', 'Iteration: 2690', 'Validation loss: 0.373114', 'Validation acc: 0.901667')\n",
      "('Epoch: 538/2000', 'Iteration: 2695', 'Train loss: 0.125761', 'Train acc: 0.960000')\n",
      "('Epoch: 539/2000', 'Iteration: 2700', 'Train loss: 0.106323', 'Train acc: 0.975000')\n",
      "('Epoch: 539/2000', 'Iteration: 2700', 'Validation loss: 0.369997', 'Validation acc: 0.903333')\n",
      "('Epoch: 540/2000', 'Iteration: 2705', 'Train loss: 0.104814', 'Train acc: 0.978333')\n",
      "('Epoch: 541/2000', 'Iteration: 2710', 'Train loss: 0.102220', 'Train acc: 0.966667')\n",
      "('Epoch: 541/2000', 'Iteration: 2710', 'Validation loss: 0.371714', 'Validation acc: 0.903333')\n",
      "('Epoch: 542/2000', 'Iteration: 2715', 'Train loss: 0.118706', 'Train acc: 0.963333')\n",
      "('Epoch: 543/2000', 'Iteration: 2720', 'Train loss: 0.122907', 'Train acc: 0.961667')\n",
      "('Epoch: 543/2000', 'Iteration: 2720', 'Validation loss: 0.372768', 'Validation acc: 0.903333')\n",
      "('Epoch: 544/2000', 'Iteration: 2725', 'Train loss: 0.110665', 'Train acc: 0.970000')\n",
      "('Epoch: 545/2000', 'Iteration: 2730', 'Train loss: 0.111554', 'Train acc: 0.965000')\n",
      "('Epoch: 545/2000', 'Iteration: 2730', 'Validation loss: 0.371518', 'Validation acc: 0.905000')\n",
      "('Epoch: 546/2000', 'Iteration: 2735', 'Train loss: 0.122676', 'Train acc: 0.966667')\n",
      "('Epoch: 547/2000', 'Iteration: 2740', 'Train loss: 0.099885', 'Train acc: 0.971667')\n",
      "('Epoch: 547/2000', 'Iteration: 2740', 'Validation loss: 0.368740', 'Validation acc: 0.903333')\n",
      "('Epoch: 548/2000', 'Iteration: 2745', 'Train loss: 0.116761', 'Train acc: 0.965000')\n",
      "('Epoch: 549/2000', 'Iteration: 2750', 'Train loss: 0.114321', 'Train acc: 0.965000')\n",
      "('Epoch: 549/2000', 'Iteration: 2750', 'Validation loss: 0.372286', 'Validation acc: 0.906667')\n",
      "('Epoch: 550/2000', 'Iteration: 2755', 'Train loss: 0.101902', 'Train acc: 0.970000')\n",
      "('Epoch: 551/2000', 'Iteration: 2760', 'Train loss: 0.122827', 'Train acc: 0.958333')\n",
      "('Epoch: 551/2000', 'Iteration: 2760', 'Validation loss: 0.372883', 'Validation acc: 0.896667')\n",
      "('Epoch: 552/2000', 'Iteration: 2765', 'Train loss: 0.129416', 'Train acc: 0.960000')\n",
      "('Epoch: 553/2000', 'Iteration: 2770', 'Train loss: 0.117028', 'Train acc: 0.973333')\n",
      "('Epoch: 553/2000', 'Iteration: 2770', 'Validation loss: 0.372139', 'Validation acc: 0.900000')\n",
      "('Epoch: 554/2000', 'Iteration: 2775', 'Train loss: 0.113238', 'Train acc: 0.966667')\n",
      "('Epoch: 555/2000', 'Iteration: 2780', 'Train loss: 0.113038', 'Train acc: 0.961667')\n",
      "('Epoch: 555/2000', 'Iteration: 2780', 'Validation loss: 0.372542', 'Validation acc: 0.901667')\n",
      "('Epoch: 556/2000', 'Iteration: 2785', 'Train loss: 0.115504', 'Train acc: 0.968333')\n",
      "('Epoch: 557/2000', 'Iteration: 2790', 'Train loss: 0.117326', 'Train acc: 0.966667')\n",
      "('Epoch: 557/2000', 'Iteration: 2790', 'Validation loss: 0.364577', 'Validation acc: 0.901667')\n",
      "('Epoch: 558/2000', 'Iteration: 2795', 'Train loss: 0.106294', 'Train acc: 0.963333')\n",
      "('Epoch: 559/2000', 'Iteration: 2800', 'Train loss: 0.103743', 'Train acc: 0.976667')\n",
      "('Epoch: 559/2000', 'Iteration: 2800', 'Validation loss: 0.369552', 'Validation acc: 0.901667')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 560/2000', 'Iteration: 2805', 'Train loss: 0.116892', 'Train acc: 0.968333')\n",
      "('Epoch: 561/2000', 'Iteration: 2810', 'Train loss: 0.134357', 'Train acc: 0.951667')\n",
      "('Epoch: 561/2000', 'Iteration: 2810', 'Validation loss: 0.373868', 'Validation acc: 0.903333')\n",
      "('Epoch: 562/2000', 'Iteration: 2815', 'Train loss: 0.118629', 'Train acc: 0.963333')\n",
      "('Epoch: 563/2000', 'Iteration: 2820', 'Train loss: 0.116574', 'Train acc: 0.963333')\n",
      "('Epoch: 563/2000', 'Iteration: 2820', 'Validation loss: 0.372444', 'Validation acc: 0.905000')\n",
      "('Epoch: 564/2000', 'Iteration: 2825', 'Train loss: 0.113877', 'Train acc: 0.973333')\n",
      "('Epoch: 565/2000', 'Iteration: 2830', 'Train loss: 0.114575', 'Train acc: 0.968333')\n",
      "('Epoch: 565/2000', 'Iteration: 2830', 'Validation loss: 0.366349', 'Validation acc: 0.906667')\n",
      "('Epoch: 566/2000', 'Iteration: 2835', 'Train loss: 0.096448', 'Train acc: 0.971667')\n",
      "('Epoch: 567/2000', 'Iteration: 2840', 'Train loss: 0.111456', 'Train acc: 0.966667')\n",
      "('Epoch: 567/2000', 'Iteration: 2840', 'Validation loss: 0.371168', 'Validation acc: 0.901667')\n",
      "('Epoch: 568/2000', 'Iteration: 2845', 'Train loss: 0.109190', 'Train acc: 0.965000')\n",
      "('Epoch: 569/2000', 'Iteration: 2850', 'Train loss: 0.100976', 'Train acc: 0.976667')\n",
      "('Epoch: 569/2000', 'Iteration: 2850', 'Validation loss: 0.373001', 'Validation acc: 0.901667')\n",
      "('Epoch: 570/2000', 'Iteration: 2855', 'Train loss: 0.103564', 'Train acc: 0.966667')\n",
      "('Epoch: 571/2000', 'Iteration: 2860', 'Train loss: 0.105707', 'Train acc: 0.975000')\n",
      "('Epoch: 571/2000', 'Iteration: 2860', 'Validation loss: 0.374149', 'Validation acc: 0.901667')\n",
      "('Epoch: 572/2000', 'Iteration: 2865', 'Train loss: 0.107441', 'Train acc: 0.966667')\n",
      "('Epoch: 573/2000', 'Iteration: 2870', 'Train loss: 0.105920', 'Train acc: 0.971667')\n",
      "('Epoch: 573/2000', 'Iteration: 2870', 'Validation loss: 0.374526', 'Validation acc: 0.903333')\n",
      "('Epoch: 574/2000', 'Iteration: 2875', 'Train loss: 0.108046', 'Train acc: 0.975000')\n",
      "('Epoch: 575/2000', 'Iteration: 2880', 'Train loss: 0.101464', 'Train acc: 0.973333')\n",
      "('Epoch: 575/2000', 'Iteration: 2880', 'Validation loss: 0.368945', 'Validation acc: 0.903333')\n",
      "('Epoch: 576/2000', 'Iteration: 2885', 'Train loss: 0.103048', 'Train acc: 0.973333')\n",
      "('Epoch: 577/2000', 'Iteration: 2890', 'Train loss: 0.107160', 'Train acc: 0.975000')\n",
      "('Epoch: 577/2000', 'Iteration: 2890', 'Validation loss: 0.371176', 'Validation acc: 0.903333')\n",
      "('Epoch: 578/2000', 'Iteration: 2895', 'Train loss: 0.090200', 'Train acc: 0.968333')\n",
      "('Epoch: 579/2000', 'Iteration: 2900', 'Train loss: 0.103326', 'Train acc: 0.971667')\n",
      "('Epoch: 579/2000', 'Iteration: 2900', 'Validation loss: 0.367838', 'Validation acc: 0.903333')\n",
      "('Epoch: 580/2000', 'Iteration: 2905', 'Train loss: 0.100133', 'Train acc: 0.965000')\n",
      "('Epoch: 581/2000', 'Iteration: 2910', 'Train loss: 0.120557', 'Train acc: 0.963333')\n",
      "('Epoch: 581/2000', 'Iteration: 2910', 'Validation loss: 0.371721', 'Validation acc: 0.905000')\n",
      "('Epoch: 582/2000', 'Iteration: 2915', 'Train loss: 0.116733', 'Train acc: 0.960000')\n",
      "('Epoch: 583/2000', 'Iteration: 2920', 'Train loss: 0.091197', 'Train acc: 0.985000')\n",
      "('Epoch: 583/2000', 'Iteration: 2920', 'Validation loss: 0.370289', 'Validation acc: 0.905000')\n",
      "('Epoch: 584/2000', 'Iteration: 2925', 'Train loss: 0.102152', 'Train acc: 0.968333')\n",
      "('Epoch: 585/2000', 'Iteration: 2930', 'Train loss: 0.105373', 'Train acc: 0.968333')\n",
      "('Epoch: 585/2000', 'Iteration: 2930', 'Validation loss: 0.371959', 'Validation acc: 0.901667')\n",
      "('Epoch: 586/2000', 'Iteration: 2935', 'Train loss: 0.095125', 'Train acc: 0.978333')\n",
      "('Epoch: 587/2000', 'Iteration: 2940', 'Train loss: 0.096382', 'Train acc: 0.976667')\n",
      "('Epoch: 587/2000', 'Iteration: 2940', 'Validation loss: 0.371875', 'Validation acc: 0.905000')\n",
      "('Epoch: 588/2000', 'Iteration: 2945', 'Train loss: 0.098029', 'Train acc: 0.976667')\n",
      "('Epoch: 589/2000', 'Iteration: 2950', 'Train loss: 0.102260', 'Train acc: 0.976667')\n",
      "('Epoch: 589/2000', 'Iteration: 2950', 'Validation loss: 0.371445', 'Validation acc: 0.900000')\n",
      "('Epoch: 590/2000', 'Iteration: 2955', 'Train loss: 0.102207', 'Train acc: 0.968333')\n",
      "('Epoch: 591/2000', 'Iteration: 2960', 'Train loss: 0.085604', 'Train acc: 0.978333')\n",
      "('Epoch: 591/2000', 'Iteration: 2960', 'Validation loss: 0.372105', 'Validation acc: 0.906667')\n",
      "('Epoch: 592/2000', 'Iteration: 2965', 'Train loss: 0.092679', 'Train acc: 0.975000')\n",
      "('Epoch: 593/2000', 'Iteration: 2970', 'Train loss: 0.115148', 'Train acc: 0.966667')\n",
      "('Epoch: 593/2000', 'Iteration: 2970', 'Validation loss: 0.372768', 'Validation acc: 0.901667')\n",
      "('Epoch: 594/2000', 'Iteration: 2975', 'Train loss: 0.106046', 'Train acc: 0.971667')\n",
      "('Epoch: 595/2000', 'Iteration: 2980', 'Train loss: 0.100067', 'Train acc: 0.971667')\n",
      "('Epoch: 595/2000', 'Iteration: 2980', 'Validation loss: 0.371815', 'Validation acc: 0.903333')\n",
      "('Epoch: 596/2000', 'Iteration: 2985', 'Train loss: 0.104845', 'Train acc: 0.975000')\n",
      "('Epoch: 597/2000', 'Iteration: 2990', 'Train loss: 0.089043', 'Train acc: 0.973333')\n",
      "('Epoch: 597/2000', 'Iteration: 2990', 'Validation loss: 0.370363', 'Validation acc: 0.911667')\n",
      "('Epoch: 598/2000', 'Iteration: 2995', 'Train loss: 0.121628', 'Train acc: 0.966667')\n",
      "('Epoch: 599/2000', 'Iteration: 3000', 'Train loss: 0.095446', 'Train acc: 0.973333')\n",
      "('Epoch: 599/2000', 'Iteration: 3000', 'Validation loss: 0.369457', 'Validation acc: 0.903333')\n",
      "('Epoch: 600/2000', 'Iteration: 3005', 'Train loss: 0.094520', 'Train acc: 0.973333')\n",
      "('Epoch: 601/2000', 'Iteration: 3010', 'Train loss: 0.104581', 'Train acc: 0.973333')\n",
      "('Epoch: 601/2000', 'Iteration: 3010', 'Validation loss: 0.369516', 'Validation acc: 0.903333')\n",
      "('Epoch: 602/2000', 'Iteration: 3015', 'Train loss: 0.102292', 'Train acc: 0.968333')\n",
      "('Epoch: 603/2000', 'Iteration: 3020', 'Train loss: 0.097238', 'Train acc: 0.980000')\n",
      "('Epoch: 603/2000', 'Iteration: 3020', 'Validation loss: 0.369119', 'Validation acc: 0.903333')\n",
      "('Epoch: 604/2000', 'Iteration: 3025', 'Train loss: 0.095464', 'Train acc: 0.976667')\n",
      "('Epoch: 605/2000', 'Iteration: 3030', 'Train loss: 0.109627', 'Train acc: 0.966667')\n",
      "('Epoch: 605/2000', 'Iteration: 3030', 'Validation loss: 0.371675', 'Validation acc: 0.903333')\n",
      "('Epoch: 606/2000', 'Iteration: 3035', 'Train loss: 0.097687', 'Train acc: 0.968333')\n",
      "('Epoch: 607/2000', 'Iteration: 3040', 'Train loss: 0.114645', 'Train acc: 0.958333')\n",
      "('Epoch: 607/2000', 'Iteration: 3040', 'Validation loss: 0.369200', 'Validation acc: 0.908333')\n",
      "('Epoch: 608/2000', 'Iteration: 3045', 'Train loss: 0.088651', 'Train acc: 0.978333')\n",
      "('Epoch: 609/2000', 'Iteration: 3050', 'Train loss: 0.099976', 'Train acc: 0.966667')\n",
      "('Epoch: 609/2000', 'Iteration: 3050', 'Validation loss: 0.369732', 'Validation acc: 0.906667')\n",
      "('Epoch: 610/2000', 'Iteration: 3055', 'Train loss: 0.098385', 'Train acc: 0.976667')\n",
      "('Epoch: 611/2000', 'Iteration: 3060', 'Train loss: 0.101724', 'Train acc: 0.963333')\n",
      "('Epoch: 611/2000', 'Iteration: 3060', 'Validation loss: 0.368753', 'Validation acc: 0.908333')\n",
      "('Epoch: 612/2000', 'Iteration: 3065', 'Train loss: 0.084796', 'Train acc: 0.983333')\n",
      "('Epoch: 613/2000', 'Iteration: 3070', 'Train loss: 0.111369', 'Train acc: 0.966667')\n",
      "('Epoch: 613/2000', 'Iteration: 3070', 'Validation loss: 0.366837', 'Validation acc: 0.905000')\n",
      "('Epoch: 614/2000', 'Iteration: 3075', 'Train loss: 0.096409', 'Train acc: 0.973333')\n",
      "('Epoch: 615/2000', 'Iteration: 3080', 'Train loss: 0.103536', 'Train acc: 0.968333')\n",
      "('Epoch: 615/2000', 'Iteration: 3080', 'Validation loss: 0.368005', 'Validation acc: 0.905000')\n",
      "('Epoch: 616/2000', 'Iteration: 3085', 'Train loss: 0.110564', 'Train acc: 0.973333')\n",
      "('Epoch: 617/2000', 'Iteration: 3090', 'Train loss: 0.088425', 'Train acc: 0.971667')\n",
      "('Epoch: 617/2000', 'Iteration: 3090', 'Validation loss: 0.370407', 'Validation acc: 0.910000')\n",
      "('Epoch: 618/2000', 'Iteration: 3095', 'Train loss: 0.090326', 'Train acc: 0.975000')\n",
      "('Epoch: 619/2000', 'Iteration: 3100', 'Train loss: 0.097289', 'Train acc: 0.968333')\n",
      "('Epoch: 619/2000', 'Iteration: 3100', 'Validation loss: 0.368753', 'Validation acc: 0.911667')\n",
      "('Epoch: 620/2000', 'Iteration: 3105', 'Train loss: 0.090482', 'Train acc: 0.980000')\n",
      "('Epoch: 621/2000', 'Iteration: 3110', 'Train loss: 0.087853', 'Train acc: 0.973333')\n",
      "('Epoch: 621/2000', 'Iteration: 3110', 'Validation loss: 0.370247', 'Validation acc: 0.903333')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 622/2000', 'Iteration: 3115', 'Train loss: 0.099821', 'Train acc: 0.963333')\n",
      "('Epoch: 623/2000', 'Iteration: 3120', 'Train loss: 0.096347', 'Train acc: 0.968333')\n",
      "('Epoch: 623/2000', 'Iteration: 3120', 'Validation loss: 0.368402', 'Validation acc: 0.906667')\n",
      "('Epoch: 624/2000', 'Iteration: 3125', 'Train loss: 0.099578', 'Train acc: 0.971667')\n",
      "('Epoch: 625/2000', 'Iteration: 3130', 'Train loss: 0.091897', 'Train acc: 0.970000')\n",
      "('Epoch: 625/2000', 'Iteration: 3130', 'Validation loss: 0.369683', 'Validation acc: 0.913333')\n",
      "('Epoch: 626/2000', 'Iteration: 3135', 'Train loss: 0.099571', 'Train acc: 0.976667')\n",
      "('Epoch: 627/2000', 'Iteration: 3140', 'Train loss: 0.093780', 'Train acc: 0.976667')\n",
      "('Epoch: 627/2000', 'Iteration: 3140', 'Validation loss: 0.367150', 'Validation acc: 0.908333')\n",
      "('Epoch: 628/2000', 'Iteration: 3145', 'Train loss: 0.093772', 'Train acc: 0.968333')\n",
      "('Epoch: 629/2000', 'Iteration: 3150', 'Train loss: 0.084399', 'Train acc: 0.976667')\n",
      "('Epoch: 629/2000', 'Iteration: 3150', 'Validation loss: 0.371460', 'Validation acc: 0.905000')\n",
      "('Epoch: 630/2000', 'Iteration: 3155', 'Train loss: 0.090365', 'Train acc: 0.976667')\n",
      "('Epoch: 631/2000', 'Iteration: 3160', 'Train loss: 0.082565', 'Train acc: 0.981667')\n",
      "('Epoch: 631/2000', 'Iteration: 3160', 'Validation loss: 0.374107', 'Validation acc: 0.906667')\n",
      "('Epoch: 632/2000', 'Iteration: 3165', 'Train loss: 0.089388', 'Train acc: 0.971667')\n",
      "('Epoch: 633/2000', 'Iteration: 3170', 'Train loss: 0.106196', 'Train acc: 0.965000')\n",
      "('Epoch: 633/2000', 'Iteration: 3170', 'Validation loss: 0.374636', 'Validation acc: 0.906667')\n",
      "('Epoch: 634/2000', 'Iteration: 3175', 'Train loss: 0.099850', 'Train acc: 0.971667')\n",
      "('Epoch: 635/2000', 'Iteration: 3180', 'Train loss: 0.097258', 'Train acc: 0.966667')\n",
      "('Epoch: 635/2000', 'Iteration: 3180', 'Validation loss: 0.372211', 'Validation acc: 0.910000')\n",
      "('Epoch: 636/2000', 'Iteration: 3185', 'Train loss: 0.106402', 'Train acc: 0.968333')\n",
      "('Epoch: 637/2000', 'Iteration: 3190', 'Train loss: 0.087793', 'Train acc: 0.980000')\n",
      "('Epoch: 637/2000', 'Iteration: 3190', 'Validation loss: 0.373368', 'Validation acc: 0.906667')\n",
      "('Epoch: 638/2000', 'Iteration: 3195', 'Train loss: 0.084411', 'Train acc: 0.975000')\n",
      "('Epoch: 639/2000', 'Iteration: 3200', 'Train loss: 0.096579', 'Train acc: 0.971667')\n",
      "('Epoch: 639/2000', 'Iteration: 3200', 'Validation loss: 0.370139', 'Validation acc: 0.906667')\n",
      "('Epoch: 640/2000', 'Iteration: 3205', 'Train loss: 0.083760', 'Train acc: 0.973333')\n",
      "('Epoch: 641/2000', 'Iteration: 3210', 'Train loss: 0.100005', 'Train acc: 0.975000')\n",
      "('Epoch: 641/2000', 'Iteration: 3210', 'Validation loss: 0.374070', 'Validation acc: 0.906667')\n",
      "('Epoch: 642/2000', 'Iteration: 3215', 'Train loss: 0.081270', 'Train acc: 0.973333')\n",
      "('Epoch: 643/2000', 'Iteration: 3220', 'Train loss: 0.089025', 'Train acc: 0.975000')\n",
      "('Epoch: 643/2000', 'Iteration: 3220', 'Validation loss: 0.370487', 'Validation acc: 0.910000')\n",
      "('Epoch: 644/2000', 'Iteration: 3225', 'Train loss: 0.083778', 'Train acc: 0.976667')\n",
      "('Epoch: 645/2000', 'Iteration: 3230', 'Train loss: 0.089904', 'Train acc: 0.973333')\n",
      "('Epoch: 645/2000', 'Iteration: 3230', 'Validation loss: 0.369836', 'Validation acc: 0.903333')\n",
      "('Epoch: 646/2000', 'Iteration: 3235', 'Train loss: 0.088207', 'Train acc: 0.973333')\n",
      "('Epoch: 647/2000', 'Iteration: 3240', 'Train loss: 0.091978', 'Train acc: 0.980000')\n",
      "('Epoch: 647/2000', 'Iteration: 3240', 'Validation loss: 0.369358', 'Validation acc: 0.906667')\n",
      "('Epoch: 648/2000', 'Iteration: 3245', 'Train loss: 0.076200', 'Train acc: 0.976667')\n",
      "('Epoch: 649/2000', 'Iteration: 3250', 'Train loss: 0.088550', 'Train acc: 0.976667')\n",
      "('Epoch: 649/2000', 'Iteration: 3250', 'Validation loss: 0.368918', 'Validation acc: 0.906667')\n",
      "('Epoch: 650/2000', 'Iteration: 3255', 'Train loss: 0.087211', 'Train acc: 0.973333')\n",
      "('Epoch: 651/2000', 'Iteration: 3260', 'Train loss: 0.076119', 'Train acc: 0.978333')\n",
      "('Epoch: 651/2000', 'Iteration: 3260', 'Validation loss: 0.372793', 'Validation acc: 0.906667')\n",
      "('Epoch: 652/2000', 'Iteration: 3265', 'Train loss: 0.098555', 'Train acc: 0.965000')\n",
      "('Epoch: 653/2000', 'Iteration: 3270', 'Train loss: 0.087442', 'Train acc: 0.978333')\n",
      "('Epoch: 653/2000', 'Iteration: 3270', 'Validation loss: 0.373830', 'Validation acc: 0.906667')\n",
      "('Epoch: 654/2000', 'Iteration: 3275', 'Train loss: 0.095920', 'Train acc: 0.973333')\n",
      "('Epoch: 655/2000', 'Iteration: 3280', 'Train loss: 0.079365', 'Train acc: 0.976667')\n",
      "('Epoch: 655/2000', 'Iteration: 3280', 'Validation loss: 0.372069', 'Validation acc: 0.910000')\n",
      "('Epoch: 656/2000', 'Iteration: 3285', 'Train loss: 0.074651', 'Train acc: 0.981667')\n",
      "('Epoch: 657/2000', 'Iteration: 3290', 'Train loss: 0.081527', 'Train acc: 0.978333')\n",
      "('Epoch: 657/2000', 'Iteration: 3290', 'Validation loss: 0.368312', 'Validation acc: 0.908333')\n",
      "('Epoch: 658/2000', 'Iteration: 3295', 'Train loss: 0.086205', 'Train acc: 0.971667')\n",
      "('Epoch: 659/2000', 'Iteration: 3300', 'Train loss: 0.081644', 'Train acc: 0.980000')\n",
      "('Epoch: 659/2000', 'Iteration: 3300', 'Validation loss: 0.368037', 'Validation acc: 0.906667')\n",
      "('Epoch: 660/2000', 'Iteration: 3305', 'Train loss: 0.095618', 'Train acc: 0.978333')\n",
      "('Epoch: 661/2000', 'Iteration: 3310', 'Train loss: 0.073057', 'Train acc: 0.983333')\n",
      "('Epoch: 661/2000', 'Iteration: 3310', 'Validation loss: 0.367575', 'Validation acc: 0.908333')\n",
      "('Epoch: 662/2000', 'Iteration: 3315', 'Train loss: 0.077455', 'Train acc: 0.981667')\n",
      "('Epoch: 663/2000', 'Iteration: 3320', 'Train loss: 0.085442', 'Train acc: 0.978333')\n",
      "('Epoch: 663/2000', 'Iteration: 3320', 'Validation loss: 0.370890', 'Validation acc: 0.908333')\n",
      "('Epoch: 664/2000', 'Iteration: 3325', 'Train loss: 0.088915', 'Train acc: 0.975000')\n",
      "('Epoch: 665/2000', 'Iteration: 3330', 'Train loss: 0.089323', 'Train acc: 0.970000')\n",
      "('Epoch: 665/2000', 'Iteration: 3330', 'Validation loss: 0.366389', 'Validation acc: 0.911667')\n",
      "('Epoch: 666/2000', 'Iteration: 3335', 'Train loss: 0.067305', 'Train acc: 0.985000')\n",
      "('Epoch: 667/2000', 'Iteration: 3340', 'Train loss: 0.094175', 'Train acc: 0.975000')\n",
      "('Epoch: 667/2000', 'Iteration: 3340', 'Validation loss: 0.372500', 'Validation acc: 0.910000')\n",
      "('Epoch: 668/2000', 'Iteration: 3345', 'Train loss: 0.086553', 'Train acc: 0.978333')\n",
      "('Epoch: 669/2000', 'Iteration: 3350', 'Train loss: 0.083476', 'Train acc: 0.978333')\n",
      "('Epoch: 669/2000', 'Iteration: 3350', 'Validation loss: 0.375542', 'Validation acc: 0.906667')\n",
      "('Epoch: 670/2000', 'Iteration: 3355', 'Train loss: 0.088073', 'Train acc: 0.975000')\n",
      "('Epoch: 671/2000', 'Iteration: 3360', 'Train loss: 0.096391', 'Train acc: 0.968333')\n",
      "('Epoch: 671/2000', 'Iteration: 3360', 'Validation loss: 0.370331', 'Validation acc: 0.908333')\n",
      "('Epoch: 672/2000', 'Iteration: 3365', 'Train loss: 0.077183', 'Train acc: 0.981667')\n",
      "('Epoch: 673/2000', 'Iteration: 3370', 'Train loss: 0.078676', 'Train acc: 0.981667')\n",
      "('Epoch: 673/2000', 'Iteration: 3370', 'Validation loss: 0.367947', 'Validation acc: 0.910000')\n",
      "('Epoch: 674/2000', 'Iteration: 3375', 'Train loss: 0.070064', 'Train acc: 0.983333')\n",
      "('Epoch: 675/2000', 'Iteration: 3380', 'Train loss: 0.076773', 'Train acc: 0.981667')\n",
      "('Epoch: 675/2000', 'Iteration: 3380', 'Validation loss: 0.369855', 'Validation acc: 0.911667')\n",
      "('Epoch: 676/2000', 'Iteration: 3385', 'Train loss: 0.089435', 'Train acc: 0.978333')\n",
      "('Epoch: 677/2000', 'Iteration: 3390', 'Train loss: 0.086897', 'Train acc: 0.968333')\n",
      "('Epoch: 677/2000', 'Iteration: 3390', 'Validation loss: 0.371768', 'Validation acc: 0.915000')\n",
      "('Epoch: 678/2000', 'Iteration: 3395', 'Train loss: 0.076810', 'Train acc: 0.975000')\n",
      "('Epoch: 679/2000', 'Iteration: 3400', 'Train loss: 0.088236', 'Train acc: 0.978333')\n",
      "('Epoch: 679/2000', 'Iteration: 3400', 'Validation loss: 0.372766', 'Validation acc: 0.910000')\n",
      "('Epoch: 680/2000', 'Iteration: 3405', 'Train loss: 0.091732', 'Train acc: 0.976667')\n",
      "('Epoch: 681/2000', 'Iteration: 3410', 'Train loss: 0.075804', 'Train acc: 0.980000')\n",
      "('Epoch: 681/2000', 'Iteration: 3410', 'Validation loss: 0.371246', 'Validation acc: 0.913333')\n",
      "('Epoch: 682/2000', 'Iteration: 3415', 'Train loss: 0.073716', 'Train acc: 0.990000')\n",
      "('Epoch: 683/2000', 'Iteration: 3420', 'Train loss: 0.072151', 'Train acc: 0.978333')\n",
      "('Epoch: 683/2000', 'Iteration: 3420', 'Validation loss: 0.374106', 'Validation acc: 0.905000')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 684/2000', 'Iteration: 3425', 'Train loss: 0.063379', 'Train acc: 0.980000')\n",
      "('Epoch: 685/2000', 'Iteration: 3430', 'Train loss: 0.076648', 'Train acc: 0.971667')\n",
      "('Epoch: 685/2000', 'Iteration: 3430', 'Validation loss: 0.375211', 'Validation acc: 0.911667')\n",
      "('Epoch: 686/2000', 'Iteration: 3435', 'Train loss: 0.065561', 'Train acc: 0.986667')\n",
      "('Epoch: 687/2000', 'Iteration: 3440', 'Train loss: 0.069621', 'Train acc: 0.983333')\n",
      "('Epoch: 687/2000', 'Iteration: 3440', 'Validation loss: 0.368026', 'Validation acc: 0.915000')\n",
      "('Epoch: 688/2000', 'Iteration: 3445', 'Train loss: 0.071907', 'Train acc: 0.983333')\n",
      "('Epoch: 689/2000', 'Iteration: 3450', 'Train loss: 0.075876', 'Train acc: 0.973333')\n",
      "('Epoch: 689/2000', 'Iteration: 3450', 'Validation loss: 0.371244', 'Validation acc: 0.910000')\n",
      "('Epoch: 690/2000', 'Iteration: 3455', 'Train loss: 0.067329', 'Train acc: 0.983333')\n",
      "('Epoch: 691/2000', 'Iteration: 3460', 'Train loss: 0.090080', 'Train acc: 0.963333')\n",
      "('Epoch: 691/2000', 'Iteration: 3460', 'Validation loss: 0.371068', 'Validation acc: 0.915000')\n",
      "('Epoch: 692/2000', 'Iteration: 3465', 'Train loss: 0.074149', 'Train acc: 0.978333')\n",
      "('Epoch: 693/2000', 'Iteration: 3470', 'Train loss: 0.077637', 'Train acc: 0.980000')\n",
      "('Epoch: 693/2000', 'Iteration: 3470', 'Validation loss: 0.369100', 'Validation acc: 0.911667')\n",
      "('Epoch: 694/2000', 'Iteration: 3475', 'Train loss: 0.085833', 'Train acc: 0.973333')\n",
      "('Epoch: 695/2000', 'Iteration: 3480', 'Train loss: 0.082322', 'Train acc: 0.975000')\n",
      "('Epoch: 695/2000', 'Iteration: 3480', 'Validation loss: 0.369215', 'Validation acc: 0.916667')\n",
      "('Epoch: 696/2000', 'Iteration: 3485', 'Train loss: 0.072807', 'Train acc: 0.980000')\n",
      "('Epoch: 697/2000', 'Iteration: 3490', 'Train loss: 0.073999', 'Train acc: 0.985000')\n",
      "('Epoch: 697/2000', 'Iteration: 3490', 'Validation loss: 0.372362', 'Validation acc: 0.911667')\n",
      "('Epoch: 698/2000', 'Iteration: 3495', 'Train loss: 0.081558', 'Train acc: 0.978333')\n",
      "('Epoch: 699/2000', 'Iteration: 3500', 'Train loss: 0.072454', 'Train acc: 0.981667')\n",
      "('Epoch: 699/2000', 'Iteration: 3500', 'Validation loss: 0.365506', 'Validation acc: 0.910000')\n",
      "('Epoch: 700/2000', 'Iteration: 3505', 'Train loss: 0.067627', 'Train acc: 0.985000')\n",
      "('Epoch: 701/2000', 'Iteration: 3510', 'Train loss: 0.068876', 'Train acc: 0.981667')\n",
      "('Epoch: 701/2000', 'Iteration: 3510', 'Validation loss: 0.371381', 'Validation acc: 0.908333')\n",
      "('Epoch: 702/2000', 'Iteration: 3515', 'Train loss: 0.071957', 'Train acc: 0.975000')\n",
      "('Epoch: 703/2000', 'Iteration: 3520', 'Train loss: 0.081639', 'Train acc: 0.983333')\n",
      "('Epoch: 703/2000', 'Iteration: 3520', 'Validation loss: 0.372089', 'Validation acc: 0.913333')\n",
      "('Epoch: 704/2000', 'Iteration: 3525', 'Train loss: 0.073498', 'Train acc: 0.981667')\n",
      "('Epoch: 705/2000', 'Iteration: 3530', 'Train loss: 0.086470', 'Train acc: 0.978333')\n",
      "('Epoch: 705/2000', 'Iteration: 3530', 'Validation loss: 0.374502', 'Validation acc: 0.910000')\n",
      "('Epoch: 706/2000', 'Iteration: 3535', 'Train loss: 0.082293', 'Train acc: 0.978333')\n",
      "('Epoch: 707/2000', 'Iteration: 3540', 'Train loss: 0.083672', 'Train acc: 0.983333')\n",
      "('Epoch: 707/2000', 'Iteration: 3540', 'Validation loss: 0.371290', 'Validation acc: 0.918333')\n",
      "('Epoch: 708/2000', 'Iteration: 3545', 'Train loss: 0.070115', 'Train acc: 0.981667')\n",
      "('Epoch: 709/2000', 'Iteration: 3550', 'Train loss: 0.075546', 'Train acc: 0.975000')\n",
      "('Epoch: 709/2000', 'Iteration: 3550', 'Validation loss: 0.376432', 'Validation acc: 0.910000')\n",
      "('Epoch: 710/2000', 'Iteration: 3555', 'Train loss: 0.086606', 'Train acc: 0.973333')\n",
      "('Epoch: 711/2000', 'Iteration: 3560', 'Train loss: 0.076572', 'Train acc: 0.975000')\n",
      "('Epoch: 711/2000', 'Iteration: 3560', 'Validation loss: 0.371002', 'Validation acc: 0.911667')\n",
      "('Epoch: 712/2000', 'Iteration: 3565', 'Train loss: 0.082822', 'Train acc: 0.978333')\n",
      "('Epoch: 713/2000', 'Iteration: 3570', 'Train loss: 0.076156', 'Train acc: 0.981667')\n",
      "('Epoch: 713/2000', 'Iteration: 3570', 'Validation loss: 0.376247', 'Validation acc: 0.908333')\n",
      "('Epoch: 714/2000', 'Iteration: 3575', 'Train loss: 0.077522', 'Train acc: 0.975000')\n",
      "('Epoch: 715/2000', 'Iteration: 3580', 'Train loss: 0.083775', 'Train acc: 0.970000')\n",
      "('Epoch: 715/2000', 'Iteration: 3580', 'Validation loss: 0.373072', 'Validation acc: 0.913333')\n",
      "('Epoch: 716/2000', 'Iteration: 3585', 'Train loss: 0.063394', 'Train acc: 0.991667')\n",
      "('Epoch: 717/2000', 'Iteration: 3590', 'Train loss: 0.080052', 'Train acc: 0.968333')\n",
      "('Epoch: 717/2000', 'Iteration: 3590', 'Validation loss: 0.373419', 'Validation acc: 0.915000')\n",
      "('Epoch: 718/2000', 'Iteration: 3595', 'Train loss: 0.066915', 'Train acc: 0.980000')\n",
      "('Epoch: 719/2000', 'Iteration: 3600', 'Train loss: 0.064753', 'Train acc: 0.981667')\n",
      "('Epoch: 719/2000', 'Iteration: 3600', 'Validation loss: 0.376807', 'Validation acc: 0.911667')\n",
      "('Epoch: 720/2000', 'Iteration: 3605', 'Train loss: 0.072482', 'Train acc: 0.983333')\n",
      "('Epoch: 721/2000', 'Iteration: 3610', 'Train loss: 0.065166', 'Train acc: 0.983333')\n",
      "('Epoch: 721/2000', 'Iteration: 3610', 'Validation loss: 0.373501', 'Validation acc: 0.911667')\n",
      "('Epoch: 722/2000', 'Iteration: 3615', 'Train loss: 0.063527', 'Train acc: 0.983333')\n",
      "('Epoch: 723/2000', 'Iteration: 3620', 'Train loss: 0.065296', 'Train acc: 0.980000')\n",
      "('Epoch: 723/2000', 'Iteration: 3620', 'Validation loss: 0.369799', 'Validation acc: 0.910000')\n",
      "('Epoch: 724/2000', 'Iteration: 3625', 'Train loss: 0.066296', 'Train acc: 0.981667')\n",
      "('Epoch: 725/2000', 'Iteration: 3630', 'Train loss: 0.078879', 'Train acc: 0.976667')\n",
      "('Epoch: 725/2000', 'Iteration: 3630', 'Validation loss: 0.370392', 'Validation acc: 0.916667')\n",
      "('Epoch: 726/2000', 'Iteration: 3635', 'Train loss: 0.070462', 'Train acc: 0.976667')\n",
      "('Epoch: 727/2000', 'Iteration: 3640', 'Train loss: 0.079814', 'Train acc: 0.973333')\n",
      "('Epoch: 727/2000', 'Iteration: 3640', 'Validation loss: 0.371794', 'Validation acc: 0.910000')\n",
      "('Epoch: 728/2000', 'Iteration: 3645', 'Train loss: 0.074559', 'Train acc: 0.978333')\n",
      "('Epoch: 729/2000', 'Iteration: 3650', 'Train loss: 0.075944', 'Train acc: 0.976667')\n",
      "('Epoch: 729/2000', 'Iteration: 3650', 'Validation loss: 0.374509', 'Validation acc: 0.910000')\n",
      "('Epoch: 730/2000', 'Iteration: 3655', 'Train loss: 0.079282', 'Train acc: 0.978333')\n",
      "('Epoch: 731/2000', 'Iteration: 3660', 'Train loss: 0.078711', 'Train acc: 0.973333')\n",
      "('Epoch: 731/2000', 'Iteration: 3660', 'Validation loss: 0.375875', 'Validation acc: 0.911667')\n",
      "('Epoch: 732/2000', 'Iteration: 3665', 'Train loss: 0.079562', 'Train acc: 0.973333')\n",
      "('Epoch: 733/2000', 'Iteration: 3670', 'Train loss: 0.075391', 'Train acc: 0.978333')\n",
      "('Epoch: 733/2000', 'Iteration: 3670', 'Validation loss: 0.368926', 'Validation acc: 0.915000')\n",
      "('Epoch: 734/2000', 'Iteration: 3675', 'Train loss: 0.068774', 'Train acc: 0.980000')\n",
      "('Epoch: 735/2000', 'Iteration: 3680', 'Train loss: 0.070901', 'Train acc: 0.983333')\n",
      "('Epoch: 735/2000', 'Iteration: 3680', 'Validation loss: 0.368773', 'Validation acc: 0.911667')\n",
      "('Epoch: 736/2000', 'Iteration: 3685', 'Train loss: 0.065950', 'Train acc: 0.985000')\n",
      "('Epoch: 737/2000', 'Iteration: 3690', 'Train loss: 0.072312', 'Train acc: 0.978333')\n",
      "('Epoch: 737/2000', 'Iteration: 3690', 'Validation loss: 0.374786', 'Validation acc: 0.913333')\n",
      "('Epoch: 738/2000', 'Iteration: 3695', 'Train loss: 0.075057', 'Train acc: 0.980000')\n",
      "('Epoch: 739/2000', 'Iteration: 3700', 'Train loss: 0.072137', 'Train acc: 0.983333')\n",
      "('Epoch: 739/2000', 'Iteration: 3700', 'Validation loss: 0.374574', 'Validation acc: 0.911667')\n",
      "('Epoch: 740/2000', 'Iteration: 3705', 'Train loss: 0.074293', 'Train acc: 0.980000')\n",
      "('Epoch: 741/2000', 'Iteration: 3710', 'Train loss: 0.086971', 'Train acc: 0.978333')\n",
      "('Epoch: 741/2000', 'Iteration: 3710', 'Validation loss: 0.372333', 'Validation acc: 0.915000')\n",
      "('Epoch: 742/2000', 'Iteration: 3715', 'Train loss: 0.073125', 'Train acc: 0.981667')\n",
      "('Epoch: 743/2000', 'Iteration: 3720', 'Train loss: 0.070404', 'Train acc: 0.975000')\n",
      "('Epoch: 743/2000', 'Iteration: 3720', 'Validation loss: 0.371573', 'Validation acc: 0.908333')\n",
      "('Epoch: 744/2000', 'Iteration: 3725', 'Train loss: 0.062083', 'Train acc: 0.985000')\n",
      "('Epoch: 745/2000', 'Iteration: 3730', 'Train loss: 0.065623', 'Train acc: 0.978333')\n",
      "('Epoch: 745/2000', 'Iteration: 3730', 'Validation loss: 0.370940', 'Validation acc: 0.911667')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 746/2000', 'Iteration: 3735', 'Train loss: 0.067345', 'Train acc: 0.981667')\n",
      "('Epoch: 747/2000', 'Iteration: 3740', 'Train loss: 0.060906', 'Train acc: 0.985000')\n",
      "('Epoch: 747/2000', 'Iteration: 3740', 'Validation loss: 0.368232', 'Validation acc: 0.916667')\n",
      "('Epoch: 748/2000', 'Iteration: 3745', 'Train loss: 0.078319', 'Train acc: 0.976667')\n",
      "('Epoch: 749/2000', 'Iteration: 3750', 'Train loss: 0.062511', 'Train acc: 0.981667')\n",
      "('Epoch: 749/2000', 'Iteration: 3750', 'Validation loss: 0.371364', 'Validation acc: 0.916667')\n",
      "('Epoch: 750/2000', 'Iteration: 3755', 'Train loss: 0.069362', 'Train acc: 0.980000')\n",
      "('Epoch: 751/2000', 'Iteration: 3760', 'Train loss: 0.057862', 'Train acc: 0.986667')\n",
      "('Epoch: 751/2000', 'Iteration: 3760', 'Validation loss: 0.379221', 'Validation acc: 0.913333')\n",
      "('Epoch: 752/2000', 'Iteration: 3765', 'Train loss: 0.069792', 'Train acc: 0.975000')\n",
      "('Epoch: 753/2000', 'Iteration: 3770', 'Train loss: 0.076074', 'Train acc: 0.976667')\n",
      "('Epoch: 753/2000', 'Iteration: 3770', 'Validation loss: 0.380794', 'Validation acc: 0.908333')\n",
      "('Epoch: 754/2000', 'Iteration: 3775', 'Train loss: 0.059251', 'Train acc: 0.983333')\n",
      "('Epoch: 755/2000', 'Iteration: 3780', 'Train loss: 0.053170', 'Train acc: 0.991667')\n",
      "('Epoch: 755/2000', 'Iteration: 3780', 'Validation loss: 0.375427', 'Validation acc: 0.916667')\n",
      "('Epoch: 756/2000', 'Iteration: 3785', 'Train loss: 0.073795', 'Train acc: 0.981667')\n",
      "('Epoch: 757/2000', 'Iteration: 3790', 'Train loss: 0.058130', 'Train acc: 0.986667')\n",
      "('Epoch: 757/2000', 'Iteration: 3790', 'Validation loss: 0.374585', 'Validation acc: 0.915000')\n",
      "('Epoch: 758/2000', 'Iteration: 3795', 'Train loss: 0.067441', 'Train acc: 0.986667')\n",
      "('Epoch: 759/2000', 'Iteration: 3800', 'Train loss: 0.068875', 'Train acc: 0.973333')\n",
      "('Epoch: 759/2000', 'Iteration: 3800', 'Validation loss: 0.370656', 'Validation acc: 0.913333')\n",
      "('Epoch: 760/2000', 'Iteration: 3805', 'Train loss: 0.067499', 'Train acc: 0.980000')\n",
      "('Epoch: 761/2000', 'Iteration: 3810', 'Train loss: 0.073513', 'Train acc: 0.978333')\n",
      "('Epoch: 761/2000', 'Iteration: 3810', 'Validation loss: 0.377015', 'Validation acc: 0.911667')\n",
      "('Epoch: 762/2000', 'Iteration: 3815', 'Train loss: 0.065350', 'Train acc: 0.981667')\n",
      "('Epoch: 763/2000', 'Iteration: 3820', 'Train loss: 0.069477', 'Train acc: 0.978333')\n",
      "('Epoch: 763/2000', 'Iteration: 3820', 'Validation loss: 0.377328', 'Validation acc: 0.911667')\n",
      "('Epoch: 764/2000', 'Iteration: 3825', 'Train loss: 0.072168', 'Train acc: 0.978333')\n",
      "('Epoch: 765/2000', 'Iteration: 3830', 'Train loss: 0.072874', 'Train acc: 0.973333')\n",
      "('Epoch: 765/2000', 'Iteration: 3830', 'Validation loss: 0.378791', 'Validation acc: 0.913333')\n",
      "('Epoch: 766/2000', 'Iteration: 3835', 'Train loss: 0.062312', 'Train acc: 0.988333')\n",
      "('Epoch: 767/2000', 'Iteration: 3840', 'Train loss: 0.063955', 'Train acc: 0.985000')\n",
      "('Epoch: 767/2000', 'Iteration: 3840', 'Validation loss: 0.376718', 'Validation acc: 0.913333')\n",
      "('Epoch: 768/2000', 'Iteration: 3845', 'Train loss: 0.064351', 'Train acc: 0.985000')\n",
      "('Epoch: 769/2000', 'Iteration: 3850', 'Train loss: 0.072492', 'Train acc: 0.975000')\n",
      "('Epoch: 769/2000', 'Iteration: 3850', 'Validation loss: 0.377488', 'Validation acc: 0.913333')\n",
      "('Epoch: 770/2000', 'Iteration: 3855', 'Train loss: 0.070229', 'Train acc: 0.981667')\n",
      "('Epoch: 771/2000', 'Iteration: 3860', 'Train loss: 0.063674', 'Train acc: 0.976667')\n",
      "('Epoch: 771/2000', 'Iteration: 3860', 'Validation loss: 0.376639', 'Validation acc: 0.913333')\n",
      "('Epoch: 772/2000', 'Iteration: 3865', 'Train loss: 0.067609', 'Train acc: 0.983333')\n",
      "('Epoch: 773/2000', 'Iteration: 3870', 'Train loss: 0.067174', 'Train acc: 0.978333')\n",
      "('Epoch: 773/2000', 'Iteration: 3870', 'Validation loss: 0.375913', 'Validation acc: 0.915000')\n",
      "('Epoch: 774/2000', 'Iteration: 3875', 'Train loss: 0.056114', 'Train acc: 0.980000')\n",
      "('Epoch: 775/2000', 'Iteration: 3880', 'Train loss: 0.059853', 'Train acc: 0.983333')\n",
      "('Epoch: 775/2000', 'Iteration: 3880', 'Validation loss: 0.376504', 'Validation acc: 0.916667')\n",
      "('Epoch: 776/2000', 'Iteration: 3885', 'Train loss: 0.062611', 'Train acc: 0.981667')\n",
      "('Epoch: 777/2000', 'Iteration: 3890', 'Train loss: 0.054007', 'Train acc: 0.988333')\n",
      "('Epoch: 777/2000', 'Iteration: 3890', 'Validation loss: 0.374031', 'Validation acc: 0.911667')\n",
      "('Epoch: 778/2000', 'Iteration: 3895', 'Train loss: 0.061769', 'Train acc: 0.976667')\n",
      "('Epoch: 779/2000', 'Iteration: 3900', 'Train loss: 0.061179', 'Train acc: 0.983333')\n",
      "('Epoch: 779/2000', 'Iteration: 3900', 'Validation loss: 0.373542', 'Validation acc: 0.918333')\n",
      "('Epoch: 780/2000', 'Iteration: 3905', 'Train loss: 0.055900', 'Train acc: 0.986667')\n",
      "('Epoch: 781/2000', 'Iteration: 3910', 'Train loss: 0.069848', 'Train acc: 0.983333')\n",
      "('Epoch: 781/2000', 'Iteration: 3910', 'Validation loss: 0.381282', 'Validation acc: 0.913333')\n",
      "('Epoch: 782/2000', 'Iteration: 3915', 'Train loss: 0.056770', 'Train acc: 0.983333')\n",
      "('Epoch: 783/2000', 'Iteration: 3920', 'Train loss: 0.087706', 'Train acc: 0.976667')\n",
      "('Epoch: 783/2000', 'Iteration: 3920', 'Validation loss: 0.380931', 'Validation acc: 0.908333')\n",
      "('Epoch: 784/2000', 'Iteration: 3925', 'Train loss: 0.070754', 'Train acc: 0.978333')\n",
      "('Epoch: 785/2000', 'Iteration: 3930', 'Train loss: 0.055026', 'Train acc: 0.986667')\n",
      "('Epoch: 785/2000', 'Iteration: 3930', 'Validation loss: 0.377232', 'Validation acc: 0.916667')\n",
      "('Epoch: 786/2000', 'Iteration: 3935', 'Train loss: 0.060719', 'Train acc: 0.985000')\n",
      "('Epoch: 787/2000', 'Iteration: 3940', 'Train loss: 0.055359', 'Train acc: 0.985000')\n",
      "('Epoch: 787/2000', 'Iteration: 3940', 'Validation loss: 0.375939', 'Validation acc: 0.910000')\n",
      "('Epoch: 788/2000', 'Iteration: 3945', 'Train loss: 0.060537', 'Train acc: 0.980000')\n",
      "('Epoch: 789/2000', 'Iteration: 3950', 'Train loss: 0.062151', 'Train acc: 0.981667')\n",
      "('Epoch: 789/2000', 'Iteration: 3950', 'Validation loss: 0.377724', 'Validation acc: 0.911667')\n",
      "('Epoch: 790/2000', 'Iteration: 3955', 'Train loss: 0.064139', 'Train acc: 0.988333')\n",
      "('Epoch: 791/2000', 'Iteration: 3960', 'Train loss: 0.074612', 'Train acc: 0.981667')\n",
      "('Epoch: 791/2000', 'Iteration: 3960', 'Validation loss: 0.380508', 'Validation acc: 0.911667')\n",
      "('Epoch: 792/2000', 'Iteration: 3965', 'Train loss: 0.059625', 'Train acc: 0.986667')\n",
      "('Epoch: 793/2000', 'Iteration: 3970', 'Train loss: 0.059629', 'Train acc: 0.983333')\n",
      "('Epoch: 793/2000', 'Iteration: 3970', 'Validation loss: 0.379543', 'Validation acc: 0.916667')\n",
      "('Epoch: 794/2000', 'Iteration: 3975', 'Train loss: 0.055722', 'Train acc: 0.985000')\n",
      "('Epoch: 795/2000', 'Iteration: 3980', 'Train loss: 0.066894', 'Train acc: 0.981667')\n",
      "('Epoch: 795/2000', 'Iteration: 3980', 'Validation loss: 0.381878', 'Validation acc: 0.910000')\n",
      "('Epoch: 796/2000', 'Iteration: 3985', 'Train loss: 0.056412', 'Train acc: 0.981667')\n",
      "('Epoch: 797/2000', 'Iteration: 3990', 'Train loss: 0.057963', 'Train acc: 0.983333')\n",
      "('Epoch: 797/2000', 'Iteration: 3990', 'Validation loss: 0.379614', 'Validation acc: 0.913333')\n",
      "('Epoch: 798/2000', 'Iteration: 3995', 'Train loss: 0.053978', 'Train acc: 0.988333')\n",
      "('Epoch: 799/2000', 'Iteration: 4000', 'Train loss: 0.058352', 'Train acc: 0.985000')\n",
      "('Epoch: 799/2000', 'Iteration: 4000', 'Validation loss: 0.383649', 'Validation acc: 0.910000')\n",
      "('Epoch: 800/2000', 'Iteration: 4005', 'Train loss: 0.055219', 'Train acc: 0.981667')\n",
      "('Epoch: 801/2000', 'Iteration: 4010', 'Train loss: 0.049259', 'Train acc: 0.990000')\n",
      "('Epoch: 801/2000', 'Iteration: 4010', 'Validation loss: 0.383488', 'Validation acc: 0.915000')\n",
      "('Epoch: 802/2000', 'Iteration: 4015', 'Train loss: 0.046474', 'Train acc: 0.991667')\n",
      "('Epoch: 803/2000', 'Iteration: 4020', 'Train loss: 0.068885', 'Train acc: 0.981667')\n",
      "('Epoch: 803/2000', 'Iteration: 4020', 'Validation loss: 0.385910', 'Validation acc: 0.911667')\n",
      "('Epoch: 804/2000', 'Iteration: 4025', 'Train loss: 0.059471', 'Train acc: 0.981667')\n",
      "('Epoch: 805/2000', 'Iteration: 4030', 'Train loss: 0.070066', 'Train acc: 0.976667')\n",
      "('Epoch: 805/2000', 'Iteration: 4030', 'Validation loss: 0.379284', 'Validation acc: 0.916667')\n",
      "('Epoch: 806/2000', 'Iteration: 4035', 'Train loss: 0.060097', 'Train acc: 0.986667')\n",
      "('Epoch: 807/2000', 'Iteration: 4040', 'Train loss: 0.053961', 'Train acc: 0.985000')\n",
      "('Epoch: 807/2000', 'Iteration: 4040', 'Validation loss: 0.380096', 'Validation acc: 0.913333')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 808/2000', 'Iteration: 4045', 'Train loss: 0.060342', 'Train acc: 0.983333')\n",
      "('Epoch: 809/2000', 'Iteration: 4050', 'Train loss: 0.068160', 'Train acc: 0.978333')\n",
      "('Epoch: 809/2000', 'Iteration: 4050', 'Validation loss: 0.380116', 'Validation acc: 0.918333')\n",
      "('Epoch: 810/2000', 'Iteration: 4055', 'Train loss: 0.065887', 'Train acc: 0.975000')\n",
      "('Epoch: 811/2000', 'Iteration: 4060', 'Train loss: 0.066398', 'Train acc: 0.981667')\n",
      "('Epoch: 811/2000', 'Iteration: 4060', 'Validation loss: 0.386576', 'Validation acc: 0.910000')\n",
      "('Epoch: 812/2000', 'Iteration: 4065', 'Train loss: 0.065174', 'Train acc: 0.985000')\n",
      "('Epoch: 813/2000', 'Iteration: 4070', 'Train loss: 0.046731', 'Train acc: 0.988333')\n",
      "('Epoch: 813/2000', 'Iteration: 4070', 'Validation loss: 0.383201', 'Validation acc: 0.916667')\n",
      "('Epoch: 814/2000', 'Iteration: 4075', 'Train loss: 0.049691', 'Train acc: 0.986667')\n",
      "('Epoch: 815/2000', 'Iteration: 4080', 'Train loss: 0.050913', 'Train acc: 0.985000')\n",
      "('Epoch: 815/2000', 'Iteration: 4080', 'Validation loss: 0.382071', 'Validation acc: 0.916667')\n",
      "('Epoch: 816/2000', 'Iteration: 4085', 'Train loss: 0.058935', 'Train acc: 0.978333')\n",
      "('Epoch: 817/2000', 'Iteration: 4090', 'Train loss: 0.055141', 'Train acc: 0.985000')\n",
      "('Epoch: 817/2000', 'Iteration: 4090', 'Validation loss: 0.382937', 'Validation acc: 0.918333')\n",
      "('Epoch: 818/2000', 'Iteration: 4095', 'Train loss: 0.056751', 'Train acc: 0.985000')\n",
      "('Epoch: 819/2000', 'Iteration: 4100', 'Train loss: 0.037557', 'Train acc: 0.993333')\n",
      "('Epoch: 819/2000', 'Iteration: 4100', 'Validation loss: 0.380592', 'Validation acc: 0.920000')\n",
      "('Epoch: 820/2000', 'Iteration: 4105', 'Train loss: 0.052597', 'Train acc: 0.991667')\n",
      "('Epoch: 821/2000', 'Iteration: 4110', 'Train loss: 0.056507', 'Train acc: 0.983333')\n",
      "('Epoch: 821/2000', 'Iteration: 4110', 'Validation loss: 0.383549', 'Validation acc: 0.915000')\n",
      "('Epoch: 822/2000', 'Iteration: 4115', 'Train loss: 0.059936', 'Train acc: 0.980000')\n",
      "('Epoch: 823/2000', 'Iteration: 4120', 'Train loss: 0.055527', 'Train acc: 0.981667')\n",
      "('Epoch: 823/2000', 'Iteration: 4120', 'Validation loss: 0.380335', 'Validation acc: 0.918333')\n",
      "('Epoch: 824/2000', 'Iteration: 4125', 'Train loss: 0.053245', 'Train acc: 0.993333')\n",
      "('Epoch: 825/2000', 'Iteration: 4130', 'Train loss: 0.057567', 'Train acc: 0.985000')\n",
      "('Epoch: 825/2000', 'Iteration: 4130', 'Validation loss: 0.384797', 'Validation acc: 0.906667')\n",
      "('Epoch: 826/2000', 'Iteration: 4135', 'Train loss: 0.059352', 'Train acc: 0.976667')\n",
      "('Epoch: 827/2000', 'Iteration: 4140', 'Train loss: 0.056261', 'Train acc: 0.986667')\n",
      "('Epoch: 827/2000', 'Iteration: 4140', 'Validation loss: 0.380926', 'Validation acc: 0.913333')\n",
      "('Epoch: 828/2000', 'Iteration: 4145', 'Train loss: 0.070573', 'Train acc: 0.978333')\n",
      "('Epoch: 829/2000', 'Iteration: 4150', 'Train loss: 0.056335', 'Train acc: 0.986667')\n",
      "('Epoch: 829/2000', 'Iteration: 4150', 'Validation loss: 0.379728', 'Validation acc: 0.916667')\n",
      "('Epoch: 830/2000', 'Iteration: 4155', 'Train loss: 0.067583', 'Train acc: 0.973333')\n",
      "('Epoch: 831/2000', 'Iteration: 4160', 'Train loss: 0.044097', 'Train acc: 0.985000')\n",
      "('Epoch: 831/2000', 'Iteration: 4160', 'Validation loss: 0.382321', 'Validation acc: 0.918333')\n",
      "('Epoch: 832/2000', 'Iteration: 4165', 'Train loss: 0.040671', 'Train acc: 0.990000')\n",
      "('Epoch: 833/2000', 'Iteration: 4170', 'Train loss: 0.057971', 'Train acc: 0.983333')\n",
      "('Epoch: 833/2000', 'Iteration: 4170', 'Validation loss: 0.380908', 'Validation acc: 0.915000')\n",
      "('Epoch: 834/2000', 'Iteration: 4175', 'Train loss: 0.057097', 'Train acc: 0.985000')\n",
      "('Epoch: 835/2000', 'Iteration: 4180', 'Train loss: 0.048763', 'Train acc: 0.983333')\n",
      "('Epoch: 835/2000', 'Iteration: 4180', 'Validation loss: 0.380044', 'Validation acc: 0.918333')\n",
      "('Epoch: 836/2000', 'Iteration: 4185', 'Train loss: 0.043760', 'Train acc: 0.993333')\n",
      "('Epoch: 837/2000', 'Iteration: 4190', 'Train loss: 0.041392', 'Train acc: 0.990000')\n",
      "('Epoch: 837/2000', 'Iteration: 4190', 'Validation loss: 0.380345', 'Validation acc: 0.915000')\n",
      "('Epoch: 838/2000', 'Iteration: 4195', 'Train loss: 0.051015', 'Train acc: 0.985000')\n",
      "('Epoch: 839/2000', 'Iteration: 4200', 'Train loss: 0.061891', 'Train acc: 0.980000')\n",
      "('Epoch: 839/2000', 'Iteration: 4200', 'Validation loss: 0.378006', 'Validation acc: 0.911667')\n",
      "('Epoch: 840/2000', 'Iteration: 4205', 'Train loss: 0.059394', 'Train acc: 0.980000')\n",
      "('Epoch: 841/2000', 'Iteration: 4210', 'Train loss: 0.052070', 'Train acc: 0.986667')\n",
      "('Epoch: 841/2000', 'Iteration: 4210', 'Validation loss: 0.382557', 'Validation acc: 0.915000')\n",
      "('Epoch: 842/2000', 'Iteration: 4215', 'Train loss: 0.055735', 'Train acc: 0.983333')\n",
      "('Epoch: 843/2000', 'Iteration: 4220', 'Train loss: 0.042372', 'Train acc: 0.990000')\n",
      "('Epoch: 843/2000', 'Iteration: 4220', 'Validation loss: 0.380116', 'Validation acc: 0.910000')\n",
      "('Epoch: 844/2000', 'Iteration: 4225', 'Train loss: 0.049050', 'Train acc: 0.990000')\n",
      "('Epoch: 845/2000', 'Iteration: 4230', 'Train loss: 0.050647', 'Train acc: 0.986667')\n",
      "('Epoch: 845/2000', 'Iteration: 4230', 'Validation loss: 0.382703', 'Validation acc: 0.910000')\n",
      "('Epoch: 846/2000', 'Iteration: 4235', 'Train loss: 0.056984', 'Train acc: 0.986667')\n",
      "('Epoch: 847/2000', 'Iteration: 4240', 'Train loss: 0.044921', 'Train acc: 0.993333')\n",
      "('Epoch: 847/2000', 'Iteration: 4240', 'Validation loss: 0.381787', 'Validation acc: 0.916667')\n",
      "('Epoch: 848/2000', 'Iteration: 4245', 'Train loss: 0.042975', 'Train acc: 0.993333')\n",
      "('Epoch: 849/2000', 'Iteration: 4250', 'Train loss: 0.052833', 'Train acc: 0.981667')\n",
      "('Epoch: 849/2000', 'Iteration: 4250', 'Validation loss: 0.381336', 'Validation acc: 0.916667')\n",
      "('Epoch: 850/2000', 'Iteration: 4255', 'Train loss: 0.047531', 'Train acc: 0.988333')\n",
      "('Epoch: 851/2000', 'Iteration: 4260', 'Train loss: 0.060825', 'Train acc: 0.980000')\n",
      "('Epoch: 851/2000', 'Iteration: 4260', 'Validation loss: 0.379794', 'Validation acc: 0.915000')\n",
      "('Epoch: 852/2000', 'Iteration: 4265', 'Train loss: 0.045970', 'Train acc: 0.988333')\n",
      "('Epoch: 853/2000', 'Iteration: 4270', 'Train loss: 0.055605', 'Train acc: 0.981667')\n",
      "('Epoch: 853/2000', 'Iteration: 4270', 'Validation loss: 0.383283', 'Validation acc: 0.913333')\n",
      "('Epoch: 854/2000', 'Iteration: 4275', 'Train loss: 0.058953', 'Train acc: 0.983333')\n",
      "('Epoch: 855/2000', 'Iteration: 4280', 'Train loss: 0.051707', 'Train acc: 0.986667')\n",
      "('Epoch: 855/2000', 'Iteration: 4280', 'Validation loss: 0.378215', 'Validation acc: 0.918333')\n",
      "('Epoch: 856/2000', 'Iteration: 4285', 'Train loss: 0.046213', 'Train acc: 0.986667')\n",
      "('Epoch: 857/2000', 'Iteration: 4290', 'Train loss: 0.042796', 'Train acc: 0.996667')\n",
      "('Epoch: 857/2000', 'Iteration: 4290', 'Validation loss: 0.381864', 'Validation acc: 0.916667')\n",
      "('Epoch: 858/2000', 'Iteration: 4295', 'Train loss: 0.044607', 'Train acc: 0.988333')\n",
      "('Epoch: 859/2000', 'Iteration: 4300', 'Train loss: 0.059994', 'Train acc: 0.983333')\n",
      "('Epoch: 859/2000', 'Iteration: 4300', 'Validation loss: 0.383789', 'Validation acc: 0.916667')\n",
      "('Epoch: 860/2000', 'Iteration: 4305', 'Train loss: 0.048564', 'Train acc: 0.986667')\n",
      "('Epoch: 861/2000', 'Iteration: 4310', 'Train loss: 0.055265', 'Train acc: 0.985000')\n",
      "('Epoch: 861/2000', 'Iteration: 4310', 'Validation loss: 0.388673', 'Validation acc: 0.911667')\n",
      "('Epoch: 862/2000', 'Iteration: 4315', 'Train loss: 0.050397', 'Train acc: 0.980000')\n",
      "('Epoch: 863/2000', 'Iteration: 4320', 'Train loss: 0.043047', 'Train acc: 0.986667')\n",
      "('Epoch: 863/2000', 'Iteration: 4320', 'Validation loss: 0.385405', 'Validation acc: 0.915000')\n",
      "('Epoch: 864/2000', 'Iteration: 4325', 'Train loss: 0.045889', 'Train acc: 0.990000')\n",
      "('Epoch: 865/2000', 'Iteration: 4330', 'Train loss: 0.048161', 'Train acc: 0.990000')\n",
      "('Epoch: 865/2000', 'Iteration: 4330', 'Validation loss: 0.386766', 'Validation acc: 0.911667')\n",
      "('Epoch: 866/2000', 'Iteration: 4335', 'Train loss: 0.040615', 'Train acc: 0.996667')\n",
      "('Epoch: 867/2000', 'Iteration: 4340', 'Train loss: 0.035683', 'Train acc: 0.993333')\n",
      "('Epoch: 867/2000', 'Iteration: 4340', 'Validation loss: 0.380285', 'Validation acc: 0.918333')\n",
      "('Epoch: 868/2000', 'Iteration: 4345', 'Train loss: 0.052495', 'Train acc: 0.986667')\n",
      "('Epoch: 869/2000', 'Iteration: 4350', 'Train loss: 0.056768', 'Train acc: 0.981667')\n",
      "('Epoch: 869/2000', 'Iteration: 4350', 'Validation loss: 0.389527', 'Validation acc: 0.913333')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 870/2000', 'Iteration: 4355', 'Train loss: 0.050729', 'Train acc: 0.983333')\n",
      "('Epoch: 871/2000', 'Iteration: 4360', 'Train loss: 0.048838', 'Train acc: 0.990000')\n",
      "('Epoch: 871/2000', 'Iteration: 4360', 'Validation loss: 0.388852', 'Validation acc: 0.920000')\n",
      "('Epoch: 872/2000', 'Iteration: 4365', 'Train loss: 0.048734', 'Train acc: 0.988333')\n",
      "('Epoch: 873/2000', 'Iteration: 4370', 'Train loss: 0.055476', 'Train acc: 0.985000')\n",
      "('Epoch: 873/2000', 'Iteration: 4370', 'Validation loss: 0.393558', 'Validation acc: 0.911667')\n",
      "('Epoch: 874/2000', 'Iteration: 4375', 'Train loss: 0.044622', 'Train acc: 0.990000')\n",
      "('Epoch: 875/2000', 'Iteration: 4380', 'Train loss: 0.045462', 'Train acc: 0.995000')\n",
      "('Epoch: 875/2000', 'Iteration: 4380', 'Validation loss: 0.382838', 'Validation acc: 0.916667')\n",
      "('Epoch: 876/2000', 'Iteration: 4385', 'Train loss: 0.041967', 'Train acc: 0.990000')\n",
      "('Epoch: 877/2000', 'Iteration: 4390', 'Train loss: 0.043862', 'Train acc: 0.988333')\n",
      "('Epoch: 877/2000', 'Iteration: 4390', 'Validation loss: 0.385374', 'Validation acc: 0.913333')\n",
      "('Epoch: 878/2000', 'Iteration: 4395', 'Train loss: 0.050159', 'Train acc: 0.988333')\n",
      "('Epoch: 879/2000', 'Iteration: 4400', 'Train loss: 0.054684', 'Train acc: 0.985000')\n",
      "('Epoch: 879/2000', 'Iteration: 4400', 'Validation loss: 0.388385', 'Validation acc: 0.916667')\n",
      "('Epoch: 880/2000', 'Iteration: 4405', 'Train loss: 0.048138', 'Train acc: 0.986667')\n",
      "('Epoch: 881/2000', 'Iteration: 4410', 'Train loss: 0.054782', 'Train acc: 0.985000')\n",
      "('Epoch: 881/2000', 'Iteration: 4410', 'Validation loss: 0.385600', 'Validation acc: 0.920000')\n",
      "('Epoch: 882/2000', 'Iteration: 4415', 'Train loss: 0.043356', 'Train acc: 0.990000')\n",
      "('Epoch: 883/2000', 'Iteration: 4420', 'Train loss: 0.051953', 'Train acc: 0.980000')\n",
      "('Epoch: 883/2000', 'Iteration: 4420', 'Validation loss: 0.387995', 'Validation acc: 0.913333')\n",
      "('Epoch: 884/2000', 'Iteration: 4425', 'Train loss: 0.043683', 'Train acc: 0.981667')\n",
      "('Epoch: 885/2000', 'Iteration: 4430', 'Train loss: 0.040945', 'Train acc: 0.991667')\n",
      "('Epoch: 885/2000', 'Iteration: 4430', 'Validation loss: 0.379916', 'Validation acc: 0.918333')\n",
      "('Epoch: 886/2000', 'Iteration: 4435', 'Train loss: 0.048427', 'Train acc: 0.991667')\n",
      "('Epoch: 887/2000', 'Iteration: 4440', 'Train loss: 0.045844', 'Train acc: 0.991667')\n",
      "('Epoch: 887/2000', 'Iteration: 4440', 'Validation loss: 0.382702', 'Validation acc: 0.918333')\n",
      "('Epoch: 888/2000', 'Iteration: 4445', 'Train loss: 0.046923', 'Train acc: 0.985000')\n",
      "('Epoch: 889/2000', 'Iteration: 4450', 'Train loss: 0.048764', 'Train acc: 0.988333')\n",
      "('Epoch: 889/2000', 'Iteration: 4450', 'Validation loss: 0.385760', 'Validation acc: 0.911667')\n",
      "('Epoch: 890/2000', 'Iteration: 4455', 'Train loss: 0.035632', 'Train acc: 0.993333')\n",
      "('Epoch: 891/2000', 'Iteration: 4460', 'Train loss: 0.046827', 'Train acc: 0.986667')\n",
      "('Epoch: 891/2000', 'Iteration: 4460', 'Validation loss: 0.385646', 'Validation acc: 0.916667')\n",
      "('Epoch: 892/2000', 'Iteration: 4465', 'Train loss: 0.044886', 'Train acc: 0.990000')\n",
      "('Epoch: 893/2000', 'Iteration: 4470', 'Train loss: 0.048186', 'Train acc: 0.983333')\n",
      "('Epoch: 893/2000', 'Iteration: 4470', 'Validation loss: 0.384660', 'Validation acc: 0.915000')\n",
      "('Epoch: 894/2000', 'Iteration: 4475', 'Train loss: 0.042003', 'Train acc: 0.990000')\n",
      "('Epoch: 895/2000', 'Iteration: 4480', 'Train loss: 0.041209', 'Train acc: 0.990000')\n",
      "('Epoch: 895/2000', 'Iteration: 4480', 'Validation loss: 0.389223', 'Validation acc: 0.915000')\n",
      "('Epoch: 896/2000', 'Iteration: 4485', 'Train loss: 0.041605', 'Train acc: 0.986667')\n",
      "('Epoch: 897/2000', 'Iteration: 4490', 'Train loss: 0.046483', 'Train acc: 0.986667')\n",
      "('Epoch: 897/2000', 'Iteration: 4490', 'Validation loss: 0.390062', 'Validation acc: 0.920000')\n",
      "('Epoch: 898/2000', 'Iteration: 4495', 'Train loss: 0.062260', 'Train acc: 0.980000')\n",
      "('Epoch: 899/2000', 'Iteration: 4500', 'Train loss: 0.060062', 'Train acc: 0.983333')\n",
      "('Epoch: 899/2000', 'Iteration: 4500', 'Validation loss: 0.387791', 'Validation acc: 0.918333')\n",
      "('Epoch: 900/2000', 'Iteration: 4505', 'Train loss: 0.038147', 'Train acc: 0.995000')\n",
      "('Epoch: 901/2000', 'Iteration: 4510', 'Train loss: 0.040896', 'Train acc: 0.990000')\n",
      "('Epoch: 901/2000', 'Iteration: 4510', 'Validation loss: 0.388154', 'Validation acc: 0.916667')\n",
      "('Epoch: 902/2000', 'Iteration: 4515', 'Train loss: 0.043588', 'Train acc: 0.988333')\n",
      "('Epoch: 903/2000', 'Iteration: 4520', 'Train loss: 0.038025', 'Train acc: 0.990000')\n",
      "('Epoch: 903/2000', 'Iteration: 4520', 'Validation loss: 0.386449', 'Validation acc: 0.921667')\n",
      "('Epoch: 904/2000', 'Iteration: 4525', 'Train loss: 0.041191', 'Train acc: 0.988333')\n",
      "('Epoch: 905/2000', 'Iteration: 4530', 'Train loss: 0.055331', 'Train acc: 0.986667')\n",
      "('Epoch: 905/2000', 'Iteration: 4530', 'Validation loss: 0.386053', 'Validation acc: 0.918333')\n",
      "('Epoch: 906/2000', 'Iteration: 4535', 'Train loss: 0.046606', 'Train acc: 0.991667')\n",
      "('Epoch: 907/2000', 'Iteration: 4540', 'Train loss: 0.053615', 'Train acc: 0.985000')\n",
      "('Epoch: 907/2000', 'Iteration: 4540', 'Validation loss: 0.388808', 'Validation acc: 0.911667')\n",
      "('Epoch: 908/2000', 'Iteration: 4545', 'Train loss: 0.053512', 'Train acc: 0.983333')\n",
      "('Epoch: 909/2000', 'Iteration: 4550', 'Train loss: 0.030860', 'Train acc: 0.996667')\n",
      "('Epoch: 909/2000', 'Iteration: 4550', 'Validation loss: 0.386085', 'Validation acc: 0.918333')\n",
      "('Epoch: 910/2000', 'Iteration: 4555', 'Train loss: 0.049074', 'Train acc: 0.986667')\n",
      "('Epoch: 911/2000', 'Iteration: 4560', 'Train loss: 0.042494', 'Train acc: 0.988333')\n",
      "('Epoch: 911/2000', 'Iteration: 4560', 'Validation loss: 0.386713', 'Validation acc: 0.916667')\n",
      "('Epoch: 912/2000', 'Iteration: 4565', 'Train loss: 0.046064', 'Train acc: 0.986667')\n",
      "('Epoch: 913/2000', 'Iteration: 4570', 'Train loss: 0.044123', 'Train acc: 0.993333')\n",
      "('Epoch: 913/2000', 'Iteration: 4570', 'Validation loss: 0.386034', 'Validation acc: 0.918333')\n",
      "('Epoch: 914/2000', 'Iteration: 4575', 'Train loss: 0.053378', 'Train acc: 0.990000')\n",
      "('Epoch: 915/2000', 'Iteration: 4580', 'Train loss: 0.035104', 'Train acc: 0.991667')\n",
      "('Epoch: 915/2000', 'Iteration: 4580', 'Validation loss: 0.389727', 'Validation acc: 0.913333')\n",
      "('Epoch: 916/2000', 'Iteration: 4585', 'Train loss: 0.033754', 'Train acc: 0.995000')\n",
      "('Epoch: 917/2000', 'Iteration: 4590', 'Train loss: 0.053679', 'Train acc: 0.980000')\n",
      "('Epoch: 917/2000', 'Iteration: 4590', 'Validation loss: 0.386449', 'Validation acc: 0.916667')\n",
      "('Epoch: 918/2000', 'Iteration: 4595', 'Train loss: 0.039861', 'Train acc: 0.988333')\n",
      "('Epoch: 919/2000', 'Iteration: 4600', 'Train loss: 0.047367', 'Train acc: 0.986667')\n",
      "('Epoch: 919/2000', 'Iteration: 4600', 'Validation loss: 0.386748', 'Validation acc: 0.916667')\n",
      "('Epoch: 920/2000', 'Iteration: 4605', 'Train loss: 0.042955', 'Train acc: 0.990000')\n",
      "('Epoch: 921/2000', 'Iteration: 4610', 'Train loss: 0.036172', 'Train acc: 0.995000')\n",
      "('Epoch: 921/2000', 'Iteration: 4610', 'Validation loss: 0.380331', 'Validation acc: 0.918333')\n",
      "('Epoch: 922/2000', 'Iteration: 4615', 'Train loss: 0.035378', 'Train acc: 0.990000')\n",
      "('Epoch: 923/2000', 'Iteration: 4620', 'Train loss: 0.039357', 'Train acc: 0.991667')\n",
      "('Epoch: 923/2000', 'Iteration: 4620', 'Validation loss: 0.382739', 'Validation acc: 0.918333')\n",
      "('Epoch: 924/2000', 'Iteration: 4625', 'Train loss: 0.044375', 'Train acc: 0.991667')\n",
      "('Epoch: 925/2000', 'Iteration: 4630', 'Train loss: 0.041343', 'Train acc: 0.990000')\n",
      "('Epoch: 925/2000', 'Iteration: 4630', 'Validation loss: 0.389374', 'Validation acc: 0.918333')\n",
      "('Epoch: 926/2000', 'Iteration: 4635', 'Train loss: 0.044170', 'Train acc: 0.988333')\n",
      "('Epoch: 927/2000', 'Iteration: 4640', 'Train loss: 0.036697', 'Train acc: 0.991667')\n",
      "('Epoch: 927/2000', 'Iteration: 4640', 'Validation loss: 0.393836', 'Validation acc: 0.915000')\n",
      "('Epoch: 928/2000', 'Iteration: 4645', 'Train loss: 0.040501', 'Train acc: 0.993333')\n",
      "('Epoch: 929/2000', 'Iteration: 4650', 'Train loss: 0.048102', 'Train acc: 0.986667')\n",
      "('Epoch: 929/2000', 'Iteration: 4650', 'Validation loss: 0.393452', 'Validation acc: 0.915000')\n",
      "('Epoch: 930/2000', 'Iteration: 4655', 'Train loss: 0.039291', 'Train acc: 0.995000')\n",
      "('Epoch: 931/2000', 'Iteration: 4660', 'Train loss: 0.041588', 'Train acc: 0.988333')\n",
      "('Epoch: 931/2000', 'Iteration: 4660', 'Validation loss: 0.387020', 'Validation acc: 0.920000')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 932/2000', 'Iteration: 4665', 'Train loss: 0.039726', 'Train acc: 0.991667')\n",
      "('Epoch: 933/2000', 'Iteration: 4670', 'Train loss: 0.049671', 'Train acc: 0.990000')\n",
      "('Epoch: 933/2000', 'Iteration: 4670', 'Validation loss: 0.391594', 'Validation acc: 0.916667')\n",
      "('Epoch: 934/2000', 'Iteration: 4675', 'Train loss: 0.038953', 'Train acc: 0.988333')\n",
      "('Epoch: 935/2000', 'Iteration: 4680', 'Train loss: 0.048954', 'Train acc: 0.986667')\n",
      "('Epoch: 935/2000', 'Iteration: 4680', 'Validation loss: 0.387826', 'Validation acc: 0.920000')\n",
      "('Epoch: 936/2000', 'Iteration: 4685', 'Train loss: 0.050744', 'Train acc: 0.983333')\n",
      "('Epoch: 937/2000', 'Iteration: 4690', 'Train loss: 0.050776', 'Train acc: 0.985000')\n",
      "('Epoch: 937/2000', 'Iteration: 4690', 'Validation loss: 0.393483', 'Validation acc: 0.916667')\n",
      "('Epoch: 938/2000', 'Iteration: 4695', 'Train loss: 0.043723', 'Train acc: 0.991667')\n",
      "('Epoch: 939/2000', 'Iteration: 4700', 'Train loss: 0.040921', 'Train acc: 0.991667')\n",
      "('Epoch: 939/2000', 'Iteration: 4700', 'Validation loss: 0.392193', 'Validation acc: 0.915000')\n",
      "('Epoch: 940/2000', 'Iteration: 4705', 'Train loss: 0.041993', 'Train acc: 0.988333')\n",
      "('Epoch: 941/2000', 'Iteration: 4710', 'Train loss: 0.041747', 'Train acc: 0.990000')\n",
      "('Epoch: 941/2000', 'Iteration: 4710', 'Validation loss: 0.386986', 'Validation acc: 0.920000')\n",
      "('Epoch: 942/2000', 'Iteration: 4715', 'Train loss: 0.044251', 'Train acc: 0.988333')\n",
      "('Epoch: 943/2000', 'Iteration: 4720', 'Train loss: 0.038865', 'Train acc: 0.991667')\n",
      "('Epoch: 943/2000', 'Iteration: 4720', 'Validation loss: 0.388837', 'Validation acc: 0.918333')\n",
      "('Epoch: 944/2000', 'Iteration: 4725', 'Train loss: 0.041943', 'Train acc: 0.990000')\n",
      "('Epoch: 945/2000', 'Iteration: 4730', 'Train loss: 0.038400', 'Train acc: 0.981667')\n",
      "('Epoch: 945/2000', 'Iteration: 4730', 'Validation loss: 0.388908', 'Validation acc: 0.920000')\n",
      "('Epoch: 946/2000', 'Iteration: 4735', 'Train loss: 0.036983', 'Train acc: 0.986667')\n",
      "('Epoch: 947/2000', 'Iteration: 4740', 'Train loss: 0.048537', 'Train acc: 0.985000')\n",
      "('Epoch: 947/2000', 'Iteration: 4740', 'Validation loss: 0.387142', 'Validation acc: 0.916667')\n",
      "('Epoch: 948/2000', 'Iteration: 4745', 'Train loss: 0.043064', 'Train acc: 0.983333')\n",
      "('Epoch: 949/2000', 'Iteration: 4750', 'Train loss: 0.040028', 'Train acc: 0.988333')\n",
      "('Epoch: 949/2000', 'Iteration: 4750', 'Validation loss: 0.388244', 'Validation acc: 0.916667')\n",
      "('Epoch: 950/2000', 'Iteration: 4755', 'Train loss: 0.043856', 'Train acc: 0.986667')\n",
      "('Epoch: 951/2000', 'Iteration: 4760', 'Train loss: 0.056217', 'Train acc: 0.985000')\n",
      "('Epoch: 951/2000', 'Iteration: 4760', 'Validation loss: 0.391619', 'Validation acc: 0.913333')\n",
      "('Epoch: 952/2000', 'Iteration: 4765', 'Train loss: 0.035956', 'Train acc: 0.991667')\n",
      "('Epoch: 953/2000', 'Iteration: 4770', 'Train loss: 0.046878', 'Train acc: 0.986667')\n",
      "('Epoch: 953/2000', 'Iteration: 4770', 'Validation loss: 0.390153', 'Validation acc: 0.918333')\n",
      "('Epoch: 954/2000', 'Iteration: 4775', 'Train loss: 0.043824', 'Train acc: 0.985000')\n",
      "('Epoch: 955/2000', 'Iteration: 4780', 'Train loss: 0.039811', 'Train acc: 0.986667')\n",
      "('Epoch: 955/2000', 'Iteration: 4780', 'Validation loss: 0.390688', 'Validation acc: 0.913333')\n",
      "('Epoch: 956/2000', 'Iteration: 4785', 'Train loss: 0.036603', 'Train acc: 0.991667')\n",
      "('Epoch: 957/2000', 'Iteration: 4790', 'Train loss: 0.032259', 'Train acc: 0.995000')\n",
      "('Epoch: 957/2000', 'Iteration: 4790', 'Validation loss: 0.392050', 'Validation acc: 0.916667')\n",
      "('Epoch: 958/2000', 'Iteration: 4795', 'Train loss: 0.048034', 'Train acc: 0.983333')\n",
      "('Epoch: 959/2000', 'Iteration: 4800', 'Train loss: 0.038655', 'Train acc: 0.993333')\n",
      "('Epoch: 959/2000', 'Iteration: 4800', 'Validation loss: 0.395270', 'Validation acc: 0.915000')\n",
      "('Epoch: 960/2000', 'Iteration: 4805', 'Train loss: 0.045298', 'Train acc: 0.986667')\n",
      "('Epoch: 961/2000', 'Iteration: 4810', 'Train loss: 0.035247', 'Train acc: 0.993333')\n",
      "('Epoch: 961/2000', 'Iteration: 4810', 'Validation loss: 0.398586', 'Validation acc: 0.916667')\n",
      "('Epoch: 962/2000', 'Iteration: 4815', 'Train loss: 0.043489', 'Train acc: 0.990000')\n",
      "('Epoch: 963/2000', 'Iteration: 4820', 'Train loss: 0.041093', 'Train acc: 0.988333')\n",
      "('Epoch: 963/2000', 'Iteration: 4820', 'Validation loss: 0.389609', 'Validation acc: 0.920000')\n",
      "('Epoch: 964/2000', 'Iteration: 4825', 'Train loss: 0.045141', 'Train acc: 0.986667')\n",
      "('Epoch: 965/2000', 'Iteration: 4830', 'Train loss: 0.041884', 'Train acc: 0.988333')\n",
      "('Epoch: 965/2000', 'Iteration: 4830', 'Validation loss: 0.392064', 'Validation acc: 0.920000')\n",
      "('Epoch: 966/2000', 'Iteration: 4835', 'Train loss: 0.043416', 'Train acc: 0.988333')\n",
      "('Epoch: 967/2000', 'Iteration: 4840', 'Train loss: 0.034600', 'Train acc: 0.995000')\n",
      "('Epoch: 967/2000', 'Iteration: 4840', 'Validation loss: 0.393983', 'Validation acc: 0.918333')\n",
      "('Epoch: 968/2000', 'Iteration: 4845', 'Train loss: 0.037106', 'Train acc: 0.986667')\n",
      "('Epoch: 969/2000', 'Iteration: 4850', 'Train loss: 0.041411', 'Train acc: 0.988333')\n",
      "('Epoch: 969/2000', 'Iteration: 4850', 'Validation loss: 0.394492', 'Validation acc: 0.916667')\n",
      "('Epoch: 970/2000', 'Iteration: 4855', 'Train loss: 0.033666', 'Train acc: 0.990000')\n",
      "('Epoch: 971/2000', 'Iteration: 4860', 'Train loss: 0.037179', 'Train acc: 0.991667')\n",
      "('Epoch: 971/2000', 'Iteration: 4860', 'Validation loss: 0.392536', 'Validation acc: 0.916667')\n",
      "('Epoch: 972/2000', 'Iteration: 4865', 'Train loss: 0.039814', 'Train acc: 0.991667')\n",
      "('Epoch: 973/2000', 'Iteration: 4870', 'Train loss: 0.032664', 'Train acc: 0.993333')\n",
      "('Epoch: 973/2000', 'Iteration: 4870', 'Validation loss: 0.392982', 'Validation acc: 0.913333')\n",
      "('Epoch: 974/2000', 'Iteration: 4875', 'Train loss: 0.033137', 'Train acc: 0.995000')\n",
      "('Epoch: 975/2000', 'Iteration: 4880', 'Train loss: 0.053691', 'Train acc: 0.985000')\n",
      "('Epoch: 975/2000', 'Iteration: 4880', 'Validation loss: 0.389259', 'Validation acc: 0.920000')\n",
      "('Epoch: 976/2000', 'Iteration: 4885', 'Train loss: 0.054378', 'Train acc: 0.983333')\n",
      "('Epoch: 977/2000', 'Iteration: 4890', 'Train loss: 0.036429', 'Train acc: 0.990000')\n",
      "('Epoch: 977/2000', 'Iteration: 4890', 'Validation loss: 0.394890', 'Validation acc: 0.918333')\n",
      "('Epoch: 978/2000', 'Iteration: 4895', 'Train loss: 0.035839', 'Train acc: 0.993333')\n",
      "('Epoch: 979/2000', 'Iteration: 4900', 'Train loss: 0.033717', 'Train acc: 0.991667')\n",
      "('Epoch: 979/2000', 'Iteration: 4900', 'Validation loss: 0.396985', 'Validation acc: 0.916667')\n",
      "('Epoch: 980/2000', 'Iteration: 4905', 'Train loss: 0.033151', 'Train acc: 0.995000')\n",
      "('Epoch: 981/2000', 'Iteration: 4910', 'Train loss: 0.033923', 'Train acc: 0.993333')\n",
      "('Epoch: 981/2000', 'Iteration: 4910', 'Validation loss: 0.399686', 'Validation acc: 0.915000')\n",
      "('Epoch: 982/2000', 'Iteration: 4915', 'Train loss: 0.034975', 'Train acc: 0.988333')\n",
      "('Epoch: 983/2000', 'Iteration: 4920', 'Train loss: 0.037218', 'Train acc: 0.990000')\n",
      "('Epoch: 983/2000', 'Iteration: 4920', 'Validation loss: 0.392522', 'Validation acc: 0.916667')\n",
      "('Epoch: 984/2000', 'Iteration: 4925', 'Train loss: 0.039020', 'Train acc: 0.985000')\n",
      "('Epoch: 985/2000', 'Iteration: 4930', 'Train loss: 0.037406', 'Train acc: 0.993333')\n",
      "('Epoch: 985/2000', 'Iteration: 4930', 'Validation loss: 0.395927', 'Validation acc: 0.918333')\n",
      "('Epoch: 986/2000', 'Iteration: 4935', 'Train loss: 0.036539', 'Train acc: 0.988333')\n",
      "('Epoch: 987/2000', 'Iteration: 4940', 'Train loss: 0.040376', 'Train acc: 0.988333')\n",
      "('Epoch: 987/2000', 'Iteration: 4940', 'Validation loss: 0.395064', 'Validation acc: 0.918333')\n",
      "('Epoch: 988/2000', 'Iteration: 4945', 'Train loss: 0.033615', 'Train acc: 0.991667')\n",
      "('Epoch: 989/2000', 'Iteration: 4950', 'Train loss: 0.037532', 'Train acc: 0.988333')\n",
      "('Epoch: 989/2000', 'Iteration: 4950', 'Validation loss: 0.393186', 'Validation acc: 0.918333')\n",
      "('Epoch: 990/2000', 'Iteration: 4955', 'Train loss: 0.050129', 'Train acc: 0.981667')\n",
      "('Epoch: 991/2000', 'Iteration: 4960', 'Train loss: 0.030980', 'Train acc: 0.991667')\n",
      "('Epoch: 991/2000', 'Iteration: 4960', 'Validation loss: 0.396693', 'Validation acc: 0.920000')\n",
      "('Epoch: 992/2000', 'Iteration: 4965', 'Train loss: 0.033955', 'Train acc: 0.991667')\n",
      "('Epoch: 993/2000', 'Iteration: 4970', 'Train loss: 0.042694', 'Train acc: 0.985000')\n",
      "('Epoch: 993/2000', 'Iteration: 4970', 'Validation loss: 0.396441', 'Validation acc: 0.915000')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 994/2000', 'Iteration: 4975', 'Train loss: 0.040805', 'Train acc: 0.985000')\n",
      "('Epoch: 995/2000', 'Iteration: 4980', 'Train loss: 0.036776', 'Train acc: 0.990000')\n",
      "('Epoch: 995/2000', 'Iteration: 4980', 'Validation loss: 0.394418', 'Validation acc: 0.921667')\n",
      "('Epoch: 996/2000', 'Iteration: 4985', 'Train loss: 0.031134', 'Train acc: 0.993333')\n",
      "('Epoch: 997/2000', 'Iteration: 4990', 'Train loss: 0.031228', 'Train acc: 0.996667')\n",
      "('Epoch: 997/2000', 'Iteration: 4990', 'Validation loss: 0.395484', 'Validation acc: 0.918333')\n",
      "('Epoch: 998/2000', 'Iteration: 4995', 'Train loss: 0.033710', 'Train acc: 0.993333')\n",
      "('Epoch: 999/2000', 'Iteration: 5000', 'Train loss: 0.036061', 'Train acc: 0.988333')\n",
      "('Epoch: 999/2000', 'Iteration: 5000', 'Validation loss: 0.399225', 'Validation acc: 0.915000')\n",
      "('Epoch: 1000/2000', 'Iteration: 5005', 'Train loss: 0.039010', 'Train acc: 0.993333')\n",
      "('Epoch: 1001/2000', 'Iteration: 5010', 'Train loss: 0.028893', 'Train acc: 0.996667')\n",
      "('Epoch: 1001/2000', 'Iteration: 5010', 'Validation loss: 0.399974', 'Validation acc: 0.916667')\n",
      "('Epoch: 1002/2000', 'Iteration: 5015', 'Train loss: 0.032777', 'Train acc: 0.995000')\n",
      "('Epoch: 1003/2000', 'Iteration: 5020', 'Train loss: 0.038265', 'Train acc: 0.990000')\n",
      "('Epoch: 1003/2000', 'Iteration: 5020', 'Validation loss: 0.399838', 'Validation acc: 0.918333')\n",
      "('Epoch: 1004/2000', 'Iteration: 5025', 'Train loss: 0.032462', 'Train acc: 0.993333')\n",
      "('Epoch: 1005/2000', 'Iteration: 5030', 'Train loss: 0.032046', 'Train acc: 0.996667')\n",
      "('Epoch: 1005/2000', 'Iteration: 5030', 'Validation loss: 0.398437', 'Validation acc: 0.920000')\n",
      "('Epoch: 1006/2000', 'Iteration: 5035', 'Train loss: 0.039375', 'Train acc: 0.988333')\n",
      "('Epoch: 1007/2000', 'Iteration: 5040', 'Train loss: 0.029480', 'Train acc: 0.998333')\n",
      "('Epoch: 1007/2000', 'Iteration: 5040', 'Validation loss: 0.402283', 'Validation acc: 0.923333')\n",
      "('Epoch: 1008/2000', 'Iteration: 5045', 'Train loss: 0.035271', 'Train acc: 0.996667')\n",
      "('Epoch: 1009/2000', 'Iteration: 5050', 'Train loss: 0.028567', 'Train acc: 0.995000')\n",
      "('Epoch: 1009/2000', 'Iteration: 5050', 'Validation loss: 0.401811', 'Validation acc: 0.915000')\n",
      "('Epoch: 1010/2000', 'Iteration: 5055', 'Train loss: 0.034852', 'Train acc: 0.990000')\n",
      "('Epoch: 1011/2000', 'Iteration: 5060', 'Train loss: 0.030550', 'Train acc: 0.993333')\n",
      "('Epoch: 1011/2000', 'Iteration: 5060', 'Validation loss: 0.396448', 'Validation acc: 0.918333')\n",
      "('Epoch: 1012/2000', 'Iteration: 5065', 'Train loss: 0.035734', 'Train acc: 0.988333')\n",
      "('Epoch: 1013/2000', 'Iteration: 5070', 'Train loss: 0.032430', 'Train acc: 0.991667')\n",
      "('Epoch: 1013/2000', 'Iteration: 5070', 'Validation loss: 0.389904', 'Validation acc: 0.920000')\n",
      "('Epoch: 1014/2000', 'Iteration: 5075', 'Train loss: 0.030137', 'Train acc: 0.990000')\n",
      "('Epoch: 1015/2000', 'Iteration: 5080', 'Train loss: 0.030024', 'Train acc: 0.991667')\n",
      "('Epoch: 1015/2000', 'Iteration: 5080', 'Validation loss: 0.402525', 'Validation acc: 0.913333')\n",
      "('Epoch: 1016/2000', 'Iteration: 5085', 'Train loss: 0.041759', 'Train acc: 0.990000')\n",
      "('Epoch: 1017/2000', 'Iteration: 5090', 'Train loss: 0.032612', 'Train acc: 0.991667')\n",
      "('Epoch: 1017/2000', 'Iteration: 5090', 'Validation loss: 0.398310', 'Validation acc: 0.916667')\n",
      "('Epoch: 1018/2000', 'Iteration: 5095', 'Train loss: 0.031793', 'Train acc: 0.993333')\n",
      "('Epoch: 1019/2000', 'Iteration: 5100', 'Train loss: 0.033288', 'Train acc: 0.993333')\n",
      "('Epoch: 1019/2000', 'Iteration: 5100', 'Validation loss: 0.394844', 'Validation acc: 0.918333')\n",
      "('Epoch: 1020/2000', 'Iteration: 5105', 'Train loss: 0.027589', 'Train acc: 0.991667')\n",
      "('Epoch: 1021/2000', 'Iteration: 5110', 'Train loss: 0.046148', 'Train acc: 0.986667')\n",
      "('Epoch: 1021/2000', 'Iteration: 5110', 'Validation loss: 0.395041', 'Validation acc: 0.920000')\n",
      "('Epoch: 1022/2000', 'Iteration: 5115', 'Train loss: 0.038882', 'Train acc: 0.985000')\n",
      "('Epoch: 1023/2000', 'Iteration: 5120', 'Train loss: 0.030771', 'Train acc: 0.990000')\n",
      "('Epoch: 1023/2000', 'Iteration: 5120', 'Validation loss: 0.397087', 'Validation acc: 0.920000')\n",
      "('Epoch: 1024/2000', 'Iteration: 5125', 'Train loss: 0.035858', 'Train acc: 0.991667')\n",
      "('Epoch: 1025/2000', 'Iteration: 5130', 'Train loss: 0.030528', 'Train acc: 0.991667')\n",
      "('Epoch: 1025/2000', 'Iteration: 5130', 'Validation loss: 0.401798', 'Validation acc: 0.916667')\n",
      "('Epoch: 1026/2000', 'Iteration: 5135', 'Train loss: 0.031848', 'Train acc: 0.988333')\n",
      "('Epoch: 1027/2000', 'Iteration: 5140', 'Train loss: 0.031156', 'Train acc: 0.993333')\n",
      "('Epoch: 1027/2000', 'Iteration: 5140', 'Validation loss: 0.396585', 'Validation acc: 0.918333')\n",
      "('Epoch: 1028/2000', 'Iteration: 5145', 'Train loss: 0.039815', 'Train acc: 0.988333')\n",
      "('Epoch: 1029/2000', 'Iteration: 5150', 'Train loss: 0.046212', 'Train acc: 0.988333')\n",
      "('Epoch: 1029/2000', 'Iteration: 5150', 'Validation loss: 0.396248', 'Validation acc: 0.920000')\n",
      "('Epoch: 1030/2000', 'Iteration: 5155', 'Train loss: 0.042720', 'Train acc: 0.985000')\n",
      "('Epoch: 1031/2000', 'Iteration: 5160', 'Train loss: 0.025899', 'Train acc: 0.995000')\n",
      "('Epoch: 1031/2000', 'Iteration: 5160', 'Validation loss: 0.398701', 'Validation acc: 0.916667')\n",
      "('Epoch: 1032/2000', 'Iteration: 5165', 'Train loss: 0.035717', 'Train acc: 0.990000')\n",
      "('Epoch: 1033/2000', 'Iteration: 5170', 'Train loss: 0.025994', 'Train acc: 0.998333')\n",
      "('Epoch: 1033/2000', 'Iteration: 5170', 'Validation loss: 0.396877', 'Validation acc: 0.920000')\n",
      "('Epoch: 1034/2000', 'Iteration: 5175', 'Train loss: 0.032871', 'Train acc: 0.988333')\n",
      "('Epoch: 1035/2000', 'Iteration: 5180', 'Train loss: 0.032342', 'Train acc: 0.995000')\n",
      "('Epoch: 1035/2000', 'Iteration: 5180', 'Validation loss: 0.401368', 'Validation acc: 0.916667')\n",
      "('Epoch: 1036/2000', 'Iteration: 5185', 'Train loss: 0.033729', 'Train acc: 0.988333')\n",
      "('Epoch: 1037/2000', 'Iteration: 5190', 'Train loss: 0.040910', 'Train acc: 0.990000')\n",
      "('Epoch: 1037/2000', 'Iteration: 5190', 'Validation loss: 0.394131', 'Validation acc: 0.923333')\n",
      "('Epoch: 1038/2000', 'Iteration: 5195', 'Train loss: 0.030674', 'Train acc: 0.991667')\n",
      "('Epoch: 1039/2000', 'Iteration: 5200', 'Train loss: 0.028165', 'Train acc: 0.998333')\n",
      "('Epoch: 1039/2000', 'Iteration: 5200', 'Validation loss: 0.402216', 'Validation acc: 0.918333')\n",
      "('Epoch: 1040/2000', 'Iteration: 5205', 'Train loss: 0.039112', 'Train acc: 0.993333')\n",
      "('Epoch: 1041/2000', 'Iteration: 5210', 'Train loss: 0.029632', 'Train acc: 0.990000')\n",
      "('Epoch: 1041/2000', 'Iteration: 5210', 'Validation loss: 0.403273', 'Validation acc: 0.918333')\n",
      "('Epoch: 1042/2000', 'Iteration: 5215', 'Train loss: 0.036833', 'Train acc: 0.990000')\n",
      "('Epoch: 1043/2000', 'Iteration: 5220', 'Train loss: 0.037616', 'Train acc: 0.990000')\n",
      "('Epoch: 1043/2000', 'Iteration: 5220', 'Validation loss: 0.396213', 'Validation acc: 0.920000')\n",
      "('Epoch: 1044/2000', 'Iteration: 5225', 'Train loss: 0.034591', 'Train acc: 0.988333')\n",
      "('Epoch: 1045/2000', 'Iteration: 5230', 'Train loss: 0.031001', 'Train acc: 0.991667')\n",
      "('Epoch: 1045/2000', 'Iteration: 5230', 'Validation loss: 0.397565', 'Validation acc: 0.915000')\n",
      "('Epoch: 1046/2000', 'Iteration: 5235', 'Train loss: 0.033641', 'Train acc: 0.993333')\n",
      "('Epoch: 1047/2000', 'Iteration: 5240', 'Train loss: 0.037103', 'Train acc: 0.990000')\n",
      "('Epoch: 1047/2000', 'Iteration: 5240', 'Validation loss: 0.398179', 'Validation acc: 0.918333')\n",
      "('Epoch: 1048/2000', 'Iteration: 5245', 'Train loss: 0.040052', 'Train acc: 0.985000')\n",
      "('Epoch: 1049/2000', 'Iteration: 5250', 'Train loss: 0.042147', 'Train acc: 0.986667')\n",
      "('Epoch: 1049/2000', 'Iteration: 5250', 'Validation loss: 0.398718', 'Validation acc: 0.920000')\n",
      "('Epoch: 1050/2000', 'Iteration: 5255', 'Train loss: 0.025043', 'Train acc: 0.993333')\n",
      "('Epoch: 1051/2000', 'Iteration: 5260', 'Train loss: 0.035937', 'Train acc: 0.993333')\n",
      "('Epoch: 1051/2000', 'Iteration: 5260', 'Validation loss: 0.398758', 'Validation acc: 0.918333')\n",
      "('Epoch: 1052/2000', 'Iteration: 5265', 'Train loss: 0.033377', 'Train acc: 0.991667')\n",
      "('Epoch: 1053/2000', 'Iteration: 5270', 'Train loss: 0.038298', 'Train acc: 0.988333')\n",
      "('Epoch: 1053/2000', 'Iteration: 5270', 'Validation loss: 0.397436', 'Validation acc: 0.918333')\n",
      "('Epoch: 1054/2000', 'Iteration: 5275', 'Train loss: 0.039445', 'Train acc: 0.990000')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 1055/2000', 'Iteration: 5280', 'Train loss: 0.030297', 'Train acc: 0.993333')\n",
      "('Epoch: 1055/2000', 'Iteration: 5280', 'Validation loss: 0.396972', 'Validation acc: 0.920000')\n",
      "('Epoch: 1056/2000', 'Iteration: 5285', 'Train loss: 0.031336', 'Train acc: 0.993333')\n",
      "('Epoch: 1057/2000', 'Iteration: 5290', 'Train loss: 0.029218', 'Train acc: 0.991667')\n",
      "('Epoch: 1057/2000', 'Iteration: 5290', 'Validation loss: 0.395970', 'Validation acc: 0.920000')\n",
      "('Epoch: 1058/2000', 'Iteration: 5295', 'Train loss: 0.038766', 'Train acc: 0.988333')\n",
      "('Epoch: 1059/2000', 'Iteration: 5300', 'Train loss: 0.035147', 'Train acc: 0.990000')\n",
      "('Epoch: 1059/2000', 'Iteration: 5300', 'Validation loss: 0.397482', 'Validation acc: 0.923333')\n",
      "('Epoch: 1060/2000', 'Iteration: 5305', 'Train loss: 0.034558', 'Train acc: 0.988333')\n",
      "('Epoch: 1061/2000', 'Iteration: 5310', 'Train loss: 0.024061', 'Train acc: 0.995000')\n",
      "('Epoch: 1061/2000', 'Iteration: 5310', 'Validation loss: 0.393375', 'Validation acc: 0.918333')\n",
      "('Epoch: 1062/2000', 'Iteration: 5315', 'Train loss: 0.033901', 'Train acc: 0.990000')\n",
      "('Epoch: 1063/2000', 'Iteration: 5320', 'Train loss: 0.028102', 'Train acc: 0.990000')\n",
      "('Epoch: 1063/2000', 'Iteration: 5320', 'Validation loss: 0.399798', 'Validation acc: 0.916667')\n",
      "('Epoch: 1064/2000', 'Iteration: 5325', 'Train loss: 0.030573', 'Train acc: 0.993333')\n",
      "('Epoch: 1065/2000', 'Iteration: 5330', 'Train loss: 0.025500', 'Train acc: 0.991667')\n",
      "('Epoch: 1065/2000', 'Iteration: 5330', 'Validation loss: 0.400335', 'Validation acc: 0.923333')\n",
      "('Epoch: 1066/2000', 'Iteration: 5335', 'Train loss: 0.029861', 'Train acc: 0.995000')\n",
      "('Epoch: 1067/2000', 'Iteration: 5340', 'Train loss: 0.032207', 'Train acc: 0.995000')\n",
      "('Epoch: 1067/2000', 'Iteration: 5340', 'Validation loss: 0.399384', 'Validation acc: 0.918333')\n",
      "('Epoch: 1068/2000', 'Iteration: 5345', 'Train loss: 0.035322', 'Train acc: 0.991667')\n",
      "('Epoch: 1069/2000', 'Iteration: 5350', 'Train loss: 0.035217', 'Train acc: 0.986667')\n",
      "('Epoch: 1069/2000', 'Iteration: 5350', 'Validation loss: 0.404414', 'Validation acc: 0.916667')\n",
      "('Epoch: 1070/2000', 'Iteration: 5355', 'Train loss: 0.031859', 'Train acc: 0.991667')\n",
      "('Epoch: 1071/2000', 'Iteration: 5360', 'Train loss: 0.031366', 'Train acc: 0.990000')\n",
      "('Epoch: 1071/2000', 'Iteration: 5360', 'Validation loss: 0.397636', 'Validation acc: 0.916667')\n",
      "('Epoch: 1072/2000', 'Iteration: 5365', 'Train loss: 0.028727', 'Train acc: 0.993333')\n",
      "('Epoch: 1073/2000', 'Iteration: 5370', 'Train loss: 0.030550', 'Train acc: 0.995000')\n",
      "('Epoch: 1073/2000', 'Iteration: 5370', 'Validation loss: 0.396947', 'Validation acc: 0.920000')\n",
      "('Epoch: 1074/2000', 'Iteration: 5375', 'Train loss: 0.034809', 'Train acc: 0.991667')\n",
      "('Epoch: 1075/2000', 'Iteration: 5380', 'Train loss: 0.027823', 'Train acc: 0.996667')\n",
      "('Epoch: 1075/2000', 'Iteration: 5380', 'Validation loss: 0.396547', 'Validation acc: 0.921667')\n",
      "('Epoch: 1076/2000', 'Iteration: 5385', 'Train loss: 0.030847', 'Train acc: 0.995000')\n",
      "('Epoch: 1077/2000', 'Iteration: 5390', 'Train loss: 0.037453', 'Train acc: 0.988333')\n",
      "('Epoch: 1077/2000', 'Iteration: 5390', 'Validation loss: 0.395553', 'Validation acc: 0.918333')\n",
      "('Epoch: 1078/2000', 'Iteration: 5395', 'Train loss: 0.023121', 'Train acc: 0.998333')\n",
      "('Epoch: 1079/2000', 'Iteration: 5400', 'Train loss: 0.035951', 'Train acc: 0.988333')\n",
      "('Epoch: 1079/2000', 'Iteration: 5400', 'Validation loss: 0.399547', 'Validation acc: 0.918333')\n",
      "('Epoch: 1080/2000', 'Iteration: 5405', 'Train loss: 0.026199', 'Train acc: 0.993333')\n",
      "('Epoch: 1081/2000', 'Iteration: 5410', 'Train loss: 0.025158', 'Train acc: 0.991667')\n",
      "('Epoch: 1081/2000', 'Iteration: 5410', 'Validation loss: 0.398777', 'Validation acc: 0.918333')\n",
      "('Epoch: 1082/2000', 'Iteration: 5415', 'Train loss: 0.029947', 'Train acc: 0.993333')\n",
      "('Epoch: 1083/2000', 'Iteration: 5420', 'Train loss: 0.019674', 'Train acc: 0.998333')\n",
      "('Epoch: 1083/2000', 'Iteration: 5420', 'Validation loss: 0.400926', 'Validation acc: 0.921667')\n",
      "('Epoch: 1084/2000', 'Iteration: 5425', 'Train loss: 0.034024', 'Train acc: 0.988333')\n",
      "('Epoch: 1085/2000', 'Iteration: 5430', 'Train loss: 0.029203', 'Train acc: 0.993333')\n",
      "('Epoch: 1085/2000', 'Iteration: 5430', 'Validation loss: 0.399385', 'Validation acc: 0.920000')\n",
      "('Epoch: 1086/2000', 'Iteration: 5435', 'Train loss: 0.024340', 'Train acc: 0.996667')\n",
      "('Epoch: 1087/2000', 'Iteration: 5440', 'Train loss: 0.030354', 'Train acc: 0.993333')\n",
      "('Epoch: 1087/2000', 'Iteration: 5440', 'Validation loss: 0.405493', 'Validation acc: 0.920000')\n",
      "('Epoch: 1088/2000', 'Iteration: 5445', 'Train loss: 0.036795', 'Train acc: 0.990000')\n",
      "('Epoch: 1089/2000', 'Iteration: 5450', 'Train loss: 0.028110', 'Train acc: 0.993333')\n",
      "('Epoch: 1089/2000', 'Iteration: 5450', 'Validation loss: 0.407936', 'Validation acc: 0.920000')\n",
      "('Epoch: 1090/2000', 'Iteration: 5455', 'Train loss: 0.035369', 'Train acc: 0.988333')\n",
      "('Epoch: 1091/2000', 'Iteration: 5460', 'Train loss: 0.041049', 'Train acc: 0.988333')\n",
      "('Epoch: 1091/2000', 'Iteration: 5460', 'Validation loss: 0.405164', 'Validation acc: 0.921667')\n",
      "('Epoch: 1092/2000', 'Iteration: 5465', 'Train loss: 0.021566', 'Train acc: 0.998333')\n",
      "('Epoch: 1093/2000', 'Iteration: 5470', 'Train loss: 0.029783', 'Train acc: 0.995000')\n",
      "('Epoch: 1093/2000', 'Iteration: 5470', 'Validation loss: 0.405961', 'Validation acc: 0.920000')\n",
      "('Epoch: 1094/2000', 'Iteration: 5475', 'Train loss: 0.031970', 'Train acc: 0.993333')\n",
      "('Epoch: 1095/2000', 'Iteration: 5480', 'Train loss: 0.046356', 'Train acc: 0.988333')\n",
      "('Epoch: 1095/2000', 'Iteration: 5480', 'Validation loss: 0.406430', 'Validation acc: 0.918333')\n",
      "('Epoch: 1096/2000', 'Iteration: 5485', 'Train loss: 0.028996', 'Train acc: 0.991667')\n",
      "('Epoch: 1097/2000', 'Iteration: 5490', 'Train loss: 0.030692', 'Train acc: 0.990000')\n",
      "('Epoch: 1097/2000', 'Iteration: 5490', 'Validation loss: 0.404627', 'Validation acc: 0.918333')\n",
      "('Epoch: 1098/2000', 'Iteration: 5495', 'Train loss: 0.032533', 'Train acc: 0.990000')\n",
      "('Epoch: 1099/2000', 'Iteration: 5500', 'Train loss: 0.025113', 'Train acc: 0.998333')\n",
      "('Epoch: 1099/2000', 'Iteration: 5500', 'Validation loss: 0.399276', 'Validation acc: 0.923333')\n",
      "('Epoch: 1100/2000', 'Iteration: 5505', 'Train loss: 0.033032', 'Train acc: 0.991667')\n",
      "('Epoch: 1101/2000', 'Iteration: 5510', 'Train loss: 0.025508', 'Train acc: 0.991667')\n",
      "('Epoch: 1101/2000', 'Iteration: 5510', 'Validation loss: 0.406780', 'Validation acc: 0.921667')\n",
      "('Epoch: 1102/2000', 'Iteration: 5515', 'Train loss: 0.029533', 'Train acc: 0.995000')\n",
      "('Epoch: 1103/2000', 'Iteration: 5520', 'Train loss: 0.032996', 'Train acc: 0.993333')\n",
      "('Epoch: 1103/2000', 'Iteration: 5520', 'Validation loss: 0.412820', 'Validation acc: 0.920000')\n",
      "('Epoch: 1104/2000', 'Iteration: 5525', 'Train loss: 0.037120', 'Train acc: 0.990000')\n",
      "('Epoch: 1105/2000', 'Iteration: 5530', 'Train loss: 0.034709', 'Train acc: 0.988333')\n",
      "('Epoch: 1105/2000', 'Iteration: 5530', 'Validation loss: 0.404394', 'Validation acc: 0.921667')\n",
      "('Epoch: 1106/2000', 'Iteration: 5535', 'Train loss: 0.037383', 'Train acc: 0.991667')\n",
      "('Epoch: 1107/2000', 'Iteration: 5540', 'Train loss: 0.030440', 'Train acc: 0.990000')\n",
      "('Epoch: 1107/2000', 'Iteration: 5540', 'Validation loss: 0.404472', 'Validation acc: 0.923333')\n",
      "('Epoch: 1108/2000', 'Iteration: 5545', 'Train loss: 0.026247', 'Train acc: 0.996667')\n",
      "('Epoch: 1109/2000', 'Iteration: 5550', 'Train loss: 0.029953', 'Train acc: 0.991667')\n",
      "('Epoch: 1109/2000', 'Iteration: 5550', 'Validation loss: 0.404943', 'Validation acc: 0.918333')\n",
      "('Epoch: 1110/2000', 'Iteration: 5555', 'Train loss: 0.030344', 'Train acc: 0.990000')\n",
      "('Epoch: 1111/2000', 'Iteration: 5560', 'Train loss: 0.030863', 'Train acc: 0.988333')\n",
      "('Epoch: 1111/2000', 'Iteration: 5560', 'Validation loss: 0.408046', 'Validation acc: 0.918333')\n",
      "('Epoch: 1112/2000', 'Iteration: 5565', 'Train loss: 0.028388', 'Train acc: 0.990000')\n",
      "('Epoch: 1113/2000', 'Iteration: 5570', 'Train loss: 0.035753', 'Train acc: 0.988333')\n",
      "('Epoch: 1113/2000', 'Iteration: 5570', 'Validation loss: 0.406458', 'Validation acc: 0.918333')\n",
      "('Epoch: 1114/2000', 'Iteration: 5575', 'Train loss: 0.028129', 'Train acc: 0.993333')\n",
      "('Epoch: 1115/2000', 'Iteration: 5580', 'Train loss: 0.034856', 'Train acc: 0.991667')\n",
      "('Epoch: 1115/2000', 'Iteration: 5580', 'Validation loss: 0.400507', 'Validation acc: 0.925000')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 1116/2000', 'Iteration: 5585', 'Train loss: 0.028988', 'Train acc: 0.990000')\n",
      "('Epoch: 1117/2000', 'Iteration: 5590', 'Train loss: 0.033198', 'Train acc: 0.993333')\n",
      "('Epoch: 1117/2000', 'Iteration: 5590', 'Validation loss: 0.397581', 'Validation acc: 0.920000')\n",
      "('Epoch: 1118/2000', 'Iteration: 5595', 'Train loss: 0.033911', 'Train acc: 0.993333')\n",
      "('Epoch: 1119/2000', 'Iteration: 5600', 'Train loss: 0.032845', 'Train acc: 0.991667')\n",
      "('Epoch: 1119/2000', 'Iteration: 5600', 'Validation loss: 0.403815', 'Validation acc: 0.918333')\n",
      "('Epoch: 1120/2000', 'Iteration: 5605', 'Train loss: 0.023978', 'Train acc: 0.993333')\n",
      "('Epoch: 1121/2000', 'Iteration: 5610', 'Train loss: 0.029628', 'Train acc: 0.995000')\n",
      "('Epoch: 1121/2000', 'Iteration: 5610', 'Validation loss: 0.403652', 'Validation acc: 0.921667')\n",
      "('Epoch: 1122/2000', 'Iteration: 5615', 'Train loss: 0.028995', 'Train acc: 0.993333')\n",
      "('Epoch: 1123/2000', 'Iteration: 5620', 'Train loss: 0.030040', 'Train acc: 0.990000')\n",
      "('Epoch: 1123/2000', 'Iteration: 5620', 'Validation loss: 0.404998', 'Validation acc: 0.921667')\n",
      "('Epoch: 1124/2000', 'Iteration: 5625', 'Train loss: 0.029570', 'Train acc: 0.991667')\n",
      "('Epoch: 1125/2000', 'Iteration: 5630', 'Train loss: 0.025348', 'Train acc: 0.991667')\n",
      "('Epoch: 1125/2000', 'Iteration: 5630', 'Validation loss: 0.402647', 'Validation acc: 0.920000')\n",
      "('Epoch: 1126/2000', 'Iteration: 5635', 'Train loss: 0.034046', 'Train acc: 0.993333')\n",
      "('Epoch: 1127/2000', 'Iteration: 5640', 'Train loss: 0.023389', 'Train acc: 0.996667')\n",
      "('Epoch: 1127/2000', 'Iteration: 5640', 'Validation loss: 0.402011', 'Validation acc: 0.920000')\n",
      "('Epoch: 1128/2000', 'Iteration: 5645', 'Train loss: 0.020194', 'Train acc: 0.996667')\n",
      "('Epoch: 1129/2000', 'Iteration: 5650', 'Train loss: 0.033024', 'Train acc: 0.990000')\n",
      "('Epoch: 1129/2000', 'Iteration: 5650', 'Validation loss: 0.415100', 'Validation acc: 0.921667')\n",
      "('Epoch: 1130/2000', 'Iteration: 5655', 'Train loss: 0.028898', 'Train acc: 0.993333')\n",
      "('Epoch: 1131/2000', 'Iteration: 5660', 'Train loss: 0.024543', 'Train acc: 0.996667')\n",
      "('Epoch: 1131/2000', 'Iteration: 5660', 'Validation loss: 0.412254', 'Validation acc: 0.921667')\n",
      "('Epoch: 1132/2000', 'Iteration: 5665', 'Train loss: 0.024989', 'Train acc: 0.996667')\n",
      "('Epoch: 1133/2000', 'Iteration: 5670', 'Train loss: 0.029127', 'Train acc: 0.991667')\n",
      "('Epoch: 1133/2000', 'Iteration: 5670', 'Validation loss: 0.410669', 'Validation acc: 0.921667')\n",
      "('Epoch: 1134/2000', 'Iteration: 5675', 'Train loss: 0.029384', 'Train acc: 0.993333')\n",
      "('Epoch: 1135/2000', 'Iteration: 5680', 'Train loss: 0.018373', 'Train acc: 0.996667')\n",
      "('Epoch: 1135/2000', 'Iteration: 5680', 'Validation loss: 0.410344', 'Validation acc: 0.921667')\n",
      "('Epoch: 1136/2000', 'Iteration: 5685', 'Train loss: 0.032702', 'Train acc: 0.993333')\n",
      "('Epoch: 1137/2000', 'Iteration: 5690', 'Train loss: 0.022322', 'Train acc: 0.996667')\n",
      "('Epoch: 1137/2000', 'Iteration: 5690', 'Validation loss: 0.406580', 'Validation acc: 0.926667')\n",
      "('Epoch: 1138/2000', 'Iteration: 5695', 'Train loss: 0.020497', 'Train acc: 0.995000')\n",
      "('Epoch: 1139/2000', 'Iteration: 5700', 'Train loss: 0.026511', 'Train acc: 0.993333')\n",
      "('Epoch: 1139/2000', 'Iteration: 5700', 'Validation loss: 0.402838', 'Validation acc: 0.923333')\n",
      "('Epoch: 1140/2000', 'Iteration: 5705', 'Train loss: 0.034849', 'Train acc: 0.981667')\n",
      "('Epoch: 1141/2000', 'Iteration: 5710', 'Train loss: 0.023707', 'Train acc: 0.995000')\n",
      "('Epoch: 1141/2000', 'Iteration: 5710', 'Validation loss: 0.403613', 'Validation acc: 0.918333')\n",
      "('Epoch: 1142/2000', 'Iteration: 5715', 'Train loss: 0.026796', 'Train acc: 0.991667')\n",
      "('Epoch: 1143/2000', 'Iteration: 5720', 'Train loss: 0.025663', 'Train acc: 0.996667')\n",
      "('Epoch: 1143/2000', 'Iteration: 5720', 'Validation loss: 0.403190', 'Validation acc: 0.921667')\n",
      "('Epoch: 1144/2000', 'Iteration: 5725', 'Train loss: 0.024477', 'Train acc: 0.991667')\n",
      "('Epoch: 1145/2000', 'Iteration: 5730', 'Train loss: 0.022008', 'Train acc: 0.995000')\n",
      "('Epoch: 1145/2000', 'Iteration: 5730', 'Validation loss: 0.407811', 'Validation acc: 0.921667')\n",
      "('Epoch: 1146/2000', 'Iteration: 5735', 'Train loss: 0.024444', 'Train acc: 0.995000')\n",
      "('Epoch: 1147/2000', 'Iteration: 5740', 'Train loss: 0.029258', 'Train acc: 0.993333')\n",
      "('Epoch: 1147/2000', 'Iteration: 5740', 'Validation loss: 0.408544', 'Validation acc: 0.923333')\n",
      "('Epoch: 1148/2000', 'Iteration: 5745', 'Train loss: 0.033925', 'Train acc: 0.990000')\n",
      "('Epoch: 1149/2000', 'Iteration: 5750', 'Train loss: 0.034163', 'Train acc: 0.991667')\n",
      "('Epoch: 1149/2000', 'Iteration: 5750', 'Validation loss: 0.409733', 'Validation acc: 0.920000')\n",
      "('Epoch: 1150/2000', 'Iteration: 5755', 'Train loss: 0.024160', 'Train acc: 0.993333')\n",
      "('Epoch: 1151/2000', 'Iteration: 5760', 'Train loss: 0.029259', 'Train acc: 0.991667')\n",
      "('Epoch: 1151/2000', 'Iteration: 5760', 'Validation loss: 0.407711', 'Validation acc: 0.915000')\n",
      "('Epoch: 1152/2000', 'Iteration: 5765', 'Train loss: 0.030301', 'Train acc: 0.991667')\n",
      "('Epoch: 1153/2000', 'Iteration: 5770', 'Train loss: 0.027660', 'Train acc: 0.988333')\n",
      "('Epoch: 1153/2000', 'Iteration: 5770', 'Validation loss: 0.406953', 'Validation acc: 0.925000')\n",
      "('Epoch: 1154/2000', 'Iteration: 5775', 'Train loss: 0.032168', 'Train acc: 0.991667')\n",
      "('Epoch: 1155/2000', 'Iteration: 5780', 'Train loss: 0.033653', 'Train acc: 0.990000')\n",
      "('Epoch: 1155/2000', 'Iteration: 5780', 'Validation loss: 0.405718', 'Validation acc: 0.920000')\n",
      "('Epoch: 1156/2000', 'Iteration: 5785', 'Train loss: 0.027600', 'Train acc: 0.991667')\n",
      "('Epoch: 1157/2000', 'Iteration: 5790', 'Train loss: 0.027384', 'Train acc: 0.991667')\n",
      "('Epoch: 1157/2000', 'Iteration: 5790', 'Validation loss: 0.406293', 'Validation acc: 0.920000')\n",
      "('Epoch: 1158/2000', 'Iteration: 5795', 'Train loss: 0.026672', 'Train acc: 0.996667')\n",
      "('Epoch: 1159/2000', 'Iteration: 5800', 'Train loss: 0.031842', 'Train acc: 0.991667')\n",
      "('Epoch: 1159/2000', 'Iteration: 5800', 'Validation loss: 0.408786', 'Validation acc: 0.918333')\n",
      "('Epoch: 1160/2000', 'Iteration: 5805', 'Train loss: 0.026274', 'Train acc: 0.991667')\n",
      "('Epoch: 1161/2000', 'Iteration: 5810', 'Train loss: 0.027341', 'Train acc: 0.993333')\n",
      "('Epoch: 1161/2000', 'Iteration: 5810', 'Validation loss: 0.410948', 'Validation acc: 0.921667')\n",
      "('Epoch: 1162/2000', 'Iteration: 5815', 'Train loss: 0.029924', 'Train acc: 0.990000')\n",
      "('Epoch: 1163/2000', 'Iteration: 5820', 'Train loss: 0.027002', 'Train acc: 0.990000')\n",
      "('Epoch: 1163/2000', 'Iteration: 5820', 'Validation loss: 0.407315', 'Validation acc: 0.923333')\n",
      "('Epoch: 1164/2000', 'Iteration: 5825', 'Train loss: 0.035544', 'Train acc: 0.988333')\n",
      "('Epoch: 1165/2000', 'Iteration: 5830', 'Train loss: 0.029087', 'Train acc: 0.995000')\n",
      "('Epoch: 1165/2000', 'Iteration: 5830', 'Validation loss: 0.403827', 'Validation acc: 0.921667')\n",
      "('Epoch: 1166/2000', 'Iteration: 5835', 'Train loss: 0.016520', 'Train acc: 0.998333')\n",
      "('Epoch: 1167/2000', 'Iteration: 5840', 'Train loss: 0.020039', 'Train acc: 0.995000')\n",
      "('Epoch: 1167/2000', 'Iteration: 5840', 'Validation loss: 0.402157', 'Validation acc: 0.923333')\n",
      "('Epoch: 1168/2000', 'Iteration: 5845', 'Train loss: 0.020003', 'Train acc: 0.996667')\n",
      "('Epoch: 1169/2000', 'Iteration: 5850', 'Train loss: 0.030540', 'Train acc: 0.991667')\n",
      "('Epoch: 1169/2000', 'Iteration: 5850', 'Validation loss: 0.410839', 'Validation acc: 0.925000')\n",
      "('Epoch: 1170/2000', 'Iteration: 5855', 'Train loss: 0.023415', 'Train acc: 0.993333')\n",
      "('Epoch: 1171/2000', 'Iteration: 5860', 'Train loss: 0.022536', 'Train acc: 0.995000')\n",
      "('Epoch: 1171/2000', 'Iteration: 5860', 'Validation loss: 0.414896', 'Validation acc: 0.923333')\n",
      "('Epoch: 1172/2000', 'Iteration: 5865', 'Train loss: 0.024231', 'Train acc: 0.996667')\n",
      "('Epoch: 1173/2000', 'Iteration: 5870', 'Train loss: 0.021463', 'Train acc: 0.996667')\n",
      "('Epoch: 1173/2000', 'Iteration: 5870', 'Validation loss: 0.406669', 'Validation acc: 0.918333')\n",
      "('Epoch: 1174/2000', 'Iteration: 5875', 'Train loss: 0.031621', 'Train acc: 0.991667')\n",
      "('Epoch: 1175/2000', 'Iteration: 5880', 'Train loss: 0.026895', 'Train acc: 0.993333')\n",
      "('Epoch: 1175/2000', 'Iteration: 5880', 'Validation loss: 0.406133', 'Validation acc: 0.925000')\n",
      "('Epoch: 1176/2000', 'Iteration: 5885', 'Train loss: 0.028143', 'Train acc: 0.991667')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 1177/2000', 'Iteration: 5890', 'Train loss: 0.024644', 'Train acc: 0.995000')\n",
      "('Epoch: 1177/2000', 'Iteration: 5890', 'Validation loss: 0.410100', 'Validation acc: 0.921667')\n",
      "('Epoch: 1178/2000', 'Iteration: 5895', 'Train loss: 0.037818', 'Train acc: 0.986667')\n",
      "('Epoch: 1179/2000', 'Iteration: 5900', 'Train loss: 0.026559', 'Train acc: 0.995000')\n",
      "('Epoch: 1179/2000', 'Iteration: 5900', 'Validation loss: 0.416337', 'Validation acc: 0.921667')\n",
      "('Epoch: 1180/2000', 'Iteration: 5905', 'Train loss: 0.022317', 'Train acc: 0.998333')\n",
      "('Epoch: 1181/2000', 'Iteration: 5910', 'Train loss: 0.023165', 'Train acc: 0.998333')\n",
      "('Epoch: 1181/2000', 'Iteration: 5910', 'Validation loss: 0.406670', 'Validation acc: 0.921667')\n",
      "('Epoch: 1182/2000', 'Iteration: 5915', 'Train loss: 0.023538', 'Train acc: 0.995000')\n",
      "('Epoch: 1183/2000', 'Iteration: 5920', 'Train loss: 0.019115', 'Train acc: 0.995000')\n",
      "('Epoch: 1183/2000', 'Iteration: 5920', 'Validation loss: 0.405583', 'Validation acc: 0.923333')\n",
      "('Epoch: 1184/2000', 'Iteration: 5925', 'Train loss: 0.028350', 'Train acc: 0.990000')\n",
      "('Epoch: 1185/2000', 'Iteration: 5930', 'Train loss: 0.018366', 'Train acc: 0.998333')\n",
      "('Epoch: 1185/2000', 'Iteration: 5930', 'Validation loss: 0.410770', 'Validation acc: 0.918333')\n",
      "('Epoch: 1186/2000', 'Iteration: 5935', 'Train loss: 0.023446', 'Train acc: 0.995000')\n",
      "('Epoch: 1187/2000', 'Iteration: 5940', 'Train loss: 0.023724', 'Train acc: 0.996667')\n",
      "('Epoch: 1187/2000', 'Iteration: 5940', 'Validation loss: 0.405868', 'Validation acc: 0.921667')\n",
      "('Epoch: 1188/2000', 'Iteration: 5945', 'Train loss: 0.025384', 'Train acc: 0.995000')\n",
      "('Epoch: 1189/2000', 'Iteration: 5950', 'Train loss: 0.034547', 'Train acc: 0.991667')\n",
      "('Epoch: 1189/2000', 'Iteration: 5950', 'Validation loss: 0.418288', 'Validation acc: 0.921667')\n",
      "('Epoch: 1190/2000', 'Iteration: 5955', 'Train loss: 0.025849', 'Train acc: 0.993333')\n",
      "('Epoch: 1191/2000', 'Iteration: 5960', 'Train loss: 0.025783', 'Train acc: 0.993333')\n",
      "('Epoch: 1191/2000', 'Iteration: 5960', 'Validation loss: 0.412992', 'Validation acc: 0.923333')\n",
      "('Epoch: 1192/2000', 'Iteration: 5965', 'Train loss: 0.017441', 'Train acc: 0.998333')\n",
      "('Epoch: 1193/2000', 'Iteration: 5970', 'Train loss: 0.024861', 'Train acc: 0.995000')\n",
      "('Epoch: 1193/2000', 'Iteration: 5970', 'Validation loss: 0.414666', 'Validation acc: 0.921667')\n",
      "('Epoch: 1194/2000', 'Iteration: 5975', 'Train loss: 0.028647', 'Train acc: 0.995000')\n",
      "('Epoch: 1195/2000', 'Iteration: 5980', 'Train loss: 0.024838', 'Train acc: 0.995000')\n",
      "('Epoch: 1195/2000', 'Iteration: 5980', 'Validation loss: 0.413680', 'Validation acc: 0.916667')\n",
      "('Epoch: 1196/2000', 'Iteration: 5985', 'Train loss: 0.021286', 'Train acc: 0.995000')\n",
      "('Epoch: 1197/2000', 'Iteration: 5990', 'Train loss: 0.025938', 'Train acc: 0.993333')\n",
      "('Epoch: 1197/2000', 'Iteration: 5990', 'Validation loss: 0.409305', 'Validation acc: 0.923333')\n",
      "('Epoch: 1198/2000', 'Iteration: 5995', 'Train loss: 0.022761', 'Train acc: 0.998333')\n",
      "('Epoch: 1199/2000', 'Iteration: 6000', 'Train loss: 0.023611', 'Train acc: 0.995000')\n",
      "('Epoch: 1199/2000', 'Iteration: 6000', 'Validation loss: 0.415848', 'Validation acc: 0.926667')\n",
      "('Epoch: 1200/2000', 'Iteration: 6005', 'Train loss: 0.021438', 'Train acc: 0.996667')\n",
      "('Epoch: 1201/2000', 'Iteration: 6010', 'Train loss: 0.026579', 'Train acc: 0.993333')\n",
      "('Epoch: 1201/2000', 'Iteration: 6010', 'Validation loss: 0.417006', 'Validation acc: 0.920000')\n",
      "('Epoch: 1202/2000', 'Iteration: 6015', 'Train loss: 0.017078', 'Train acc: 1.000000')\n",
      "('Epoch: 1203/2000', 'Iteration: 6020', 'Train loss: 0.031082', 'Train acc: 0.993333')\n",
      "('Epoch: 1203/2000', 'Iteration: 6020', 'Validation loss: 0.416052', 'Validation acc: 0.921667')\n",
      "('Epoch: 1204/2000', 'Iteration: 6025', 'Train loss: 0.020773', 'Train acc: 0.995000')\n",
      "('Epoch: 1205/2000', 'Iteration: 6030', 'Train loss: 0.022030', 'Train acc: 0.995000')\n",
      "('Epoch: 1205/2000', 'Iteration: 6030', 'Validation loss: 0.421026', 'Validation acc: 0.920000')\n",
      "('Epoch: 1206/2000', 'Iteration: 6035', 'Train loss: 0.025433', 'Train acc: 0.996667')\n",
      "('Epoch: 1207/2000', 'Iteration: 6040', 'Train loss: 0.018480', 'Train acc: 0.998333')\n",
      "('Epoch: 1207/2000', 'Iteration: 6040', 'Validation loss: 0.415454', 'Validation acc: 0.921667')\n",
      "('Epoch: 1208/2000', 'Iteration: 6045', 'Train loss: 0.023055', 'Train acc: 0.996667')\n",
      "('Epoch: 1209/2000', 'Iteration: 6050', 'Train loss: 0.029890', 'Train acc: 0.991667')\n",
      "('Epoch: 1209/2000', 'Iteration: 6050', 'Validation loss: 0.421464', 'Validation acc: 0.918333')\n",
      "('Epoch: 1210/2000', 'Iteration: 6055', 'Train loss: 0.019136', 'Train acc: 0.993333')\n",
      "('Epoch: 1211/2000', 'Iteration: 6060', 'Train loss: 0.017545', 'Train acc: 0.996667')\n",
      "('Epoch: 1211/2000', 'Iteration: 6060', 'Validation loss: 0.417694', 'Validation acc: 0.916667')\n",
      "('Epoch: 1212/2000', 'Iteration: 6065', 'Train loss: 0.026053', 'Train acc: 0.991667')\n",
      "('Epoch: 1213/2000', 'Iteration: 6070', 'Train loss: 0.016365', 'Train acc: 0.996667')\n",
      "('Epoch: 1213/2000', 'Iteration: 6070', 'Validation loss: 0.415104', 'Validation acc: 0.921667')\n",
      "('Epoch: 1214/2000', 'Iteration: 6075', 'Train loss: 0.026163', 'Train acc: 0.993333')\n",
      "('Epoch: 1215/2000', 'Iteration: 6080', 'Train loss: 0.020962', 'Train acc: 0.996667')\n",
      "('Epoch: 1215/2000', 'Iteration: 6080', 'Validation loss: 0.416648', 'Validation acc: 0.918333')\n",
      "('Epoch: 1216/2000', 'Iteration: 6085', 'Train loss: 0.025238', 'Train acc: 0.993333')\n",
      "('Epoch: 1217/2000', 'Iteration: 6090', 'Train loss: 0.022491', 'Train acc: 0.995000')\n",
      "('Epoch: 1217/2000', 'Iteration: 6090', 'Validation loss: 0.417755', 'Validation acc: 0.921667')\n",
      "('Epoch: 1218/2000', 'Iteration: 6095', 'Train loss: 0.022396', 'Train acc: 0.996667')\n",
      "('Epoch: 1219/2000', 'Iteration: 6100', 'Train loss: 0.023405', 'Train acc: 0.995000')\n",
      "('Epoch: 1219/2000', 'Iteration: 6100', 'Validation loss: 0.416077', 'Validation acc: 0.920000')\n",
      "('Epoch: 1220/2000', 'Iteration: 6105', 'Train loss: 0.020624', 'Train acc: 0.998333')\n",
      "('Epoch: 1221/2000', 'Iteration: 6110', 'Train loss: 0.024302', 'Train acc: 0.996667')\n",
      "('Epoch: 1221/2000', 'Iteration: 6110', 'Validation loss: 0.410323', 'Validation acc: 0.925000')\n",
      "('Epoch: 1222/2000', 'Iteration: 6115', 'Train loss: 0.024760', 'Train acc: 0.995000')\n",
      "('Epoch: 1223/2000', 'Iteration: 6120', 'Train loss: 0.024033', 'Train acc: 0.996667')\n",
      "('Epoch: 1223/2000', 'Iteration: 6120', 'Validation loss: 0.413494', 'Validation acc: 0.923333')\n",
      "('Epoch: 1224/2000', 'Iteration: 6125', 'Train loss: 0.020903', 'Train acc: 0.996667')\n",
      "('Epoch: 1225/2000', 'Iteration: 6130', 'Train loss: 0.021712', 'Train acc: 0.996667')\n",
      "('Epoch: 1225/2000', 'Iteration: 6130', 'Validation loss: 0.421804', 'Validation acc: 0.923333')\n",
      "('Epoch: 1226/2000', 'Iteration: 6135', 'Train loss: 0.021979', 'Train acc: 0.995000')\n",
      "('Epoch: 1227/2000', 'Iteration: 6140', 'Train loss: 0.028195', 'Train acc: 0.995000')\n",
      "('Epoch: 1227/2000', 'Iteration: 6140', 'Validation loss: 0.418835', 'Validation acc: 0.921667')\n",
      "('Epoch: 1228/2000', 'Iteration: 6145', 'Train loss: 0.021579', 'Train acc: 0.995000')\n",
      "('Epoch: 1229/2000', 'Iteration: 6150', 'Train loss: 0.029568', 'Train acc: 0.993333')\n",
      "('Epoch: 1229/2000', 'Iteration: 6150', 'Validation loss: 0.420473', 'Validation acc: 0.923333')\n",
      "('Epoch: 1230/2000', 'Iteration: 6155', 'Train loss: 0.019895', 'Train acc: 0.995000')\n",
      "('Epoch: 1231/2000', 'Iteration: 6160', 'Train loss: 0.017798', 'Train acc: 0.996667')\n",
      "('Epoch: 1231/2000', 'Iteration: 6160', 'Validation loss: 0.422777', 'Validation acc: 0.923333')\n",
      "('Epoch: 1232/2000', 'Iteration: 6165', 'Train loss: 0.018597', 'Train acc: 0.998333')\n",
      "('Epoch: 1233/2000', 'Iteration: 6170', 'Train loss: 0.024376', 'Train acc: 0.993333')\n",
      "('Epoch: 1233/2000', 'Iteration: 6170', 'Validation loss: 0.413974', 'Validation acc: 0.920000')\n",
      "('Epoch: 1234/2000', 'Iteration: 6175', 'Train loss: 0.021452', 'Train acc: 0.996667')\n",
      "('Epoch: 1235/2000', 'Iteration: 6180', 'Train loss: 0.021336', 'Train acc: 0.993333')\n",
      "('Epoch: 1235/2000', 'Iteration: 6180', 'Validation loss: 0.420614', 'Validation acc: 0.920000')\n",
      "('Epoch: 1236/2000', 'Iteration: 6185', 'Train loss: 0.020011', 'Train acc: 0.995000')\n",
      "('Epoch: 1237/2000', 'Iteration: 6190', 'Train loss: 0.030088', 'Train acc: 0.991667')\n",
      "('Epoch: 1237/2000', 'Iteration: 6190', 'Validation loss: 0.421477', 'Validation acc: 0.921667')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 1238/2000', 'Iteration: 6195', 'Train loss: 0.014971', 'Train acc: 0.998333')\n",
      "('Epoch: 1239/2000', 'Iteration: 6200', 'Train loss: 0.020849', 'Train acc: 0.996667')\n",
      "('Epoch: 1239/2000', 'Iteration: 6200', 'Validation loss: 0.419159', 'Validation acc: 0.920000')\n",
      "('Epoch: 1240/2000', 'Iteration: 6205', 'Train loss: 0.019891', 'Train acc: 0.998333')\n",
      "('Epoch: 1241/2000', 'Iteration: 6210', 'Train loss: 0.024244', 'Train acc: 0.996667')\n",
      "('Epoch: 1241/2000', 'Iteration: 6210', 'Validation loss: 0.416366', 'Validation acc: 0.918333')\n",
      "('Epoch: 1242/2000', 'Iteration: 6215', 'Train loss: 0.031358', 'Train acc: 0.990000')\n",
      "('Epoch: 1243/2000', 'Iteration: 6220', 'Train loss: 0.024068', 'Train acc: 0.998333')\n",
      "('Epoch: 1243/2000', 'Iteration: 6220', 'Validation loss: 0.415994', 'Validation acc: 0.923333')\n",
      "('Epoch: 1244/2000', 'Iteration: 6225', 'Train loss: 0.020251', 'Train acc: 0.995000')\n",
      "('Epoch: 1245/2000', 'Iteration: 6230', 'Train loss: 0.021164', 'Train acc: 0.995000')\n",
      "('Epoch: 1245/2000', 'Iteration: 6230', 'Validation loss: 0.418062', 'Validation acc: 0.923333')\n",
      "('Epoch: 1246/2000', 'Iteration: 6235', 'Train loss: 0.020951', 'Train acc: 0.993333')\n",
      "('Epoch: 1247/2000', 'Iteration: 6240', 'Train loss: 0.024457', 'Train acc: 0.993333')\n",
      "('Epoch: 1247/2000', 'Iteration: 6240', 'Validation loss: 0.418146', 'Validation acc: 0.925000')\n",
      "('Epoch: 1248/2000', 'Iteration: 6245', 'Train loss: 0.024269', 'Train acc: 0.993333')\n",
      "('Epoch: 1249/2000', 'Iteration: 6250', 'Train loss: 0.031379', 'Train acc: 0.991667')\n",
      "('Epoch: 1249/2000', 'Iteration: 6250', 'Validation loss: 0.420678', 'Validation acc: 0.918333')\n",
      "('Epoch: 1250/2000', 'Iteration: 6255', 'Train loss: 0.023350', 'Train acc: 0.993333')\n",
      "('Epoch: 1251/2000', 'Iteration: 6260', 'Train loss: 0.021372', 'Train acc: 1.000000')\n",
      "('Epoch: 1251/2000', 'Iteration: 6260', 'Validation loss: 0.417299', 'Validation acc: 0.920000')\n",
      "('Epoch: 1252/2000', 'Iteration: 6265', 'Train loss: 0.024296', 'Train acc: 0.991667')\n",
      "('Epoch: 1253/2000', 'Iteration: 6270', 'Train loss: 0.023438', 'Train acc: 0.996667')\n",
      "('Epoch: 1253/2000', 'Iteration: 6270', 'Validation loss: 0.420550', 'Validation acc: 0.923333')\n",
      "('Epoch: 1254/2000', 'Iteration: 6275', 'Train loss: 0.023195', 'Train acc: 0.996667')\n",
      "('Epoch: 1255/2000', 'Iteration: 6280', 'Train loss: 0.017951', 'Train acc: 1.000000')\n",
      "('Epoch: 1255/2000', 'Iteration: 6280', 'Validation loss: 0.423488', 'Validation acc: 0.921667')\n",
      "('Epoch: 1256/2000', 'Iteration: 6285', 'Train loss: 0.021673', 'Train acc: 0.996667')\n",
      "('Epoch: 1257/2000', 'Iteration: 6290', 'Train loss: 0.021972', 'Train acc: 0.996667')\n",
      "('Epoch: 1257/2000', 'Iteration: 6290', 'Validation loss: 0.424072', 'Validation acc: 0.923333')\n",
      "('Epoch: 1258/2000', 'Iteration: 6295', 'Train loss: 0.025943', 'Train acc: 0.995000')\n",
      "('Epoch: 1259/2000', 'Iteration: 6300', 'Train loss: 0.027524', 'Train acc: 0.995000')\n",
      "('Epoch: 1259/2000', 'Iteration: 6300', 'Validation loss: 0.424395', 'Validation acc: 0.921667')\n",
      "('Epoch: 1260/2000', 'Iteration: 6305', 'Train loss: 0.023000', 'Train acc: 0.993333')\n",
      "('Epoch: 1261/2000', 'Iteration: 6310', 'Train loss: 0.026395', 'Train acc: 0.993333')\n",
      "('Epoch: 1261/2000', 'Iteration: 6310', 'Validation loss: 0.416300', 'Validation acc: 0.921667')\n",
      "('Epoch: 1262/2000', 'Iteration: 6315', 'Train loss: 0.017972', 'Train acc: 0.995000')\n",
      "('Epoch: 1263/2000', 'Iteration: 6320', 'Train loss: 0.021258', 'Train acc: 0.996667')\n",
      "('Epoch: 1263/2000', 'Iteration: 6320', 'Validation loss: 0.422936', 'Validation acc: 0.921667')\n",
      "('Epoch: 1264/2000', 'Iteration: 6325', 'Train loss: 0.022984', 'Train acc: 0.993333')\n",
      "('Epoch: 1265/2000', 'Iteration: 6330', 'Train loss: 0.024960', 'Train acc: 0.996667')\n",
      "('Epoch: 1265/2000', 'Iteration: 6330', 'Validation loss: 0.423529', 'Validation acc: 0.923333')\n",
      "('Epoch: 1266/2000', 'Iteration: 6335', 'Train loss: 0.022786', 'Train acc: 0.996667')\n",
      "('Epoch: 1267/2000', 'Iteration: 6340', 'Train loss: 0.027743', 'Train acc: 0.991667')\n",
      "('Epoch: 1267/2000', 'Iteration: 6340', 'Validation loss: 0.422552', 'Validation acc: 0.918333')\n",
      "('Epoch: 1268/2000', 'Iteration: 6345', 'Train loss: 0.022867', 'Train acc: 0.995000')\n",
      "('Epoch: 1269/2000', 'Iteration: 6350', 'Train loss: 0.016685', 'Train acc: 0.996667')\n",
      "('Epoch: 1269/2000', 'Iteration: 6350', 'Validation loss: 0.422255', 'Validation acc: 0.923333')\n",
      "('Epoch: 1270/2000', 'Iteration: 6355', 'Train loss: 0.021233', 'Train acc: 0.995000')\n",
      "('Epoch: 1271/2000', 'Iteration: 6360', 'Train loss: 0.020698', 'Train acc: 0.995000')\n",
      "('Epoch: 1271/2000', 'Iteration: 6360', 'Validation loss: 0.427236', 'Validation acc: 0.921667')\n",
      "('Epoch: 1272/2000', 'Iteration: 6365', 'Train loss: 0.020726', 'Train acc: 0.995000')\n",
      "('Epoch: 1273/2000', 'Iteration: 6370', 'Train loss: 0.021845', 'Train acc: 0.996667')\n",
      "('Epoch: 1273/2000', 'Iteration: 6370', 'Validation loss: 0.414055', 'Validation acc: 0.928333')\n",
      "('Epoch: 1274/2000', 'Iteration: 6375', 'Train loss: 0.020889', 'Train acc: 0.996667')\n",
      "('Epoch: 1275/2000', 'Iteration: 6380', 'Train loss: 0.022757', 'Train acc: 0.993333')\n",
      "('Epoch: 1275/2000', 'Iteration: 6380', 'Validation loss: 0.421325', 'Validation acc: 0.921667')\n",
      "('Epoch: 1276/2000', 'Iteration: 6385', 'Train loss: 0.030110', 'Train acc: 0.986667')\n",
      "('Epoch: 1277/2000', 'Iteration: 6390', 'Train loss: 0.022925', 'Train acc: 0.993333')\n",
      "('Epoch: 1277/2000', 'Iteration: 6390', 'Validation loss: 0.424016', 'Validation acc: 0.921667')\n",
      "('Epoch: 1278/2000', 'Iteration: 6395', 'Train loss: 0.020678', 'Train acc: 0.996667')\n",
      "('Epoch: 1279/2000', 'Iteration: 6400', 'Train loss: 0.013702', 'Train acc: 1.000000')\n",
      "('Epoch: 1279/2000', 'Iteration: 6400', 'Validation loss: 0.428211', 'Validation acc: 0.915000')\n",
      "('Epoch: 1280/2000', 'Iteration: 6405', 'Train loss: 0.020272', 'Train acc: 0.998333')\n",
      "('Epoch: 1281/2000', 'Iteration: 6410', 'Train loss: 0.024191', 'Train acc: 0.993333')\n",
      "('Epoch: 1281/2000', 'Iteration: 6410', 'Validation loss: 0.420027', 'Validation acc: 0.925000')\n",
      "('Epoch: 1282/2000', 'Iteration: 6415', 'Train loss: 0.021264', 'Train acc: 0.995000')\n",
      "('Epoch: 1283/2000', 'Iteration: 6420', 'Train loss: 0.022951', 'Train acc: 0.995000')\n",
      "('Epoch: 1283/2000', 'Iteration: 6420', 'Validation loss: 0.412915', 'Validation acc: 0.923333')\n",
      "('Epoch: 1284/2000', 'Iteration: 6425', 'Train loss: 0.019817', 'Train acc: 0.995000')\n",
      "('Epoch: 1285/2000', 'Iteration: 6430', 'Train loss: 0.018428', 'Train acc: 0.991667')\n",
      "('Epoch: 1285/2000', 'Iteration: 6430', 'Validation loss: 0.420407', 'Validation acc: 0.916667')\n",
      "('Epoch: 1286/2000', 'Iteration: 6435', 'Train loss: 0.021634', 'Train acc: 0.995000')\n",
      "('Epoch: 1287/2000', 'Iteration: 6440', 'Train loss: 0.020737', 'Train acc: 0.991667')\n",
      "('Epoch: 1287/2000', 'Iteration: 6440', 'Validation loss: 0.418982', 'Validation acc: 0.920000')\n",
      "('Epoch: 1288/2000', 'Iteration: 6445', 'Train loss: 0.020748', 'Train acc: 0.996667')\n",
      "('Epoch: 1289/2000', 'Iteration: 6450', 'Train loss: 0.016930', 'Train acc: 0.995000')\n",
      "('Epoch: 1289/2000', 'Iteration: 6450', 'Validation loss: 0.420093', 'Validation acc: 0.926667')\n",
      "('Epoch: 1290/2000', 'Iteration: 6455', 'Train loss: 0.022930', 'Train acc: 0.995000')\n",
      "('Epoch: 1291/2000', 'Iteration: 6460', 'Train loss: 0.024486', 'Train acc: 0.991667')\n",
      "('Epoch: 1291/2000', 'Iteration: 6460', 'Validation loss: 0.422735', 'Validation acc: 0.921667')\n",
      "('Epoch: 1292/2000', 'Iteration: 6465', 'Train loss: 0.021659', 'Train acc: 0.995000')\n",
      "('Epoch: 1293/2000', 'Iteration: 6470', 'Train loss: 0.025605', 'Train acc: 0.993333')\n",
      "('Epoch: 1293/2000', 'Iteration: 6470', 'Validation loss: 0.422933', 'Validation acc: 0.920000')\n",
      "('Epoch: 1294/2000', 'Iteration: 6475', 'Train loss: 0.011679', 'Train acc: 0.998333')\n",
      "('Epoch: 1295/2000', 'Iteration: 6480', 'Train loss: 0.027612', 'Train acc: 0.991667')\n",
      "('Epoch: 1295/2000', 'Iteration: 6480', 'Validation loss: 0.423071', 'Validation acc: 0.923333')\n",
      "('Epoch: 1296/2000', 'Iteration: 6485', 'Train loss: 0.020548', 'Train acc: 0.993333')\n",
      "('Epoch: 1297/2000', 'Iteration: 6490', 'Train loss: 0.027157', 'Train acc: 0.991667')\n",
      "('Epoch: 1297/2000', 'Iteration: 6490', 'Validation loss: 0.422338', 'Validation acc: 0.921667')\n",
      "('Epoch: 1298/2000', 'Iteration: 6495', 'Train loss: 0.016929', 'Train acc: 0.998333')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 1299/2000', 'Iteration: 6500', 'Train loss: 0.014197', 'Train acc: 1.000000')\n",
      "('Epoch: 1299/2000', 'Iteration: 6500', 'Validation loss: 0.423943', 'Validation acc: 0.928333')\n",
      "('Epoch: 1300/2000', 'Iteration: 6505', 'Train loss: 0.019575', 'Train acc: 0.995000')\n",
      "('Epoch: 1301/2000', 'Iteration: 6510', 'Train loss: 0.015731', 'Train acc: 0.998333')\n",
      "('Epoch: 1301/2000', 'Iteration: 6510', 'Validation loss: 0.428158', 'Validation acc: 0.921667')\n",
      "('Epoch: 1302/2000', 'Iteration: 6515', 'Train loss: 0.019946', 'Train acc: 0.996667')\n",
      "('Epoch: 1303/2000', 'Iteration: 6520', 'Train loss: 0.021286', 'Train acc: 0.993333')\n",
      "('Epoch: 1303/2000', 'Iteration: 6520', 'Validation loss: 0.421756', 'Validation acc: 0.921667')\n",
      "('Epoch: 1304/2000', 'Iteration: 6525', 'Train loss: 0.010197', 'Train acc: 1.000000')\n",
      "('Epoch: 1305/2000', 'Iteration: 6530', 'Train loss: 0.016757', 'Train acc: 0.998333')\n",
      "('Epoch: 1305/2000', 'Iteration: 6530', 'Validation loss: 0.424661', 'Validation acc: 0.920000')\n",
      "('Epoch: 1306/2000', 'Iteration: 6535', 'Train loss: 0.023257', 'Train acc: 0.995000')\n",
      "('Epoch: 1307/2000', 'Iteration: 6540', 'Train loss: 0.021114', 'Train acc: 0.996667')\n",
      "('Epoch: 1307/2000', 'Iteration: 6540', 'Validation loss: 0.420666', 'Validation acc: 0.921667')\n",
      "('Epoch: 1308/2000', 'Iteration: 6545', 'Train loss: 0.022903', 'Train acc: 0.993333')\n",
      "('Epoch: 1309/2000', 'Iteration: 6550', 'Train loss: 0.016563', 'Train acc: 0.998333')\n",
      "('Epoch: 1309/2000', 'Iteration: 6550', 'Validation loss: 0.416197', 'Validation acc: 0.923333')\n",
      "('Epoch: 1310/2000', 'Iteration: 6555', 'Train loss: 0.015115', 'Train acc: 0.998333')\n",
      "('Epoch: 1311/2000', 'Iteration: 6560', 'Train loss: 0.022731', 'Train acc: 0.993333')\n",
      "('Epoch: 1311/2000', 'Iteration: 6560', 'Validation loss: 0.420759', 'Validation acc: 0.918333')\n",
      "('Epoch: 1312/2000', 'Iteration: 6565', 'Train loss: 0.016469', 'Train acc: 0.998333')\n",
      "('Epoch: 1313/2000', 'Iteration: 6570', 'Train loss: 0.018089', 'Train acc: 0.998333')\n",
      "('Epoch: 1313/2000', 'Iteration: 6570', 'Validation loss: 0.419773', 'Validation acc: 0.926667')\n",
      "('Epoch: 1314/2000', 'Iteration: 6575', 'Train loss: 0.017568', 'Train acc: 0.996667')\n",
      "('Epoch: 1315/2000', 'Iteration: 6580', 'Train loss: 0.021488', 'Train acc: 0.993333')\n",
      "('Epoch: 1315/2000', 'Iteration: 6580', 'Validation loss: 0.429718', 'Validation acc: 0.918333')\n",
      "('Epoch: 1316/2000', 'Iteration: 6585', 'Train loss: 0.019267', 'Train acc: 0.996667')\n",
      "('Epoch: 1317/2000', 'Iteration: 6590', 'Train loss: 0.016136', 'Train acc: 0.998333')\n",
      "('Epoch: 1317/2000', 'Iteration: 6590', 'Validation loss: 0.421284', 'Validation acc: 0.925000')\n",
      "('Epoch: 1318/2000', 'Iteration: 6595', 'Train loss: 0.017467', 'Train acc: 0.998333')\n",
      "('Epoch: 1319/2000', 'Iteration: 6600', 'Train loss: 0.022851', 'Train acc: 0.991667')\n",
      "('Epoch: 1319/2000', 'Iteration: 6600', 'Validation loss: 0.426133', 'Validation acc: 0.921667')\n",
      "('Epoch: 1320/2000', 'Iteration: 6605', 'Train loss: 0.015495', 'Train acc: 0.998333')\n",
      "('Epoch: 1321/2000', 'Iteration: 6610', 'Train loss: 0.023559', 'Train acc: 0.993333')\n",
      "('Epoch: 1321/2000', 'Iteration: 6610', 'Validation loss: 0.429090', 'Validation acc: 0.925000')\n",
      "('Epoch: 1322/2000', 'Iteration: 6615', 'Train loss: 0.015700', 'Train acc: 0.996667')\n",
      "('Epoch: 1323/2000', 'Iteration: 6620', 'Train loss: 0.014056', 'Train acc: 0.998333')\n",
      "('Epoch: 1323/2000', 'Iteration: 6620', 'Validation loss: 0.426582', 'Validation acc: 0.925000')\n",
      "('Epoch: 1324/2000', 'Iteration: 6625', 'Train loss: 0.013799', 'Train acc: 0.998333')\n",
      "('Epoch: 1325/2000', 'Iteration: 6630', 'Train loss: 0.018116', 'Train acc: 0.998333')\n",
      "('Epoch: 1325/2000', 'Iteration: 6630', 'Validation loss: 0.419760', 'Validation acc: 0.925000')\n",
      "('Epoch: 1326/2000', 'Iteration: 6635', 'Train loss: 0.024370', 'Train acc: 0.995000')\n",
      "('Epoch: 1327/2000', 'Iteration: 6640', 'Train loss: 0.016648', 'Train acc: 0.998333')\n",
      "('Epoch: 1327/2000', 'Iteration: 6640', 'Validation loss: 0.421430', 'Validation acc: 0.923333')\n",
      "('Epoch: 1328/2000', 'Iteration: 6645', 'Train loss: 0.019839', 'Train acc: 0.996667')\n",
      "('Epoch: 1329/2000', 'Iteration: 6650', 'Train loss: 0.014109', 'Train acc: 0.998333')\n",
      "('Epoch: 1329/2000', 'Iteration: 6650', 'Validation loss: 0.419088', 'Validation acc: 0.925000')\n",
      "('Epoch: 1330/2000', 'Iteration: 6655', 'Train loss: 0.018687', 'Train acc: 0.996667')\n",
      "('Epoch: 1331/2000', 'Iteration: 6660', 'Train loss: 0.022363', 'Train acc: 0.991667')\n",
      "('Epoch: 1331/2000', 'Iteration: 6660', 'Validation loss: 0.427274', 'Validation acc: 0.915000')\n",
      "('Epoch: 1332/2000', 'Iteration: 6665', 'Train loss: 0.016253', 'Train acc: 0.996667')\n",
      "('Epoch: 1333/2000', 'Iteration: 6670', 'Train loss: 0.022599', 'Train acc: 0.993333')\n",
      "('Epoch: 1333/2000', 'Iteration: 6670', 'Validation loss: 0.426819', 'Validation acc: 0.920000')\n",
      "('Epoch: 1334/2000', 'Iteration: 6675', 'Train loss: 0.022853', 'Train acc: 0.993333')\n",
      "('Epoch: 1335/2000', 'Iteration: 6680', 'Train loss: 0.012791', 'Train acc: 1.000000')\n",
      "('Epoch: 1335/2000', 'Iteration: 6680', 'Validation loss: 0.424214', 'Validation acc: 0.925000')\n",
      "('Epoch: 1336/2000', 'Iteration: 6685', 'Train loss: 0.026505', 'Train acc: 0.991667')\n",
      "('Epoch: 1337/2000', 'Iteration: 6690', 'Train loss: 0.021786', 'Train acc: 0.995000')\n",
      "('Epoch: 1337/2000', 'Iteration: 6690', 'Validation loss: 0.429150', 'Validation acc: 0.918333')\n",
      "('Epoch: 1338/2000', 'Iteration: 6695', 'Train loss: 0.019706', 'Train acc: 0.995000')\n",
      "('Epoch: 1339/2000', 'Iteration: 6700', 'Train loss: 0.013587', 'Train acc: 0.998333')\n",
      "('Epoch: 1339/2000', 'Iteration: 6700', 'Validation loss: 0.425110', 'Validation acc: 0.921667')\n",
      "('Epoch: 1340/2000', 'Iteration: 6705', 'Train loss: 0.014693', 'Train acc: 0.995000')\n",
      "('Epoch: 1341/2000', 'Iteration: 6710', 'Train loss: 0.017174', 'Train acc: 0.995000')\n",
      "('Epoch: 1341/2000', 'Iteration: 6710', 'Validation loss: 0.423871', 'Validation acc: 0.921667')\n",
      "('Epoch: 1342/2000', 'Iteration: 6715', 'Train loss: 0.018596', 'Train acc: 0.996667')\n",
      "('Epoch: 1343/2000', 'Iteration: 6720', 'Train loss: 0.017363', 'Train acc: 0.998333')\n",
      "('Epoch: 1343/2000', 'Iteration: 6720', 'Validation loss: 0.438070', 'Validation acc: 0.918333')\n",
      "('Epoch: 1344/2000', 'Iteration: 6725', 'Train loss: 0.015544', 'Train acc: 0.996667')\n",
      "('Epoch: 1345/2000', 'Iteration: 6730', 'Train loss: 0.014099', 'Train acc: 1.000000')\n",
      "('Epoch: 1345/2000', 'Iteration: 6730', 'Validation loss: 0.428219', 'Validation acc: 0.920000')\n",
      "('Epoch: 1346/2000', 'Iteration: 6735', 'Train loss: 0.017075', 'Train acc: 0.998333')\n",
      "('Epoch: 1347/2000', 'Iteration: 6740', 'Train loss: 0.029651', 'Train acc: 0.991667')\n",
      "('Epoch: 1347/2000', 'Iteration: 6740', 'Validation loss: 0.430234', 'Validation acc: 0.920000')\n",
      "('Epoch: 1348/2000', 'Iteration: 6745', 'Train loss: 0.014579', 'Train acc: 0.998333')\n",
      "('Epoch: 1349/2000', 'Iteration: 6750', 'Train loss: 0.020513', 'Train acc: 0.995000')\n",
      "('Epoch: 1349/2000', 'Iteration: 6750', 'Validation loss: 0.431605', 'Validation acc: 0.923333')\n",
      "('Epoch: 1350/2000', 'Iteration: 6755', 'Train loss: 0.015205', 'Train acc: 0.998333')\n",
      "('Epoch: 1351/2000', 'Iteration: 6760', 'Train loss: 0.036084', 'Train acc: 0.991667')\n",
      "('Epoch: 1351/2000', 'Iteration: 6760', 'Validation loss: 0.431724', 'Validation acc: 0.926667')\n",
      "('Epoch: 1352/2000', 'Iteration: 6765', 'Train loss: 0.020774', 'Train acc: 0.991667')\n",
      "('Epoch: 1353/2000', 'Iteration: 6770', 'Train loss: 0.019400', 'Train acc: 0.995000')\n",
      "('Epoch: 1353/2000', 'Iteration: 6770', 'Validation loss: 0.426095', 'Validation acc: 0.920000')\n",
      "('Epoch: 1354/2000', 'Iteration: 6775', 'Train loss: 0.017029', 'Train acc: 0.995000')\n",
      "('Epoch: 1355/2000', 'Iteration: 6780', 'Train loss: 0.016776', 'Train acc: 0.998333')\n",
      "('Epoch: 1355/2000', 'Iteration: 6780', 'Validation loss: 0.424261', 'Validation acc: 0.925000')\n",
      "('Epoch: 1356/2000', 'Iteration: 6785', 'Train loss: 0.015920', 'Train acc: 0.998333')\n",
      "('Epoch: 1357/2000', 'Iteration: 6790', 'Train loss: 0.023154', 'Train acc: 0.995000')\n",
      "('Epoch: 1357/2000', 'Iteration: 6790', 'Validation loss: 0.423896', 'Validation acc: 0.925000')\n",
      "('Epoch: 1358/2000', 'Iteration: 6795', 'Train loss: 0.022544', 'Train acc: 0.993333')\n",
      "('Epoch: 1359/2000', 'Iteration: 6800', 'Train loss: 0.019892', 'Train acc: 0.991667')\n",
      "('Epoch: 1359/2000', 'Iteration: 6800', 'Validation loss: 0.423893', 'Validation acc: 0.921667')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 1360/2000', 'Iteration: 6805', 'Train loss: 0.018071', 'Train acc: 0.998333')\n",
      "('Epoch: 1361/2000', 'Iteration: 6810', 'Train loss: 0.014884', 'Train acc: 0.998333')\n",
      "('Epoch: 1361/2000', 'Iteration: 6810', 'Validation loss: 0.434434', 'Validation acc: 0.920000')\n",
      "('Epoch: 1362/2000', 'Iteration: 6815', 'Train loss: 0.014224', 'Train acc: 1.000000')\n",
      "('Epoch: 1363/2000', 'Iteration: 6820', 'Train loss: 0.023908', 'Train acc: 0.991667')\n",
      "('Epoch: 1363/2000', 'Iteration: 6820', 'Validation loss: 0.427603', 'Validation acc: 0.925000')\n",
      "('Epoch: 1364/2000', 'Iteration: 6825', 'Train loss: 0.012986', 'Train acc: 1.000000')\n",
      "('Epoch: 1365/2000', 'Iteration: 6830', 'Train loss: 0.016240', 'Train acc: 0.998333')\n",
      "('Epoch: 1365/2000', 'Iteration: 6830', 'Validation loss: 0.429810', 'Validation acc: 0.923333')\n",
      "('Epoch: 1366/2000', 'Iteration: 6835', 'Train loss: 0.020926', 'Train acc: 0.993333')\n",
      "('Epoch: 1367/2000', 'Iteration: 6840', 'Train loss: 0.018040', 'Train acc: 0.993333')\n",
      "('Epoch: 1367/2000', 'Iteration: 6840', 'Validation loss: 0.436330', 'Validation acc: 0.923333')\n",
      "('Epoch: 1368/2000', 'Iteration: 6845', 'Train loss: 0.019435', 'Train acc: 0.993333')\n",
      "('Epoch: 1369/2000', 'Iteration: 6850', 'Train loss: 0.017050', 'Train acc: 0.991667')\n",
      "('Epoch: 1369/2000', 'Iteration: 6850', 'Validation loss: 0.432185', 'Validation acc: 0.925000')\n",
      "('Epoch: 1370/2000', 'Iteration: 6855', 'Train loss: 0.023711', 'Train acc: 0.995000')\n",
      "('Epoch: 1371/2000', 'Iteration: 6860', 'Train loss: 0.018457', 'Train acc: 0.995000')\n",
      "('Epoch: 1371/2000', 'Iteration: 6860', 'Validation loss: 0.427498', 'Validation acc: 0.928333')\n",
      "('Epoch: 1372/2000', 'Iteration: 6865', 'Train loss: 0.010583', 'Train acc: 1.000000')\n",
      "('Epoch: 1373/2000', 'Iteration: 6870', 'Train loss: 0.018340', 'Train acc: 0.993333')\n",
      "('Epoch: 1373/2000', 'Iteration: 6870', 'Validation loss: 0.421182', 'Validation acc: 0.925000')\n",
      "('Epoch: 1374/2000', 'Iteration: 6875', 'Train loss: 0.015440', 'Train acc: 1.000000')\n",
      "('Epoch: 1375/2000', 'Iteration: 6880', 'Train loss: 0.012758', 'Train acc: 0.996667')\n",
      "('Epoch: 1375/2000', 'Iteration: 6880', 'Validation loss: 0.422504', 'Validation acc: 0.926667')\n",
      "('Epoch: 1376/2000', 'Iteration: 6885', 'Train loss: 0.008675', 'Train acc: 1.000000')\n",
      "('Epoch: 1377/2000', 'Iteration: 6890', 'Train loss: 0.022595', 'Train acc: 0.991667')\n",
      "('Epoch: 1377/2000', 'Iteration: 6890', 'Validation loss: 0.426634', 'Validation acc: 0.923333')\n",
      "('Epoch: 1378/2000', 'Iteration: 6895', 'Train loss: 0.016883', 'Train acc: 0.996667')\n",
      "('Epoch: 1379/2000', 'Iteration: 6900', 'Train loss: 0.021816', 'Train acc: 0.993333')\n",
      "('Epoch: 1379/2000', 'Iteration: 6900', 'Validation loss: 0.423356', 'Validation acc: 0.921667')\n",
      "('Epoch: 1380/2000', 'Iteration: 6905', 'Train loss: 0.017431', 'Train acc: 0.995000')\n",
      "('Epoch: 1381/2000', 'Iteration: 6910', 'Train loss: 0.017128', 'Train acc: 0.995000')\n",
      "('Epoch: 1381/2000', 'Iteration: 6910', 'Validation loss: 0.429781', 'Validation acc: 0.921667')\n",
      "('Epoch: 1382/2000', 'Iteration: 6915', 'Train loss: 0.021165', 'Train acc: 0.996667')\n",
      "('Epoch: 1383/2000', 'Iteration: 6920', 'Train loss: 0.016117', 'Train acc: 0.998333')\n",
      "('Epoch: 1383/2000', 'Iteration: 6920', 'Validation loss: 0.428415', 'Validation acc: 0.925000')\n",
      "('Epoch: 1384/2000', 'Iteration: 6925', 'Train loss: 0.012975', 'Train acc: 0.996667')\n",
      "('Epoch: 1385/2000', 'Iteration: 6930', 'Train loss: 0.012273', 'Train acc: 0.998333')\n",
      "('Epoch: 1385/2000', 'Iteration: 6930', 'Validation loss: 0.428778', 'Validation acc: 0.921667')\n",
      "('Epoch: 1386/2000', 'Iteration: 6935', 'Train loss: 0.022401', 'Train acc: 0.993333')\n",
      "('Epoch: 1387/2000', 'Iteration: 6940', 'Train loss: 0.022587', 'Train acc: 0.993333')\n",
      "('Epoch: 1387/2000', 'Iteration: 6940', 'Validation loss: 0.426901', 'Validation acc: 0.916667')\n",
      "('Epoch: 1388/2000', 'Iteration: 6945', 'Train loss: 0.018478', 'Train acc: 0.996667')\n",
      "('Epoch: 1389/2000', 'Iteration: 6950', 'Train loss: 0.011850', 'Train acc: 1.000000')\n",
      "('Epoch: 1389/2000', 'Iteration: 6950', 'Validation loss: 0.425702', 'Validation acc: 0.925000')\n",
      "('Epoch: 1390/2000', 'Iteration: 6955', 'Train loss: 0.027914', 'Train acc: 0.996667')\n",
      "('Epoch: 1391/2000', 'Iteration: 6960', 'Train loss: 0.016651', 'Train acc: 0.996667')\n",
      "('Epoch: 1391/2000', 'Iteration: 6960', 'Validation loss: 0.434694', 'Validation acc: 0.925000')\n",
      "('Epoch: 1392/2000', 'Iteration: 6965', 'Train loss: 0.014069', 'Train acc: 0.998333')\n",
      "('Epoch: 1393/2000', 'Iteration: 6970', 'Train loss: 0.021775', 'Train acc: 0.993333')\n",
      "('Epoch: 1393/2000', 'Iteration: 6970', 'Validation loss: 0.428393', 'Validation acc: 0.923333')\n",
      "('Epoch: 1394/2000', 'Iteration: 6975', 'Train loss: 0.018667', 'Train acc: 0.996667')\n",
      "('Epoch: 1395/2000', 'Iteration: 6980', 'Train loss: 0.011698', 'Train acc: 0.998333')\n",
      "('Epoch: 1395/2000', 'Iteration: 6980', 'Validation loss: 0.439083', 'Validation acc: 0.925000')\n",
      "('Epoch: 1396/2000', 'Iteration: 6985', 'Train loss: 0.018206', 'Train acc: 0.998333')\n",
      "('Epoch: 1397/2000', 'Iteration: 6990', 'Train loss: 0.012961', 'Train acc: 1.000000')\n",
      "('Epoch: 1397/2000', 'Iteration: 6990', 'Validation loss: 0.428648', 'Validation acc: 0.930000')\n",
      "('Epoch: 1398/2000', 'Iteration: 6995', 'Train loss: 0.017184', 'Train acc: 0.995000')\n",
      "('Epoch: 1399/2000', 'Iteration: 7000', 'Train loss: 0.014007', 'Train acc: 0.996667')\n",
      "('Epoch: 1399/2000', 'Iteration: 7000', 'Validation loss: 0.419639', 'Validation acc: 0.928333')\n",
      "('Epoch: 1400/2000', 'Iteration: 7005', 'Train loss: 0.017294', 'Train acc: 0.993333')\n",
      "('Epoch: 1401/2000', 'Iteration: 7010', 'Train loss: 0.016035', 'Train acc: 0.996667')\n",
      "('Epoch: 1401/2000', 'Iteration: 7010', 'Validation loss: 0.432543', 'Validation acc: 0.920000')\n",
      "('Epoch: 1402/2000', 'Iteration: 7015', 'Train loss: 0.016430', 'Train acc: 0.998333')\n",
      "('Epoch: 1403/2000', 'Iteration: 7020', 'Train loss: 0.014149', 'Train acc: 0.998333')\n",
      "('Epoch: 1403/2000', 'Iteration: 7020', 'Validation loss: 0.429029', 'Validation acc: 0.926667')\n",
      "('Epoch: 1404/2000', 'Iteration: 7025', 'Train loss: 0.019218', 'Train acc: 0.995000')\n",
      "('Epoch: 1405/2000', 'Iteration: 7030', 'Train loss: 0.021245', 'Train acc: 0.993333')\n",
      "('Epoch: 1405/2000', 'Iteration: 7030', 'Validation loss: 0.429424', 'Validation acc: 0.925000')\n",
      "('Epoch: 1406/2000', 'Iteration: 7035', 'Train loss: 0.012936', 'Train acc: 0.995000')\n",
      "('Epoch: 1407/2000', 'Iteration: 7040', 'Train loss: 0.021809', 'Train acc: 0.993333')\n",
      "('Epoch: 1407/2000', 'Iteration: 7040', 'Validation loss: 0.424038', 'Validation acc: 0.920000')\n",
      "('Epoch: 1408/2000', 'Iteration: 7045', 'Train loss: 0.019704', 'Train acc: 0.996667')\n",
      "('Epoch: 1409/2000', 'Iteration: 7050', 'Train loss: 0.016866', 'Train acc: 0.996667')\n",
      "('Epoch: 1409/2000', 'Iteration: 7050', 'Validation loss: 0.423835', 'Validation acc: 0.923333')\n",
      "('Epoch: 1410/2000', 'Iteration: 7055', 'Train loss: 0.011062', 'Train acc: 1.000000')\n",
      "('Epoch: 1411/2000', 'Iteration: 7060', 'Train loss: 0.014123', 'Train acc: 0.998333')\n",
      "('Epoch: 1411/2000', 'Iteration: 7060', 'Validation loss: 0.423383', 'Validation acc: 0.926667')\n",
      "('Epoch: 1412/2000', 'Iteration: 7065', 'Train loss: 0.019082', 'Train acc: 0.996667')\n",
      "('Epoch: 1413/2000', 'Iteration: 7070', 'Train loss: 0.025591', 'Train acc: 0.991667')\n",
      "('Epoch: 1413/2000', 'Iteration: 7070', 'Validation loss: 0.429984', 'Validation acc: 0.925000')\n",
      "('Epoch: 1414/2000', 'Iteration: 7075', 'Train loss: 0.016206', 'Train acc: 0.996667')\n",
      "('Epoch: 1415/2000', 'Iteration: 7080', 'Train loss: 0.015949', 'Train acc: 0.998333')\n",
      "('Epoch: 1415/2000', 'Iteration: 7080', 'Validation loss: 0.433895', 'Validation acc: 0.923333')\n",
      "('Epoch: 1416/2000', 'Iteration: 7085', 'Train loss: 0.012121', 'Train acc: 0.998333')\n",
      "('Epoch: 1417/2000', 'Iteration: 7090', 'Train loss: 0.026766', 'Train acc: 0.991667')\n",
      "('Epoch: 1417/2000', 'Iteration: 7090', 'Validation loss: 0.430679', 'Validation acc: 0.921667')\n",
      "('Epoch: 1418/2000', 'Iteration: 7095', 'Train loss: 0.017134', 'Train acc: 0.995000')\n",
      "('Epoch: 1419/2000', 'Iteration: 7100', 'Train loss: 0.019204', 'Train acc: 0.996667')\n",
      "('Epoch: 1419/2000', 'Iteration: 7100', 'Validation loss: 0.433113', 'Validation acc: 0.923333')\n",
      "('Epoch: 1420/2000', 'Iteration: 7105', 'Train loss: 0.016567', 'Train acc: 0.996667')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 1421/2000', 'Iteration: 7110', 'Train loss: 0.020627', 'Train acc: 0.995000')\n",
      "('Epoch: 1421/2000', 'Iteration: 7110', 'Validation loss: 0.427820', 'Validation acc: 0.928333')\n",
      "('Epoch: 1422/2000', 'Iteration: 7115', 'Train loss: 0.017034', 'Train acc: 0.995000')\n",
      "('Epoch: 1423/2000', 'Iteration: 7120', 'Train loss: 0.009577', 'Train acc: 1.000000')\n",
      "('Epoch: 1423/2000', 'Iteration: 7120', 'Validation loss: 0.426376', 'Validation acc: 0.923333')\n",
      "('Epoch: 1424/2000', 'Iteration: 7125', 'Train loss: 0.021012', 'Train acc: 0.996667')\n",
      "('Epoch: 1425/2000', 'Iteration: 7130', 'Train loss: 0.008864', 'Train acc: 1.000000')\n",
      "('Epoch: 1425/2000', 'Iteration: 7130', 'Validation loss: 0.432451', 'Validation acc: 0.921667')\n",
      "('Epoch: 1426/2000', 'Iteration: 7135', 'Train loss: 0.012171', 'Train acc: 0.998333')\n",
      "('Epoch: 1427/2000', 'Iteration: 7140', 'Train loss: 0.011562', 'Train acc: 0.996667')\n",
      "('Epoch: 1427/2000', 'Iteration: 7140', 'Validation loss: 0.431209', 'Validation acc: 0.920000')\n",
      "('Epoch: 1428/2000', 'Iteration: 7145', 'Train loss: 0.018187', 'Train acc: 0.995000')\n",
      "('Epoch: 1429/2000', 'Iteration: 7150', 'Train loss: 0.012455', 'Train acc: 0.998333')\n",
      "('Epoch: 1429/2000', 'Iteration: 7150', 'Validation loss: 0.426314', 'Validation acc: 0.925000')\n",
      "('Epoch: 1430/2000', 'Iteration: 7155', 'Train loss: 0.014951', 'Train acc: 0.996667')\n",
      "('Epoch: 1431/2000', 'Iteration: 7160', 'Train loss: 0.018818', 'Train acc: 0.993333')\n",
      "('Epoch: 1431/2000', 'Iteration: 7160', 'Validation loss: 0.432414', 'Validation acc: 0.926667')\n",
      "('Epoch: 1432/2000', 'Iteration: 7165', 'Train loss: 0.019072', 'Train acc: 0.996667')\n",
      "('Epoch: 1433/2000', 'Iteration: 7170', 'Train loss: 0.016433', 'Train acc: 0.996667')\n",
      "('Epoch: 1433/2000', 'Iteration: 7170', 'Validation loss: 0.426684', 'Validation acc: 0.925000')\n",
      "('Epoch: 1434/2000', 'Iteration: 7175', 'Train loss: 0.015338', 'Train acc: 0.996667')\n",
      "('Epoch: 1435/2000', 'Iteration: 7180', 'Train loss: 0.015197', 'Train acc: 0.998333')\n",
      "('Epoch: 1435/2000', 'Iteration: 7180', 'Validation loss: 0.438224', 'Validation acc: 0.920000')\n",
      "('Epoch: 1436/2000', 'Iteration: 7185', 'Train loss: 0.013303', 'Train acc: 0.996667')\n",
      "('Epoch: 1437/2000', 'Iteration: 7190', 'Train loss: 0.019848', 'Train acc: 0.995000')\n",
      "('Epoch: 1437/2000', 'Iteration: 7190', 'Validation loss: 0.434410', 'Validation acc: 0.923333')\n",
      "('Epoch: 1438/2000', 'Iteration: 7195', 'Train loss: 0.016158', 'Train acc: 0.996667')\n",
      "('Epoch: 1439/2000', 'Iteration: 7200', 'Train loss: 0.020865', 'Train acc: 0.996667')\n",
      "('Epoch: 1439/2000', 'Iteration: 7200', 'Validation loss: 0.430873', 'Validation acc: 0.926667')\n",
      "('Epoch: 1440/2000', 'Iteration: 7205', 'Train loss: 0.013916', 'Train acc: 0.996667')\n",
      "('Epoch: 1441/2000', 'Iteration: 7210', 'Train loss: 0.021081', 'Train acc: 0.993333')\n",
      "('Epoch: 1441/2000', 'Iteration: 7210', 'Validation loss: 0.431180', 'Validation acc: 0.928333')\n",
      "('Epoch: 1442/2000', 'Iteration: 7215', 'Train loss: 0.015299', 'Train acc: 1.000000')\n",
      "('Epoch: 1443/2000', 'Iteration: 7220', 'Train loss: 0.012933', 'Train acc: 0.998333')\n",
      "('Epoch: 1443/2000', 'Iteration: 7220', 'Validation loss: 0.433646', 'Validation acc: 0.923333')\n",
      "('Epoch: 1444/2000', 'Iteration: 7225', 'Train loss: 0.019427', 'Train acc: 0.995000')\n",
      "('Epoch: 1445/2000', 'Iteration: 7230', 'Train loss: 0.013511', 'Train acc: 0.998333')\n",
      "('Epoch: 1445/2000', 'Iteration: 7230', 'Validation loss: 0.435570', 'Validation acc: 0.925000')\n",
      "('Epoch: 1446/2000', 'Iteration: 7235', 'Train loss: 0.018770', 'Train acc: 0.993333')\n",
      "('Epoch: 1447/2000', 'Iteration: 7240', 'Train loss: 0.017296', 'Train acc: 0.993333')\n",
      "('Epoch: 1447/2000', 'Iteration: 7240', 'Validation loss: 0.431984', 'Validation acc: 0.925000')\n",
      "('Epoch: 1448/2000', 'Iteration: 7245', 'Train loss: 0.011560', 'Train acc: 0.998333')\n",
      "('Epoch: 1449/2000', 'Iteration: 7250', 'Train loss: 0.017032', 'Train acc: 0.996667')\n",
      "('Epoch: 1449/2000', 'Iteration: 7250', 'Validation loss: 0.431689', 'Validation acc: 0.923333')\n",
      "('Epoch: 1450/2000', 'Iteration: 7255', 'Train loss: 0.011702', 'Train acc: 0.996667')\n",
      "('Epoch: 1451/2000', 'Iteration: 7260', 'Train loss: 0.013474', 'Train acc: 0.998333')\n",
      "('Epoch: 1451/2000', 'Iteration: 7260', 'Validation loss: 0.432010', 'Validation acc: 0.923333')\n",
      "('Epoch: 1452/2000', 'Iteration: 7265', 'Train loss: 0.014024', 'Train acc: 0.998333')\n",
      "('Epoch: 1453/2000', 'Iteration: 7270', 'Train loss: 0.015480', 'Train acc: 0.998333')\n",
      "('Epoch: 1453/2000', 'Iteration: 7270', 'Validation loss: 0.432603', 'Validation acc: 0.923333')\n",
      "('Epoch: 1454/2000', 'Iteration: 7275', 'Train loss: 0.022232', 'Train acc: 0.991667')\n",
      "('Epoch: 1455/2000', 'Iteration: 7280', 'Train loss: 0.010149', 'Train acc: 1.000000')\n",
      "('Epoch: 1455/2000', 'Iteration: 7280', 'Validation loss: 0.433564', 'Validation acc: 0.925000')\n",
      "('Epoch: 1456/2000', 'Iteration: 7285', 'Train loss: 0.013882', 'Train acc: 0.998333')\n",
      "('Epoch: 1457/2000', 'Iteration: 7290', 'Train loss: 0.015649', 'Train acc: 0.996667')\n",
      "('Epoch: 1457/2000', 'Iteration: 7290', 'Validation loss: 0.438880', 'Validation acc: 0.925000')\n",
      "('Epoch: 1458/2000', 'Iteration: 7295', 'Train loss: 0.013694', 'Train acc: 0.998333')\n",
      "('Epoch: 1459/2000', 'Iteration: 7300', 'Train loss: 0.021944', 'Train acc: 0.993333')\n",
      "('Epoch: 1459/2000', 'Iteration: 7300', 'Validation loss: 0.444726', 'Validation acc: 0.926667')\n",
      "('Epoch: 1460/2000', 'Iteration: 7305', 'Train loss: 0.031469', 'Train acc: 0.990000')\n",
      "('Epoch: 1461/2000', 'Iteration: 7310', 'Train loss: 0.013188', 'Train acc: 0.998333')\n",
      "('Epoch: 1461/2000', 'Iteration: 7310', 'Validation loss: 0.434343', 'Validation acc: 0.925000')\n",
      "('Epoch: 1462/2000', 'Iteration: 7315', 'Train loss: 0.020010', 'Train acc: 0.993333')\n",
      "('Epoch: 1463/2000', 'Iteration: 7320', 'Train loss: 0.015787', 'Train acc: 0.996667')\n",
      "('Epoch: 1463/2000', 'Iteration: 7320', 'Validation loss: 0.441870', 'Validation acc: 0.928333')\n",
      "('Epoch: 1464/2000', 'Iteration: 7325', 'Train loss: 0.010456', 'Train acc: 0.996667')\n",
      "('Epoch: 1465/2000', 'Iteration: 7330', 'Train loss: 0.011278', 'Train acc: 0.998333')\n",
      "('Epoch: 1465/2000', 'Iteration: 7330', 'Validation loss: 0.439494', 'Validation acc: 0.925000')\n",
      "('Epoch: 1466/2000', 'Iteration: 7335', 'Train loss: 0.011897', 'Train acc: 0.996667')\n",
      "('Epoch: 1467/2000', 'Iteration: 7340', 'Train loss: 0.012943', 'Train acc: 0.998333')\n",
      "('Epoch: 1467/2000', 'Iteration: 7340', 'Validation loss: 0.428391', 'Validation acc: 0.925000')\n",
      "('Epoch: 1468/2000', 'Iteration: 7345', 'Train loss: 0.018667', 'Train acc: 0.996667')\n",
      "('Epoch: 1469/2000', 'Iteration: 7350', 'Train loss: 0.021117', 'Train acc: 0.995000')\n",
      "('Epoch: 1469/2000', 'Iteration: 7350', 'Validation loss: 0.442017', 'Validation acc: 0.923333')\n",
      "('Epoch: 1470/2000', 'Iteration: 7355', 'Train loss: 0.011494', 'Train acc: 0.998333')\n",
      "('Epoch: 1471/2000', 'Iteration: 7360', 'Train loss: 0.013136', 'Train acc: 0.998333')\n",
      "('Epoch: 1471/2000', 'Iteration: 7360', 'Validation loss: 0.438173', 'Validation acc: 0.921667')\n",
      "('Epoch: 1472/2000', 'Iteration: 7365', 'Train loss: 0.017367', 'Train acc: 0.996667')\n",
      "('Epoch: 1473/2000', 'Iteration: 7370', 'Train loss: 0.016350', 'Train acc: 0.996667')\n",
      "('Epoch: 1473/2000', 'Iteration: 7370', 'Validation loss: 0.439883', 'Validation acc: 0.920000')\n",
      "('Epoch: 1474/2000', 'Iteration: 7375', 'Train loss: 0.014242', 'Train acc: 0.998333')\n",
      "('Epoch: 1475/2000', 'Iteration: 7380', 'Train loss: 0.013146', 'Train acc: 0.996667')\n",
      "('Epoch: 1475/2000', 'Iteration: 7380', 'Validation loss: 0.441052', 'Validation acc: 0.920000')\n",
      "('Epoch: 1476/2000', 'Iteration: 7385', 'Train loss: 0.019173', 'Train acc: 0.993333')\n",
      "('Epoch: 1477/2000', 'Iteration: 7390', 'Train loss: 0.012315', 'Train acc: 1.000000')\n",
      "('Epoch: 1477/2000', 'Iteration: 7390', 'Validation loss: 0.435491', 'Validation acc: 0.921667')\n",
      "('Epoch: 1478/2000', 'Iteration: 7395', 'Train loss: 0.017909', 'Train acc: 0.993333')\n",
      "('Epoch: 1479/2000', 'Iteration: 7400', 'Train loss: 0.014171', 'Train acc: 0.998333')\n",
      "('Epoch: 1479/2000', 'Iteration: 7400', 'Validation loss: 0.442682', 'Validation acc: 0.923333')\n",
      "('Epoch: 1480/2000', 'Iteration: 7405', 'Train loss: 0.009718', 'Train acc: 0.998333')\n",
      "('Epoch: 1481/2000', 'Iteration: 7410', 'Train loss: 0.015475', 'Train acc: 0.996667')\n",
      "('Epoch: 1481/2000', 'Iteration: 7410', 'Validation loss: 0.438171', 'Validation acc: 0.920000')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 1482/2000', 'Iteration: 7415', 'Train loss: 0.013653', 'Train acc: 0.998333')\n",
      "('Epoch: 1483/2000', 'Iteration: 7420', 'Train loss: 0.016502', 'Train acc: 0.996667')\n",
      "('Epoch: 1483/2000', 'Iteration: 7420', 'Validation loss: 0.440678', 'Validation acc: 0.920000')\n",
      "('Epoch: 1484/2000', 'Iteration: 7425', 'Train loss: 0.007843', 'Train acc: 1.000000')\n",
      "('Epoch: 1485/2000', 'Iteration: 7430', 'Train loss: 0.020111', 'Train acc: 0.995000')\n",
      "('Epoch: 1485/2000', 'Iteration: 7430', 'Validation loss: 0.435465', 'Validation acc: 0.923333')\n",
      "('Epoch: 1486/2000', 'Iteration: 7435', 'Train loss: 0.017163', 'Train acc: 0.993333')\n",
      "('Epoch: 1487/2000', 'Iteration: 7440', 'Train loss: 0.014419', 'Train acc: 0.998333')\n",
      "('Epoch: 1487/2000', 'Iteration: 7440', 'Validation loss: 0.428718', 'Validation acc: 0.925000')\n",
      "('Epoch: 1488/2000', 'Iteration: 7445', 'Train loss: 0.010995', 'Train acc: 1.000000')\n",
      "('Epoch: 1489/2000', 'Iteration: 7450', 'Train loss: 0.010853', 'Train acc: 1.000000')\n",
      "('Epoch: 1489/2000', 'Iteration: 7450', 'Validation loss: 0.440418', 'Validation acc: 0.921667')\n",
      "('Epoch: 1490/2000', 'Iteration: 7455', 'Train loss: 0.014169', 'Train acc: 0.996667')\n",
      "('Epoch: 1491/2000', 'Iteration: 7460', 'Train loss: 0.012275', 'Train acc: 0.998333')\n",
      "('Epoch: 1491/2000', 'Iteration: 7460', 'Validation loss: 0.433984', 'Validation acc: 0.925000')\n",
      "('Epoch: 1492/2000', 'Iteration: 7465', 'Train loss: 0.012622', 'Train acc: 0.998333')\n",
      "('Epoch: 1493/2000', 'Iteration: 7470', 'Train loss: 0.011623', 'Train acc: 0.998333')\n",
      "('Epoch: 1493/2000', 'Iteration: 7470', 'Validation loss: 0.432051', 'Validation acc: 0.923333')\n",
      "('Epoch: 1494/2000', 'Iteration: 7475', 'Train loss: 0.013146', 'Train acc: 0.998333')\n",
      "('Epoch: 1495/2000', 'Iteration: 7480', 'Train loss: 0.014030', 'Train acc: 0.995000')\n",
      "('Epoch: 1495/2000', 'Iteration: 7480', 'Validation loss: 0.439242', 'Validation acc: 0.926667')\n",
      "('Epoch: 1496/2000', 'Iteration: 7485', 'Train loss: 0.008773', 'Train acc: 1.000000')\n",
      "('Epoch: 1497/2000', 'Iteration: 7490', 'Train loss: 0.009277', 'Train acc: 0.998333')\n",
      "('Epoch: 1497/2000', 'Iteration: 7490', 'Validation loss: 0.441322', 'Validation acc: 0.925000')\n",
      "('Epoch: 1498/2000', 'Iteration: 7495', 'Train loss: 0.011551', 'Train acc: 0.998333')\n",
      "('Epoch: 1499/2000', 'Iteration: 7500', 'Train loss: 0.013105', 'Train acc: 0.996667')\n",
      "('Epoch: 1499/2000', 'Iteration: 7500', 'Validation loss: 0.449392', 'Validation acc: 0.928333')\n",
      "('Epoch: 1500/2000', 'Iteration: 7505', 'Train loss: 0.011354', 'Train acc: 0.996667')\n",
      "('Epoch: 1501/2000', 'Iteration: 7510', 'Train loss: 0.013777', 'Train acc: 0.998333')\n",
      "('Epoch: 1501/2000', 'Iteration: 7510', 'Validation loss: 0.437423', 'Validation acc: 0.928333')\n",
      "('Epoch: 1502/2000', 'Iteration: 7515', 'Train loss: 0.011988', 'Train acc: 0.996667')\n",
      "('Epoch: 1503/2000', 'Iteration: 7520', 'Train loss: 0.010597', 'Train acc: 0.998333')\n",
      "('Epoch: 1503/2000', 'Iteration: 7520', 'Validation loss: 0.439957', 'Validation acc: 0.928333')\n",
      "('Epoch: 1504/2000', 'Iteration: 7525', 'Train loss: 0.013944', 'Train acc: 0.996667')\n",
      "('Epoch: 1505/2000', 'Iteration: 7530', 'Train loss: 0.016425', 'Train acc: 0.998333')\n",
      "('Epoch: 1505/2000', 'Iteration: 7530', 'Validation loss: 0.435946', 'Validation acc: 0.925000')\n",
      "('Epoch: 1506/2000', 'Iteration: 7535', 'Train loss: 0.013470', 'Train acc: 0.996667')\n",
      "('Epoch: 1507/2000', 'Iteration: 7540', 'Train loss: 0.017722', 'Train acc: 0.996667')\n",
      "('Epoch: 1507/2000', 'Iteration: 7540', 'Validation loss: 0.436802', 'Validation acc: 0.923333')\n",
      "('Epoch: 1508/2000', 'Iteration: 7545', 'Train loss: 0.020839', 'Train acc: 0.998333')\n",
      "('Epoch: 1509/2000', 'Iteration: 7550', 'Train loss: 0.011289', 'Train acc: 1.000000')\n",
      "('Epoch: 1509/2000', 'Iteration: 7550', 'Validation loss: 0.442944', 'Validation acc: 0.923333')\n",
      "('Epoch: 1510/2000', 'Iteration: 7555', 'Train loss: 0.015624', 'Train acc: 0.996667')\n",
      "('Epoch: 1511/2000', 'Iteration: 7560', 'Train loss: 0.011570', 'Train acc: 0.998333')\n",
      "('Epoch: 1511/2000', 'Iteration: 7560', 'Validation loss: 0.438752', 'Validation acc: 0.926667')\n",
      "('Epoch: 1512/2000', 'Iteration: 7565', 'Train loss: 0.010978', 'Train acc: 0.998333')\n",
      "('Epoch: 1513/2000', 'Iteration: 7570', 'Train loss: 0.021352', 'Train acc: 0.993333')\n",
      "('Epoch: 1513/2000', 'Iteration: 7570', 'Validation loss: 0.438433', 'Validation acc: 0.928333')\n",
      "('Epoch: 1514/2000', 'Iteration: 7575', 'Train loss: 0.017033', 'Train acc: 0.996667')\n",
      "('Epoch: 1515/2000', 'Iteration: 7580', 'Train loss: 0.012866', 'Train acc: 0.996667')\n",
      "('Epoch: 1515/2000', 'Iteration: 7580', 'Validation loss: 0.434425', 'Validation acc: 0.928333')\n",
      "('Epoch: 1516/2000', 'Iteration: 7585', 'Train loss: 0.013829', 'Train acc: 0.996667')\n",
      "('Epoch: 1517/2000', 'Iteration: 7590', 'Train loss: 0.012150', 'Train acc: 1.000000')\n",
      "('Epoch: 1517/2000', 'Iteration: 7590', 'Validation loss: 0.442301', 'Validation acc: 0.928333')\n",
      "('Epoch: 1518/2000', 'Iteration: 7595', 'Train loss: 0.016149', 'Train acc: 0.998333')\n",
      "('Epoch: 1519/2000', 'Iteration: 7600', 'Train loss: 0.014028', 'Train acc: 0.998333')\n",
      "('Epoch: 1519/2000', 'Iteration: 7600', 'Validation loss: 0.430063', 'Validation acc: 0.925000')\n",
      "('Epoch: 1520/2000', 'Iteration: 7605', 'Train loss: 0.012882', 'Train acc: 0.998333')\n",
      "('Epoch: 1521/2000', 'Iteration: 7610', 'Train loss: 0.008407', 'Train acc: 1.000000')\n",
      "('Epoch: 1521/2000', 'Iteration: 7610', 'Validation loss: 0.434953', 'Validation acc: 0.925000')\n",
      "('Epoch: 1522/2000', 'Iteration: 7615', 'Train loss: 0.013433', 'Train acc: 0.996667')\n",
      "('Epoch: 1523/2000', 'Iteration: 7620', 'Train loss: 0.009229', 'Train acc: 1.000000')\n",
      "('Epoch: 1523/2000', 'Iteration: 7620', 'Validation loss: 0.442527', 'Validation acc: 0.928333')\n",
      "('Epoch: 1524/2000', 'Iteration: 7625', 'Train loss: 0.013970', 'Train acc: 0.995000')\n",
      "('Epoch: 1525/2000', 'Iteration: 7630', 'Train loss: 0.024250', 'Train acc: 0.993333')\n",
      "('Epoch: 1525/2000', 'Iteration: 7630', 'Validation loss: 0.439057', 'Validation acc: 0.925000')\n",
      "('Epoch: 1526/2000', 'Iteration: 7635', 'Train loss: 0.018482', 'Train acc: 0.996667')\n",
      "('Epoch: 1527/2000', 'Iteration: 7640', 'Train loss: 0.016256', 'Train acc: 0.995000')\n",
      "('Epoch: 1527/2000', 'Iteration: 7640', 'Validation loss: 0.438339', 'Validation acc: 0.921667')\n",
      "('Epoch: 1528/2000', 'Iteration: 7645', 'Train loss: 0.014258', 'Train acc: 0.996667')\n",
      "('Epoch: 1529/2000', 'Iteration: 7650', 'Train loss: 0.021488', 'Train acc: 0.993333')\n",
      "('Epoch: 1529/2000', 'Iteration: 7650', 'Validation loss: 0.439604', 'Validation acc: 0.923333')\n",
      "('Epoch: 1530/2000', 'Iteration: 7655', 'Train loss: 0.013200', 'Train acc: 1.000000')\n",
      "('Epoch: 1531/2000', 'Iteration: 7660', 'Train loss: 0.014813', 'Train acc: 0.995000')\n",
      "('Epoch: 1531/2000', 'Iteration: 7660', 'Validation loss: 0.440242', 'Validation acc: 0.926667')\n",
      "('Epoch: 1532/2000', 'Iteration: 7665', 'Train loss: 0.010631', 'Train acc: 1.000000')\n",
      "('Epoch: 1533/2000', 'Iteration: 7670', 'Train loss: 0.017253', 'Train acc: 0.996667')\n",
      "('Epoch: 1533/2000', 'Iteration: 7670', 'Validation loss: 0.444921', 'Validation acc: 0.926667')\n",
      "('Epoch: 1534/2000', 'Iteration: 7675', 'Train loss: 0.017469', 'Train acc: 0.995000')\n",
      "('Epoch: 1535/2000', 'Iteration: 7680', 'Train loss: 0.017276', 'Train acc: 0.993333')\n",
      "('Epoch: 1535/2000', 'Iteration: 7680', 'Validation loss: 0.439005', 'Validation acc: 0.926667')\n",
      "('Epoch: 1536/2000', 'Iteration: 7685', 'Train loss: 0.011141', 'Train acc: 0.998333')\n",
      "('Epoch: 1537/2000', 'Iteration: 7690', 'Train loss: 0.012001', 'Train acc: 0.998333')\n",
      "('Epoch: 1537/2000', 'Iteration: 7690', 'Validation loss: 0.442868', 'Validation acc: 0.926667')\n",
      "('Epoch: 1538/2000', 'Iteration: 7695', 'Train loss: 0.024686', 'Train acc: 0.990000')\n",
      "('Epoch: 1539/2000', 'Iteration: 7700', 'Train loss: 0.012322', 'Train acc: 0.998333')\n",
      "('Epoch: 1539/2000', 'Iteration: 7700', 'Validation loss: 0.443561', 'Validation acc: 0.921667')\n",
      "('Epoch: 1540/2000', 'Iteration: 7705', 'Train loss: 0.013941', 'Train acc: 0.998333')\n",
      "('Epoch: 1541/2000', 'Iteration: 7710', 'Train loss: 0.011934', 'Train acc: 0.995000')\n",
      "('Epoch: 1541/2000', 'Iteration: 7710', 'Validation loss: 0.449587', 'Validation acc: 0.926667')\n",
      "('Epoch: 1542/2000', 'Iteration: 7715', 'Train loss: 0.012598', 'Train acc: 0.998333')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 1543/2000', 'Iteration: 7720', 'Train loss: 0.013810', 'Train acc: 0.996667')\n",
      "('Epoch: 1543/2000', 'Iteration: 7720', 'Validation loss: 0.439201', 'Validation acc: 0.926667')\n",
      "('Epoch: 1544/2000', 'Iteration: 7725', 'Train loss: 0.013029', 'Train acc: 0.996667')\n",
      "('Epoch: 1545/2000', 'Iteration: 7730', 'Train loss: 0.018270', 'Train acc: 0.993333')\n",
      "('Epoch: 1545/2000', 'Iteration: 7730', 'Validation loss: 0.444325', 'Validation acc: 0.923333')\n",
      "('Epoch: 1546/2000', 'Iteration: 7735', 'Train loss: 0.007791', 'Train acc: 1.000000')\n",
      "('Epoch: 1547/2000', 'Iteration: 7740', 'Train loss: 0.015619', 'Train acc: 0.996667')\n",
      "('Epoch: 1547/2000', 'Iteration: 7740', 'Validation loss: 0.446481', 'Validation acc: 0.928333')\n",
      "('Epoch: 1548/2000', 'Iteration: 7745', 'Train loss: 0.008487', 'Train acc: 1.000000')\n",
      "('Epoch: 1549/2000', 'Iteration: 7750', 'Train loss: 0.015676', 'Train acc: 0.993333')\n",
      "('Epoch: 1549/2000', 'Iteration: 7750', 'Validation loss: 0.450659', 'Validation acc: 0.926667')\n",
      "('Epoch: 1550/2000', 'Iteration: 7755', 'Train loss: 0.012841', 'Train acc: 0.998333')\n",
      "('Epoch: 1551/2000', 'Iteration: 7760', 'Train loss: 0.013667', 'Train acc: 0.995000')\n",
      "('Epoch: 1551/2000', 'Iteration: 7760', 'Validation loss: 0.445433', 'Validation acc: 0.928333')\n",
      "('Epoch: 1552/2000', 'Iteration: 7765', 'Train loss: 0.010102', 'Train acc: 0.996667')\n",
      "('Epoch: 1553/2000', 'Iteration: 7770', 'Train loss: 0.012999', 'Train acc: 0.996667')\n",
      "('Epoch: 1553/2000', 'Iteration: 7770', 'Validation loss: 0.441879', 'Validation acc: 0.920000')\n",
      "('Epoch: 1554/2000', 'Iteration: 7775', 'Train loss: 0.015022', 'Train acc: 0.998333')\n",
      "('Epoch: 1555/2000', 'Iteration: 7780', 'Train loss: 0.014413', 'Train acc: 0.996667')\n",
      "('Epoch: 1555/2000', 'Iteration: 7780', 'Validation loss: 0.439061', 'Validation acc: 0.923333')\n",
      "('Epoch: 1556/2000', 'Iteration: 7785', 'Train loss: 0.011297', 'Train acc: 0.998333')\n",
      "('Epoch: 1557/2000', 'Iteration: 7790', 'Train loss: 0.012309', 'Train acc: 1.000000')\n",
      "('Epoch: 1557/2000', 'Iteration: 7790', 'Validation loss: 0.453424', 'Validation acc: 0.923333')\n",
      "('Epoch: 1558/2000', 'Iteration: 7795', 'Train loss: 0.010900', 'Train acc: 0.996667')\n",
      "('Epoch: 1559/2000', 'Iteration: 7800', 'Train loss: 0.013055', 'Train acc: 0.998333')\n",
      "('Epoch: 1559/2000', 'Iteration: 7800', 'Validation loss: 0.438871', 'Validation acc: 0.928333')\n",
      "('Epoch: 1560/2000', 'Iteration: 7805', 'Train loss: 0.013353', 'Train acc: 0.995000')\n",
      "('Epoch: 1561/2000', 'Iteration: 7810', 'Train loss: 0.013465', 'Train acc: 0.998333')\n",
      "('Epoch: 1561/2000', 'Iteration: 7810', 'Validation loss: 0.439472', 'Validation acc: 0.925000')\n",
      "('Epoch: 1562/2000', 'Iteration: 7815', 'Train loss: 0.011364', 'Train acc: 1.000000')\n",
      "('Epoch: 1563/2000', 'Iteration: 7820', 'Train loss: 0.013337', 'Train acc: 0.998333')\n",
      "('Epoch: 1563/2000', 'Iteration: 7820', 'Validation loss: 0.446075', 'Validation acc: 0.930000')\n",
      "('Epoch: 1564/2000', 'Iteration: 7825', 'Train loss: 0.012755', 'Train acc: 0.996667')\n",
      "('Epoch: 1565/2000', 'Iteration: 7830', 'Train loss: 0.012050', 'Train acc: 0.998333')\n",
      "('Epoch: 1565/2000', 'Iteration: 7830', 'Validation loss: 0.442653', 'Validation acc: 0.926667')\n",
      "('Epoch: 1566/2000', 'Iteration: 7835', 'Train loss: 0.008757', 'Train acc: 1.000000')\n",
      "('Epoch: 1567/2000', 'Iteration: 7840', 'Train loss: 0.010233', 'Train acc: 1.000000')\n",
      "('Epoch: 1567/2000', 'Iteration: 7840', 'Validation loss: 0.441652', 'Validation acc: 0.921667')\n",
      "('Epoch: 1568/2000', 'Iteration: 7845', 'Train loss: 0.010892', 'Train acc: 0.998333')\n",
      "('Epoch: 1569/2000', 'Iteration: 7850', 'Train loss: 0.015277', 'Train acc: 0.996667')\n",
      "('Epoch: 1569/2000', 'Iteration: 7850', 'Validation loss: 0.438860', 'Validation acc: 0.928333')\n",
      "('Epoch: 1570/2000', 'Iteration: 7855', 'Train loss: 0.015144', 'Train acc: 0.993333')\n",
      "('Epoch: 1571/2000', 'Iteration: 7860', 'Train loss: 0.009404', 'Train acc: 0.998333')\n",
      "('Epoch: 1571/2000', 'Iteration: 7860', 'Validation loss: 0.439209', 'Validation acc: 0.926667')\n",
      "('Epoch: 1572/2000', 'Iteration: 7865', 'Train loss: 0.007380', 'Train acc: 1.000000')\n",
      "('Epoch: 1573/2000', 'Iteration: 7870', 'Train loss: 0.016581', 'Train acc: 0.996667')\n",
      "('Epoch: 1573/2000', 'Iteration: 7870', 'Validation loss: 0.442481', 'Validation acc: 0.928333')\n",
      "('Epoch: 1574/2000', 'Iteration: 7875', 'Train loss: 0.013927', 'Train acc: 0.995000')\n",
      "('Epoch: 1575/2000', 'Iteration: 7880', 'Train loss: 0.017105', 'Train acc: 0.996667')\n",
      "('Epoch: 1575/2000', 'Iteration: 7880', 'Validation loss: 0.451959', 'Validation acc: 0.926667')\n",
      "('Epoch: 1576/2000', 'Iteration: 7885', 'Train loss: 0.009233', 'Train acc: 0.998333')\n",
      "('Epoch: 1577/2000', 'Iteration: 7890', 'Train loss: 0.007045', 'Train acc: 1.000000')\n",
      "('Epoch: 1577/2000', 'Iteration: 7890', 'Validation loss: 0.445095', 'Validation acc: 0.926667')\n",
      "('Epoch: 1578/2000', 'Iteration: 7895', 'Train loss: 0.008169', 'Train acc: 1.000000')\n",
      "('Epoch: 1579/2000', 'Iteration: 7900', 'Train loss: 0.015390', 'Train acc: 0.993333')\n",
      "('Epoch: 1579/2000', 'Iteration: 7900', 'Validation loss: 0.436055', 'Validation acc: 0.923333')\n",
      "('Epoch: 1580/2000', 'Iteration: 7905', 'Train loss: 0.010604', 'Train acc: 0.998333')\n",
      "('Epoch: 1581/2000', 'Iteration: 7910', 'Train loss: 0.012182', 'Train acc: 1.000000')\n",
      "('Epoch: 1581/2000', 'Iteration: 7910', 'Validation loss: 0.441697', 'Validation acc: 0.923333')\n",
      "('Epoch: 1582/2000', 'Iteration: 7915', 'Train loss: 0.019217', 'Train acc: 0.993333')\n",
      "('Epoch: 1583/2000', 'Iteration: 7920', 'Train loss: 0.020186', 'Train acc: 0.993333')\n",
      "('Epoch: 1583/2000', 'Iteration: 7920', 'Validation loss: 0.441154', 'Validation acc: 0.925000')\n",
      "('Epoch: 1584/2000', 'Iteration: 7925', 'Train loss: 0.013345', 'Train acc: 0.996667')\n",
      "('Epoch: 1585/2000', 'Iteration: 7930', 'Train loss: 0.010923', 'Train acc: 0.998333')\n",
      "('Epoch: 1585/2000', 'Iteration: 7930', 'Validation loss: 0.449769', 'Validation acc: 0.921667')\n",
      "('Epoch: 1586/2000', 'Iteration: 7935', 'Train loss: 0.011585', 'Train acc: 0.998333')\n",
      "('Epoch: 1587/2000', 'Iteration: 7940', 'Train loss: 0.011186', 'Train acc: 0.998333')\n",
      "('Epoch: 1587/2000', 'Iteration: 7940', 'Validation loss: 0.446310', 'Validation acc: 0.926667')\n",
      "('Epoch: 1588/2000', 'Iteration: 7945', 'Train loss: 0.010082', 'Train acc: 0.998333')\n",
      "('Epoch: 1589/2000', 'Iteration: 7950', 'Train loss: 0.013242', 'Train acc: 0.996667')\n",
      "('Epoch: 1589/2000', 'Iteration: 7950', 'Validation loss: 0.448543', 'Validation acc: 0.921667')\n",
      "('Epoch: 1590/2000', 'Iteration: 7955', 'Train loss: 0.015269', 'Train acc: 0.995000')\n",
      "('Epoch: 1591/2000', 'Iteration: 7960', 'Train loss: 0.009564', 'Train acc: 1.000000')\n",
      "('Epoch: 1591/2000', 'Iteration: 7960', 'Validation loss: 0.444603', 'Validation acc: 0.920000')\n",
      "('Epoch: 1592/2000', 'Iteration: 7965', 'Train loss: 0.017655', 'Train acc: 0.996667')\n",
      "('Epoch: 1593/2000', 'Iteration: 7970', 'Train loss: 0.013097', 'Train acc: 0.996667')\n",
      "('Epoch: 1593/2000', 'Iteration: 7970', 'Validation loss: 0.438882', 'Validation acc: 0.930000')\n",
      "('Epoch: 1594/2000', 'Iteration: 7975', 'Train loss: 0.007825', 'Train acc: 1.000000')\n",
      "('Epoch: 1595/2000', 'Iteration: 7980', 'Train loss: 0.022880', 'Train acc: 0.996667')\n",
      "('Epoch: 1595/2000', 'Iteration: 7980', 'Validation loss: 0.449425', 'Validation acc: 0.926667')\n",
      "('Epoch: 1596/2000', 'Iteration: 7985', 'Train loss: 0.010877', 'Train acc: 1.000000')\n",
      "('Epoch: 1597/2000', 'Iteration: 7990', 'Train loss: 0.016630', 'Train acc: 0.998333')\n",
      "('Epoch: 1597/2000', 'Iteration: 7990', 'Validation loss: 0.456614', 'Validation acc: 0.925000')\n",
      "('Epoch: 1598/2000', 'Iteration: 7995', 'Train loss: 0.009262', 'Train acc: 1.000000')\n",
      "('Epoch: 1599/2000', 'Iteration: 8000', 'Train loss: 0.007998', 'Train acc: 1.000000')\n",
      "('Epoch: 1599/2000', 'Iteration: 8000', 'Validation loss: 0.451501', 'Validation acc: 0.928333')\n",
      "('Epoch: 1600/2000', 'Iteration: 8005', 'Train loss: 0.007480', 'Train acc: 1.000000')\n",
      "('Epoch: 1601/2000', 'Iteration: 8010', 'Train loss: 0.012647', 'Train acc: 0.998333')\n",
      "('Epoch: 1601/2000', 'Iteration: 8010', 'Validation loss: 0.450061', 'Validation acc: 0.928333')\n",
      "('Epoch: 1602/2000', 'Iteration: 8015', 'Train loss: 0.013189', 'Train acc: 0.998333')\n",
      "('Epoch: 1603/2000', 'Iteration: 8020', 'Train loss: 0.010478', 'Train acc: 1.000000')\n",
      "('Epoch: 1603/2000', 'Iteration: 8020', 'Validation loss: 0.457331', 'Validation acc: 0.926667')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 1604/2000', 'Iteration: 8025', 'Train loss: 0.015214', 'Train acc: 0.993333')\n",
      "('Epoch: 1605/2000', 'Iteration: 8030', 'Train loss: 0.008871', 'Train acc: 1.000000')\n",
      "('Epoch: 1605/2000', 'Iteration: 8030', 'Validation loss: 0.456900', 'Validation acc: 0.925000')\n",
      "('Epoch: 1606/2000', 'Iteration: 8035', 'Train loss: 0.015085', 'Train acc: 0.998333')\n",
      "('Epoch: 1607/2000', 'Iteration: 8040', 'Train loss: 0.009538', 'Train acc: 0.998333')\n",
      "('Epoch: 1607/2000', 'Iteration: 8040', 'Validation loss: 0.455128', 'Validation acc: 0.916667')\n",
      "('Epoch: 1608/2000', 'Iteration: 8045', 'Train loss: 0.009451', 'Train acc: 0.998333')\n",
      "('Epoch: 1609/2000', 'Iteration: 8050', 'Train loss: 0.012038', 'Train acc: 0.996667')\n",
      "('Epoch: 1609/2000', 'Iteration: 8050', 'Validation loss: 0.450367', 'Validation acc: 0.925000')\n",
      "('Epoch: 1610/2000', 'Iteration: 8055', 'Train loss: 0.010191', 'Train acc: 1.000000')\n",
      "('Epoch: 1611/2000', 'Iteration: 8060', 'Train loss: 0.006988', 'Train acc: 1.000000')\n",
      "('Epoch: 1611/2000', 'Iteration: 8060', 'Validation loss: 0.448749', 'Validation acc: 0.921667')\n",
      "('Epoch: 1612/2000', 'Iteration: 8065', 'Train loss: 0.017604', 'Train acc: 0.995000')\n",
      "('Epoch: 1613/2000', 'Iteration: 8070', 'Train loss: 0.020543', 'Train acc: 0.993333')\n",
      "('Epoch: 1613/2000', 'Iteration: 8070', 'Validation loss: 0.449568', 'Validation acc: 0.926667')\n",
      "('Epoch: 1614/2000', 'Iteration: 8075', 'Train loss: 0.010394', 'Train acc: 0.998333')\n",
      "('Epoch: 1615/2000', 'Iteration: 8080', 'Train loss: 0.010706', 'Train acc: 0.996667')\n",
      "('Epoch: 1615/2000', 'Iteration: 8080', 'Validation loss: 0.454392', 'Validation acc: 0.928333')\n",
      "('Epoch: 1616/2000', 'Iteration: 8085', 'Train loss: 0.011353', 'Train acc: 1.000000')\n",
      "('Epoch: 1617/2000', 'Iteration: 8090', 'Train loss: 0.007515', 'Train acc: 1.000000')\n",
      "('Epoch: 1617/2000', 'Iteration: 8090', 'Validation loss: 0.451897', 'Validation acc: 0.926667')\n",
      "('Epoch: 1618/2000', 'Iteration: 8095', 'Train loss: 0.011360', 'Train acc: 0.996667')\n",
      "('Epoch: 1619/2000', 'Iteration: 8100', 'Train loss: 0.015188', 'Train acc: 0.998333')\n",
      "('Epoch: 1619/2000', 'Iteration: 8100', 'Validation loss: 0.453924', 'Validation acc: 0.928333')\n",
      "('Epoch: 1620/2000', 'Iteration: 8105', 'Train loss: 0.008940', 'Train acc: 0.998333')\n",
      "('Epoch: 1621/2000', 'Iteration: 8110', 'Train loss: 0.012567', 'Train acc: 0.996667')\n",
      "('Epoch: 1621/2000', 'Iteration: 8110', 'Validation loss: 0.452976', 'Validation acc: 0.923333')\n",
      "('Epoch: 1622/2000', 'Iteration: 8115', 'Train loss: 0.016657', 'Train acc: 0.995000')\n",
      "('Epoch: 1623/2000', 'Iteration: 8120', 'Train loss: 0.006879', 'Train acc: 1.000000')\n",
      "('Epoch: 1623/2000', 'Iteration: 8120', 'Validation loss: 0.449868', 'Validation acc: 0.926667')\n",
      "('Epoch: 1624/2000', 'Iteration: 8125', 'Train loss: 0.007038', 'Train acc: 1.000000')\n",
      "('Epoch: 1625/2000', 'Iteration: 8130', 'Train loss: 0.018066', 'Train acc: 0.996667')\n",
      "('Epoch: 1625/2000', 'Iteration: 8130', 'Validation loss: 0.448522', 'Validation acc: 0.931667')\n",
      "('Epoch: 1626/2000', 'Iteration: 8135', 'Train loss: 0.012729', 'Train acc: 0.996667')\n",
      "('Epoch: 1627/2000', 'Iteration: 8140', 'Train loss: 0.011824', 'Train acc: 0.998333')\n",
      "('Epoch: 1627/2000', 'Iteration: 8140', 'Validation loss: 0.451248', 'Validation acc: 0.926667')\n",
      "('Epoch: 1628/2000', 'Iteration: 8145', 'Train loss: 0.011628', 'Train acc: 0.998333')\n",
      "('Epoch: 1629/2000', 'Iteration: 8150', 'Train loss: 0.013262', 'Train acc: 0.995000')\n",
      "('Epoch: 1629/2000', 'Iteration: 8150', 'Validation loss: 0.449280', 'Validation acc: 0.925000')\n",
      "('Epoch: 1630/2000', 'Iteration: 8155', 'Train loss: 0.009331', 'Train acc: 1.000000')\n",
      "('Epoch: 1631/2000', 'Iteration: 8160', 'Train loss: 0.009005', 'Train acc: 0.998333')\n",
      "('Epoch: 1631/2000', 'Iteration: 8160', 'Validation loss: 0.446604', 'Validation acc: 0.926667')\n",
      "('Epoch: 1632/2000', 'Iteration: 8165', 'Train loss: 0.011708', 'Train acc: 0.998333')\n",
      "('Epoch: 1633/2000', 'Iteration: 8170', 'Train loss: 0.011120', 'Train acc: 1.000000')\n",
      "('Epoch: 1633/2000', 'Iteration: 8170', 'Validation loss: 0.452510', 'Validation acc: 0.930000')\n",
      "('Epoch: 1634/2000', 'Iteration: 8175', 'Train loss: 0.016425', 'Train acc: 0.995000')\n",
      "('Epoch: 1635/2000', 'Iteration: 8180', 'Train loss: 0.011829', 'Train acc: 1.000000')\n",
      "('Epoch: 1635/2000', 'Iteration: 8180', 'Validation loss: 0.446684', 'Validation acc: 0.930000')\n",
      "('Epoch: 1636/2000', 'Iteration: 8185', 'Train loss: 0.005656', 'Train acc: 1.000000')\n",
      "('Epoch: 1637/2000', 'Iteration: 8190', 'Train loss: 0.010563', 'Train acc: 1.000000')\n",
      "('Epoch: 1637/2000', 'Iteration: 8190', 'Validation loss: 0.452206', 'Validation acc: 0.930000')\n",
      "('Epoch: 1638/2000', 'Iteration: 8195', 'Train loss: 0.010573', 'Train acc: 0.998333')\n",
      "('Epoch: 1639/2000', 'Iteration: 8200', 'Train loss: 0.015040', 'Train acc: 0.995000')\n",
      "('Epoch: 1639/2000', 'Iteration: 8200', 'Validation loss: 0.456331', 'Validation acc: 0.921667')\n",
      "('Epoch: 1640/2000', 'Iteration: 8205', 'Train loss: 0.011264', 'Train acc: 0.998333')\n",
      "('Epoch: 1641/2000', 'Iteration: 8210', 'Train loss: 0.014489', 'Train acc: 0.995000')\n",
      "('Epoch: 1641/2000', 'Iteration: 8210', 'Validation loss: 0.447168', 'Validation acc: 0.928333')\n",
      "('Epoch: 1642/2000', 'Iteration: 8215', 'Train loss: 0.007560', 'Train acc: 1.000000')\n",
      "('Epoch: 1643/2000', 'Iteration: 8220', 'Train loss: 0.011830', 'Train acc: 0.996667')\n",
      "('Epoch: 1643/2000', 'Iteration: 8220', 'Validation loss: 0.460086', 'Validation acc: 0.928333')\n",
      "('Epoch: 1644/2000', 'Iteration: 8225', 'Train loss: 0.012645', 'Train acc: 0.996667')\n",
      "('Epoch: 1645/2000', 'Iteration: 8230', 'Train loss: 0.014363', 'Train acc: 0.996667')\n",
      "('Epoch: 1645/2000', 'Iteration: 8230', 'Validation loss: 0.463030', 'Validation acc: 0.928333')\n",
      "('Epoch: 1646/2000', 'Iteration: 8235', 'Train loss: 0.014858', 'Train acc: 0.993333')\n",
      "('Epoch: 1647/2000', 'Iteration: 8240', 'Train loss: 0.013131', 'Train acc: 0.998333')\n",
      "('Epoch: 1647/2000', 'Iteration: 8240', 'Validation loss: 0.441656', 'Validation acc: 0.928333')\n",
      "('Epoch: 1648/2000', 'Iteration: 8245', 'Train loss: 0.012693', 'Train acc: 0.998333')\n",
      "('Epoch: 1649/2000', 'Iteration: 8250', 'Train loss: 0.013324', 'Train acc: 0.996667')\n",
      "('Epoch: 1649/2000', 'Iteration: 8250', 'Validation loss: 0.447610', 'Validation acc: 0.925000')\n",
      "('Epoch: 1650/2000', 'Iteration: 8255', 'Train loss: 0.010886', 'Train acc: 1.000000')\n",
      "('Epoch: 1651/2000', 'Iteration: 8260', 'Train loss: 0.006637', 'Train acc: 1.000000')\n",
      "('Epoch: 1651/2000', 'Iteration: 8260', 'Validation loss: 0.453131', 'Validation acc: 0.923333')\n",
      "('Epoch: 1652/2000', 'Iteration: 8265', 'Train loss: 0.010655', 'Train acc: 0.998333')\n",
      "('Epoch: 1653/2000', 'Iteration: 8270', 'Train loss: 0.007597', 'Train acc: 0.998333')\n",
      "('Epoch: 1653/2000', 'Iteration: 8270', 'Validation loss: 0.453719', 'Validation acc: 0.926667')\n",
      "('Epoch: 1654/2000', 'Iteration: 8275', 'Train loss: 0.013009', 'Train acc: 0.996667')\n",
      "('Epoch: 1655/2000', 'Iteration: 8280', 'Train loss: 0.011582', 'Train acc: 0.998333')\n",
      "('Epoch: 1655/2000', 'Iteration: 8280', 'Validation loss: 0.454302', 'Validation acc: 0.926667')\n",
      "('Epoch: 1656/2000', 'Iteration: 8285', 'Train loss: 0.009613', 'Train acc: 1.000000')\n",
      "('Epoch: 1657/2000', 'Iteration: 8290', 'Train loss: 0.009350', 'Train acc: 0.998333')\n",
      "('Epoch: 1657/2000', 'Iteration: 8290', 'Validation loss: 0.455251', 'Validation acc: 0.926667')\n",
      "('Epoch: 1658/2000', 'Iteration: 8295', 'Train loss: 0.011463', 'Train acc: 0.998333')\n",
      "('Epoch: 1659/2000', 'Iteration: 8300', 'Train loss: 0.009732', 'Train acc: 1.000000')\n",
      "('Epoch: 1659/2000', 'Iteration: 8300', 'Validation loss: 0.465352', 'Validation acc: 0.926667')\n",
      "('Epoch: 1660/2000', 'Iteration: 8305', 'Train loss: 0.010854', 'Train acc: 0.998333')\n",
      "('Epoch: 1661/2000', 'Iteration: 8310', 'Train loss: 0.009339', 'Train acc: 0.998333')\n",
      "('Epoch: 1661/2000', 'Iteration: 8310', 'Validation loss: 0.459515', 'Validation acc: 0.925000')\n",
      "('Epoch: 1662/2000', 'Iteration: 8315', 'Train loss: 0.013083', 'Train acc: 0.996667')\n",
      "('Epoch: 1663/2000', 'Iteration: 8320', 'Train loss: 0.011661', 'Train acc: 0.996667')\n",
      "('Epoch: 1663/2000', 'Iteration: 8320', 'Validation loss: 0.456690', 'Validation acc: 0.928333')\n",
      "('Epoch: 1664/2000', 'Iteration: 8325', 'Train loss: 0.012285', 'Train acc: 0.996667')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 1665/2000', 'Iteration: 8330', 'Train loss: 0.006644', 'Train acc: 1.000000')\n",
      "('Epoch: 1665/2000', 'Iteration: 8330', 'Validation loss: 0.452378', 'Validation acc: 0.931667')\n",
      "('Epoch: 1666/2000', 'Iteration: 8335', 'Train loss: 0.012753', 'Train acc: 0.998333')\n",
      "('Epoch: 1667/2000', 'Iteration: 8340', 'Train loss: 0.012123', 'Train acc: 1.000000')\n",
      "('Epoch: 1667/2000', 'Iteration: 8340', 'Validation loss: 0.454916', 'Validation acc: 0.921667')\n",
      "('Epoch: 1668/2000', 'Iteration: 8345', 'Train loss: 0.007900', 'Train acc: 0.998333')\n",
      "('Epoch: 1669/2000', 'Iteration: 8350', 'Train loss: 0.010242', 'Train acc: 0.998333')\n",
      "('Epoch: 1669/2000', 'Iteration: 8350', 'Validation loss: 0.454840', 'Validation acc: 0.926667')\n",
      "('Epoch: 1670/2000', 'Iteration: 8355', 'Train loss: 0.010113', 'Train acc: 0.996667')\n",
      "('Epoch: 1671/2000', 'Iteration: 8360', 'Train loss: 0.011572', 'Train acc: 0.998333')\n",
      "('Epoch: 1671/2000', 'Iteration: 8360', 'Validation loss: 0.459072', 'Validation acc: 0.925000')\n",
      "('Epoch: 1672/2000', 'Iteration: 8365', 'Train loss: 0.009858', 'Train acc: 1.000000')\n",
      "('Epoch: 1673/2000', 'Iteration: 8370', 'Train loss: 0.014293', 'Train acc: 0.993333')\n",
      "('Epoch: 1673/2000', 'Iteration: 8370', 'Validation loss: 0.454482', 'Validation acc: 0.926667')\n",
      "('Epoch: 1674/2000', 'Iteration: 8375', 'Train loss: 0.013969', 'Train acc: 0.998333')\n",
      "('Epoch: 1675/2000', 'Iteration: 8380', 'Train loss: 0.008772', 'Train acc: 1.000000')\n",
      "('Epoch: 1675/2000', 'Iteration: 8380', 'Validation loss: 0.457528', 'Validation acc: 0.930000')\n",
      "('Epoch: 1676/2000', 'Iteration: 8385', 'Train loss: 0.011711', 'Train acc: 0.996667')\n",
      "('Epoch: 1677/2000', 'Iteration: 8390', 'Train loss: 0.011431', 'Train acc: 0.998333')\n",
      "('Epoch: 1677/2000', 'Iteration: 8390', 'Validation loss: 0.461792', 'Validation acc: 0.926667')\n",
      "('Epoch: 1678/2000', 'Iteration: 8395', 'Train loss: 0.016430', 'Train acc: 0.995000')\n",
      "('Epoch: 1679/2000', 'Iteration: 8400', 'Train loss: 0.017159', 'Train acc: 0.996667')\n",
      "('Epoch: 1679/2000', 'Iteration: 8400', 'Validation loss: 0.452066', 'Validation acc: 0.921667')\n",
      "('Epoch: 1680/2000', 'Iteration: 8405', 'Train loss: 0.015783', 'Train acc: 0.995000')\n",
      "('Epoch: 1681/2000', 'Iteration: 8410', 'Train loss: 0.006786', 'Train acc: 1.000000')\n",
      "('Epoch: 1681/2000', 'Iteration: 8410', 'Validation loss: 0.448852', 'Validation acc: 0.923333')\n",
      "('Epoch: 1682/2000', 'Iteration: 8415', 'Train loss: 0.011089', 'Train acc: 0.998333')\n",
      "('Epoch: 1683/2000', 'Iteration: 8420', 'Train loss: 0.007126', 'Train acc: 1.000000')\n",
      "('Epoch: 1683/2000', 'Iteration: 8420', 'Validation loss: 0.459326', 'Validation acc: 0.926667')\n",
      "('Epoch: 1684/2000', 'Iteration: 8425', 'Train loss: 0.009534', 'Train acc: 1.000000')\n",
      "('Epoch: 1685/2000', 'Iteration: 8430', 'Train loss: 0.014822', 'Train acc: 0.995000')\n",
      "('Epoch: 1685/2000', 'Iteration: 8430', 'Validation loss: 0.452115', 'Validation acc: 0.926667')\n",
      "('Epoch: 1686/2000', 'Iteration: 8435', 'Train loss: 0.007903', 'Train acc: 0.998333')\n",
      "('Epoch: 1687/2000', 'Iteration: 8440', 'Train loss: 0.009239', 'Train acc: 0.998333')\n",
      "('Epoch: 1687/2000', 'Iteration: 8440', 'Validation loss: 0.456433', 'Validation acc: 0.928333')\n",
      "('Epoch: 1688/2000', 'Iteration: 8445', 'Train loss: 0.014302', 'Train acc: 0.998333')\n",
      "('Epoch: 1689/2000', 'Iteration: 8450', 'Train loss: 0.011170', 'Train acc: 0.996667')\n",
      "('Epoch: 1689/2000', 'Iteration: 8450', 'Validation loss: 0.452838', 'Validation acc: 0.930000')\n",
      "('Epoch: 1690/2000', 'Iteration: 8455', 'Train loss: 0.008671', 'Train acc: 0.998333')\n",
      "('Epoch: 1691/2000', 'Iteration: 8460', 'Train loss: 0.010234', 'Train acc: 0.998333')\n",
      "('Epoch: 1691/2000', 'Iteration: 8460', 'Validation loss: 0.450567', 'Validation acc: 0.925000')\n",
      "('Epoch: 1692/2000', 'Iteration: 8465', 'Train loss: 0.008720', 'Train acc: 0.998333')\n",
      "('Epoch: 1693/2000', 'Iteration: 8470', 'Train loss: 0.010428', 'Train acc: 0.996667')\n",
      "('Epoch: 1693/2000', 'Iteration: 8470', 'Validation loss: 0.454073', 'Validation acc: 0.928333')\n",
      "('Epoch: 1694/2000', 'Iteration: 8475', 'Train loss: 0.010279', 'Train acc: 0.996667')\n",
      "('Epoch: 1695/2000', 'Iteration: 8480', 'Train loss: 0.011893', 'Train acc: 0.996667')\n",
      "('Epoch: 1695/2000', 'Iteration: 8480', 'Validation loss: 0.448948', 'Validation acc: 0.925000')\n",
      "('Epoch: 1696/2000', 'Iteration: 8485', 'Train loss: 0.008422', 'Train acc: 0.996667')\n",
      "('Epoch: 1697/2000', 'Iteration: 8490', 'Train loss: 0.009901', 'Train acc: 0.998333')\n",
      "('Epoch: 1697/2000', 'Iteration: 8490', 'Validation loss: 0.450715', 'Validation acc: 0.928333')\n",
      "('Epoch: 1698/2000', 'Iteration: 8495', 'Train loss: 0.008528', 'Train acc: 1.000000')\n",
      "('Epoch: 1699/2000', 'Iteration: 8500', 'Train loss: 0.011125', 'Train acc: 0.998333')\n",
      "('Epoch: 1699/2000', 'Iteration: 8500', 'Validation loss: 0.452334', 'Validation acc: 0.928333')\n",
      "('Epoch: 1700/2000', 'Iteration: 8505', 'Train loss: 0.009721', 'Train acc: 1.000000')\n",
      "('Epoch: 1701/2000', 'Iteration: 8510', 'Train loss: 0.009131', 'Train acc: 0.998333')\n",
      "('Epoch: 1701/2000', 'Iteration: 8510', 'Validation loss: 0.457964', 'Validation acc: 0.925000')\n",
      "('Epoch: 1702/2000', 'Iteration: 8515', 'Train loss: 0.009009', 'Train acc: 0.996667')\n",
      "('Epoch: 1703/2000', 'Iteration: 8520', 'Train loss: 0.012174', 'Train acc: 0.998333')\n",
      "('Epoch: 1703/2000', 'Iteration: 8520', 'Validation loss: 0.459834', 'Validation acc: 0.923333')\n",
      "('Epoch: 1704/2000', 'Iteration: 8525', 'Train loss: 0.012281', 'Train acc: 1.000000')\n",
      "('Epoch: 1705/2000', 'Iteration: 8530', 'Train loss: 0.015920', 'Train acc: 0.993333')\n",
      "('Epoch: 1705/2000', 'Iteration: 8530', 'Validation loss: 0.453093', 'Validation acc: 0.925000')\n",
      "('Epoch: 1706/2000', 'Iteration: 8535', 'Train loss: 0.009032', 'Train acc: 1.000000')\n",
      "('Epoch: 1707/2000', 'Iteration: 8540', 'Train loss: 0.010937', 'Train acc: 0.996667')\n",
      "('Epoch: 1707/2000', 'Iteration: 8540', 'Validation loss: 0.449079', 'Validation acc: 0.928333')\n",
      "('Epoch: 1708/2000', 'Iteration: 8545', 'Train loss: 0.011071', 'Train acc: 0.996667')\n",
      "('Epoch: 1709/2000', 'Iteration: 8550', 'Train loss: 0.010963', 'Train acc: 0.996667')\n",
      "('Epoch: 1709/2000', 'Iteration: 8550', 'Validation loss: 0.453240', 'Validation acc: 0.923333')\n",
      "('Epoch: 1710/2000', 'Iteration: 8555', 'Train loss: 0.009460', 'Train acc: 0.998333')\n",
      "('Epoch: 1711/2000', 'Iteration: 8560', 'Train loss: 0.008395', 'Train acc: 1.000000')\n",
      "('Epoch: 1711/2000', 'Iteration: 8560', 'Validation loss: 0.459253', 'Validation acc: 0.925000')\n",
      "('Epoch: 1712/2000', 'Iteration: 8565', 'Train loss: 0.007819', 'Train acc: 1.000000')\n",
      "('Epoch: 1713/2000', 'Iteration: 8570', 'Train loss: 0.011565', 'Train acc: 0.998333')\n",
      "('Epoch: 1713/2000', 'Iteration: 8570', 'Validation loss: 0.456742', 'Validation acc: 0.926667')\n",
      "('Epoch: 1714/2000', 'Iteration: 8575', 'Train loss: 0.010163', 'Train acc: 0.998333')\n",
      "('Epoch: 1715/2000', 'Iteration: 8580', 'Train loss: 0.012461', 'Train acc: 0.998333')\n",
      "('Epoch: 1715/2000', 'Iteration: 8580', 'Validation loss: 0.455807', 'Validation acc: 0.923333')\n",
      "('Epoch: 1716/2000', 'Iteration: 8585', 'Train loss: 0.016179', 'Train acc: 0.996667')\n",
      "('Epoch: 1717/2000', 'Iteration: 8590', 'Train loss: 0.009020', 'Train acc: 0.998333')\n",
      "('Epoch: 1717/2000', 'Iteration: 8590', 'Validation loss: 0.456319', 'Validation acc: 0.928333')\n",
      "('Epoch: 1718/2000', 'Iteration: 8595', 'Train loss: 0.011130', 'Train acc: 1.000000')\n",
      "('Epoch: 1719/2000', 'Iteration: 8600', 'Train loss: 0.009337', 'Train acc: 0.998333')\n",
      "('Epoch: 1719/2000', 'Iteration: 8600', 'Validation loss: 0.451498', 'Validation acc: 0.930000')\n",
      "('Epoch: 1720/2000', 'Iteration: 8605', 'Train loss: 0.009691', 'Train acc: 0.996667')\n",
      "('Epoch: 1721/2000', 'Iteration: 8610', 'Train loss: 0.021485', 'Train acc: 0.996667')\n",
      "('Epoch: 1721/2000', 'Iteration: 8610', 'Validation loss: 0.458893', 'Validation acc: 0.925000')\n",
      "('Epoch: 1722/2000', 'Iteration: 8615', 'Train loss: 0.012478', 'Train acc: 0.996667')\n",
      "('Epoch: 1723/2000', 'Iteration: 8620', 'Train loss: 0.008103', 'Train acc: 0.998333')\n",
      "('Epoch: 1723/2000', 'Iteration: 8620', 'Validation loss: 0.450649', 'Validation acc: 0.926667')\n",
      "('Epoch: 1724/2000', 'Iteration: 8625', 'Train loss: 0.008154', 'Train acc: 1.000000')\n",
      "('Epoch: 1725/2000', 'Iteration: 8630', 'Train loss: 0.007122', 'Train acc: 1.000000')\n",
      "('Epoch: 1725/2000', 'Iteration: 8630', 'Validation loss: 0.452881', 'Validation acc: 0.923333')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 1726/2000', 'Iteration: 8635', 'Train loss: 0.019756', 'Train acc: 0.996667')\n",
      "('Epoch: 1727/2000', 'Iteration: 8640', 'Train loss: 0.010250', 'Train acc: 0.995000')\n",
      "('Epoch: 1727/2000', 'Iteration: 8640', 'Validation loss: 0.457349', 'Validation acc: 0.925000')\n",
      "('Epoch: 1728/2000', 'Iteration: 8645', 'Train loss: 0.007941', 'Train acc: 1.000000')\n",
      "('Epoch: 1729/2000', 'Iteration: 8650', 'Train loss: 0.013022', 'Train acc: 0.996667')\n",
      "('Epoch: 1729/2000', 'Iteration: 8650', 'Validation loss: 0.454785', 'Validation acc: 0.925000')\n",
      "('Epoch: 1730/2000', 'Iteration: 8655', 'Train loss: 0.010994', 'Train acc: 0.996667')\n",
      "('Epoch: 1731/2000', 'Iteration: 8660', 'Train loss: 0.013826', 'Train acc: 0.995000')\n",
      "('Epoch: 1731/2000', 'Iteration: 8660', 'Validation loss: 0.457687', 'Validation acc: 0.920000')\n",
      "('Epoch: 1732/2000', 'Iteration: 8665', 'Train loss: 0.013138', 'Train acc: 0.996667')\n",
      "('Epoch: 1733/2000', 'Iteration: 8670', 'Train loss: 0.008660', 'Train acc: 0.998333')\n",
      "('Epoch: 1733/2000', 'Iteration: 8670', 'Validation loss: 0.458354', 'Validation acc: 0.928333')\n",
      "('Epoch: 1734/2000', 'Iteration: 8675', 'Train loss: 0.008906', 'Train acc: 0.998333')\n",
      "('Epoch: 1735/2000', 'Iteration: 8680', 'Train loss: 0.007281', 'Train acc: 0.998333')\n",
      "('Epoch: 1735/2000', 'Iteration: 8680', 'Validation loss: 0.465549', 'Validation acc: 0.926667')\n",
      "('Epoch: 1736/2000', 'Iteration: 8685', 'Train loss: 0.007817', 'Train acc: 1.000000')\n",
      "('Epoch: 1737/2000', 'Iteration: 8690', 'Train loss: 0.012239', 'Train acc: 0.996667')\n",
      "('Epoch: 1737/2000', 'Iteration: 8690', 'Validation loss: 0.457821', 'Validation acc: 0.930000')\n",
      "('Epoch: 1738/2000', 'Iteration: 8695', 'Train loss: 0.011605', 'Train acc: 0.998333')\n",
      "('Epoch: 1739/2000', 'Iteration: 8700', 'Train loss: 0.008877', 'Train acc: 1.000000')\n",
      "('Epoch: 1739/2000', 'Iteration: 8700', 'Validation loss: 0.457472', 'Validation acc: 0.926667')\n",
      "('Epoch: 1740/2000', 'Iteration: 8705', 'Train loss: 0.007600', 'Train acc: 0.998333')\n",
      "('Epoch: 1741/2000', 'Iteration: 8710', 'Train loss: 0.012228', 'Train acc: 0.996667')\n",
      "('Epoch: 1741/2000', 'Iteration: 8710', 'Validation loss: 0.462638', 'Validation acc: 0.926667')\n",
      "('Epoch: 1742/2000', 'Iteration: 8715', 'Train loss: 0.008490', 'Train acc: 0.998333')\n",
      "('Epoch: 1743/2000', 'Iteration: 8720', 'Train loss: 0.007088', 'Train acc: 1.000000')\n",
      "('Epoch: 1743/2000', 'Iteration: 8720', 'Validation loss: 0.455654', 'Validation acc: 0.928333')\n",
      "('Epoch: 1744/2000', 'Iteration: 8725', 'Train loss: 0.010412', 'Train acc: 0.996667')\n",
      "('Epoch: 1745/2000', 'Iteration: 8730', 'Train loss: 0.008569', 'Train acc: 0.998333')\n",
      "('Epoch: 1745/2000', 'Iteration: 8730', 'Validation loss: 0.460906', 'Validation acc: 0.926667')\n",
      "('Epoch: 1746/2000', 'Iteration: 8735', 'Train loss: 0.010816', 'Train acc: 1.000000')\n",
      "('Epoch: 1747/2000', 'Iteration: 8740', 'Train loss: 0.013040', 'Train acc: 0.995000')\n",
      "('Epoch: 1747/2000', 'Iteration: 8740', 'Validation loss: 0.458799', 'Validation acc: 0.928333')\n",
      "('Epoch: 1748/2000', 'Iteration: 8745', 'Train loss: 0.011059', 'Train acc: 0.998333')\n",
      "('Epoch: 1749/2000', 'Iteration: 8750', 'Train loss: 0.006654', 'Train acc: 1.000000')\n",
      "('Epoch: 1749/2000', 'Iteration: 8750', 'Validation loss: 0.461651', 'Validation acc: 0.930000')\n",
      "('Epoch: 1750/2000', 'Iteration: 8755', 'Train loss: 0.009627', 'Train acc: 0.996667')\n",
      "('Epoch: 1751/2000', 'Iteration: 8760', 'Train loss: 0.007954', 'Train acc: 1.000000')\n",
      "('Epoch: 1751/2000', 'Iteration: 8760', 'Validation loss: 0.463547', 'Validation acc: 0.925000')\n",
      "('Epoch: 1752/2000', 'Iteration: 8765', 'Train loss: 0.011676', 'Train acc: 0.995000')\n",
      "('Epoch: 1753/2000', 'Iteration: 8770', 'Train loss: 0.014525', 'Train acc: 0.993333')\n",
      "('Epoch: 1753/2000', 'Iteration: 8770', 'Validation loss: 0.460725', 'Validation acc: 0.928333')\n",
      "('Epoch: 1754/2000', 'Iteration: 8775', 'Train loss: 0.016036', 'Train acc: 0.995000')\n",
      "('Epoch: 1755/2000', 'Iteration: 8780', 'Train loss: 0.009636', 'Train acc: 0.996667')\n",
      "('Epoch: 1755/2000', 'Iteration: 8780', 'Validation loss: 0.461929', 'Validation acc: 0.925000')\n",
      "('Epoch: 1756/2000', 'Iteration: 8785', 'Train loss: 0.011196', 'Train acc: 0.996667')\n",
      "('Epoch: 1757/2000', 'Iteration: 8790', 'Train loss: 0.013393', 'Train acc: 0.996667')\n",
      "('Epoch: 1757/2000', 'Iteration: 8790', 'Validation loss: 0.457816', 'Validation acc: 0.930000')\n",
      "('Epoch: 1758/2000', 'Iteration: 8795', 'Train loss: 0.007474', 'Train acc: 1.000000')\n",
      "('Epoch: 1759/2000', 'Iteration: 8800', 'Train loss: 0.010845', 'Train acc: 1.000000')\n",
      "('Epoch: 1759/2000', 'Iteration: 8800', 'Validation loss: 0.466250', 'Validation acc: 0.921667')\n",
      "('Epoch: 1760/2000', 'Iteration: 8805', 'Train loss: 0.005333', 'Train acc: 1.000000')\n",
      "('Epoch: 1761/2000', 'Iteration: 8810', 'Train loss: 0.010556', 'Train acc: 0.995000')\n",
      "('Epoch: 1761/2000', 'Iteration: 8810', 'Validation loss: 0.459734', 'Validation acc: 0.925000')\n",
      "('Epoch: 1762/2000', 'Iteration: 8815', 'Train loss: 0.010202', 'Train acc: 0.998333')\n",
      "('Epoch: 1763/2000', 'Iteration: 8820', 'Train loss: 0.011199', 'Train acc: 0.996667')\n",
      "('Epoch: 1763/2000', 'Iteration: 8820', 'Validation loss: 0.443595', 'Validation acc: 0.926667')\n",
      "('Epoch: 1764/2000', 'Iteration: 8825', 'Train loss: 0.015037', 'Train acc: 0.996667')\n",
      "('Epoch: 1765/2000', 'Iteration: 8830', 'Train loss: 0.020277', 'Train acc: 0.993333')\n",
      "('Epoch: 1765/2000', 'Iteration: 8830', 'Validation loss: 0.449994', 'Validation acc: 0.931667')\n",
      "('Epoch: 1766/2000', 'Iteration: 8835', 'Train loss: 0.008675', 'Train acc: 0.998333')\n",
      "('Epoch: 1767/2000', 'Iteration: 8840', 'Train loss: 0.010452', 'Train acc: 0.998333')\n",
      "('Epoch: 1767/2000', 'Iteration: 8840', 'Validation loss: 0.458782', 'Validation acc: 0.931667')\n",
      "('Epoch: 1768/2000', 'Iteration: 8845', 'Train loss: 0.006479', 'Train acc: 0.998333')\n",
      "('Epoch: 1769/2000', 'Iteration: 8850', 'Train loss: 0.010136', 'Train acc: 0.998333')\n",
      "('Epoch: 1769/2000', 'Iteration: 8850', 'Validation loss: 0.457516', 'Validation acc: 0.928333')\n",
      "('Epoch: 1770/2000', 'Iteration: 8855', 'Train loss: 0.007219', 'Train acc: 0.998333')\n",
      "('Epoch: 1771/2000', 'Iteration: 8860', 'Train loss: 0.008389', 'Train acc: 1.000000')\n",
      "('Epoch: 1771/2000', 'Iteration: 8860', 'Validation loss: 0.457568', 'Validation acc: 0.926667')\n",
      "('Epoch: 1772/2000', 'Iteration: 8865', 'Train loss: 0.009417', 'Train acc: 1.000000')\n",
      "('Epoch: 1773/2000', 'Iteration: 8870', 'Train loss: 0.010532', 'Train acc: 0.996667')\n",
      "('Epoch: 1773/2000', 'Iteration: 8870', 'Validation loss: 0.463807', 'Validation acc: 0.926667')\n",
      "('Epoch: 1774/2000', 'Iteration: 8875', 'Train loss: 0.008043', 'Train acc: 1.000000')\n",
      "('Epoch: 1775/2000', 'Iteration: 8880', 'Train loss: 0.005001', 'Train acc: 1.000000')\n",
      "('Epoch: 1775/2000', 'Iteration: 8880', 'Validation loss: 0.461117', 'Validation acc: 0.930000')\n",
      "('Epoch: 1776/2000', 'Iteration: 8885', 'Train loss: 0.013285', 'Train acc: 0.995000')\n",
      "('Epoch: 1777/2000', 'Iteration: 8890', 'Train loss: 0.008579', 'Train acc: 1.000000')\n",
      "('Epoch: 1777/2000', 'Iteration: 8890', 'Validation loss: 0.453918', 'Validation acc: 0.928333')\n",
      "('Epoch: 1778/2000', 'Iteration: 8895', 'Train loss: 0.016198', 'Train acc: 0.995000')\n",
      "('Epoch: 1779/2000', 'Iteration: 8900', 'Train loss: 0.011164', 'Train acc: 0.996667')\n",
      "('Epoch: 1779/2000', 'Iteration: 8900', 'Validation loss: 0.457394', 'Validation acc: 0.926667')\n",
      "('Epoch: 1780/2000', 'Iteration: 8905', 'Train loss: 0.007742', 'Train acc: 1.000000')\n",
      "('Epoch: 1781/2000', 'Iteration: 8910', 'Train loss: 0.008402', 'Train acc: 1.000000')\n",
      "('Epoch: 1781/2000', 'Iteration: 8910', 'Validation loss: 0.457760', 'Validation acc: 0.928333')\n",
      "('Epoch: 1782/2000', 'Iteration: 8915', 'Train loss: 0.008599', 'Train acc: 0.998333')\n",
      "('Epoch: 1783/2000', 'Iteration: 8920', 'Train loss: 0.006673', 'Train acc: 0.998333')\n",
      "('Epoch: 1783/2000', 'Iteration: 8920', 'Validation loss: 0.458892', 'Validation acc: 0.928333')\n",
      "('Epoch: 1784/2000', 'Iteration: 8925', 'Train loss: 0.014945', 'Train acc: 0.996667')\n",
      "('Epoch: 1785/2000', 'Iteration: 8930', 'Train loss: 0.008164', 'Train acc: 0.998333')\n",
      "('Epoch: 1785/2000', 'Iteration: 8930', 'Validation loss: 0.466013', 'Validation acc: 0.930000')\n",
      "('Epoch: 1786/2000', 'Iteration: 8935', 'Train loss: 0.008986', 'Train acc: 0.998333')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 1787/2000', 'Iteration: 8940', 'Train loss: 0.010088', 'Train acc: 0.998333')\n",
      "('Epoch: 1787/2000', 'Iteration: 8940', 'Validation loss: 0.465612', 'Validation acc: 0.931667')\n",
      "('Epoch: 1788/2000', 'Iteration: 8945', 'Train loss: 0.008404', 'Train acc: 0.998333')\n",
      "('Epoch: 1789/2000', 'Iteration: 8950', 'Train loss: 0.011343', 'Train acc: 1.000000')\n",
      "('Epoch: 1789/2000', 'Iteration: 8950', 'Validation loss: 0.467136', 'Validation acc: 0.926667')\n",
      "('Epoch: 1790/2000', 'Iteration: 8955', 'Train loss: 0.006089', 'Train acc: 1.000000')\n",
      "('Epoch: 1791/2000', 'Iteration: 8960', 'Train loss: 0.006989', 'Train acc: 0.998333')\n",
      "('Epoch: 1791/2000', 'Iteration: 8960', 'Validation loss: 0.470007', 'Validation acc: 0.923333')\n",
      "('Epoch: 1792/2000', 'Iteration: 8965', 'Train loss: 0.006721', 'Train acc: 1.000000')\n",
      "('Epoch: 1793/2000', 'Iteration: 8970', 'Train loss: 0.007150', 'Train acc: 1.000000')\n",
      "('Epoch: 1793/2000', 'Iteration: 8970', 'Validation loss: 0.470767', 'Validation acc: 0.931667')\n",
      "('Epoch: 1794/2000', 'Iteration: 8975', 'Train loss: 0.008759', 'Train acc: 0.998333')\n",
      "('Epoch: 1795/2000', 'Iteration: 8980', 'Train loss: 0.008106', 'Train acc: 0.998333')\n",
      "('Epoch: 1795/2000', 'Iteration: 8980', 'Validation loss: 0.459271', 'Validation acc: 0.928333')\n",
      "('Epoch: 1796/2000', 'Iteration: 8985', 'Train loss: 0.011130', 'Train acc: 0.995000')\n",
      "('Epoch: 1797/2000', 'Iteration: 8990', 'Train loss: 0.011149', 'Train acc: 0.998333')\n",
      "('Epoch: 1797/2000', 'Iteration: 8990', 'Validation loss: 0.461499', 'Validation acc: 0.928333')\n",
      "('Epoch: 1798/2000', 'Iteration: 8995', 'Train loss: 0.007181', 'Train acc: 1.000000')\n",
      "('Epoch: 1799/2000', 'Iteration: 9000', 'Train loss: 0.007481', 'Train acc: 0.998333')\n",
      "('Epoch: 1799/2000', 'Iteration: 9000', 'Validation loss: 0.465598', 'Validation acc: 0.928333')\n",
      "('Epoch: 1800/2000', 'Iteration: 9005', 'Train loss: 0.009376', 'Train acc: 0.996667')\n",
      "('Epoch: 1801/2000', 'Iteration: 9010', 'Train loss: 0.007195', 'Train acc: 1.000000')\n",
      "('Epoch: 1801/2000', 'Iteration: 9010', 'Validation loss: 0.465849', 'Validation acc: 0.930000')\n",
      "('Epoch: 1802/2000', 'Iteration: 9015', 'Train loss: 0.009800', 'Train acc: 0.998333')\n",
      "('Epoch: 1803/2000', 'Iteration: 9020', 'Train loss: 0.008151', 'Train acc: 1.000000')\n",
      "('Epoch: 1803/2000', 'Iteration: 9020', 'Validation loss: 0.461369', 'Validation acc: 0.928333')\n",
      "('Epoch: 1804/2000', 'Iteration: 9025', 'Train loss: 0.006995', 'Train acc: 1.000000')\n",
      "('Epoch: 1805/2000', 'Iteration: 9030', 'Train loss: 0.006702', 'Train acc: 1.000000')\n",
      "('Epoch: 1805/2000', 'Iteration: 9030', 'Validation loss: 0.462627', 'Validation acc: 0.930000')\n",
      "('Epoch: 1806/2000', 'Iteration: 9035', 'Train loss: 0.014917', 'Train acc: 0.995000')\n",
      "('Epoch: 1807/2000', 'Iteration: 9040', 'Train loss: 0.010601', 'Train acc: 0.998333')\n",
      "('Epoch: 1807/2000', 'Iteration: 9040', 'Validation loss: 0.459130', 'Validation acc: 0.926667')\n",
      "('Epoch: 1808/2000', 'Iteration: 9045', 'Train loss: 0.008384', 'Train acc: 0.996667')\n",
      "('Epoch: 1809/2000', 'Iteration: 9050', 'Train loss: 0.010327', 'Train acc: 0.996667')\n",
      "('Epoch: 1809/2000', 'Iteration: 9050', 'Validation loss: 0.464397', 'Validation acc: 0.930000')\n",
      "('Epoch: 1810/2000', 'Iteration: 9055', 'Train loss: 0.007755', 'Train acc: 1.000000')\n",
      "('Epoch: 1811/2000', 'Iteration: 9060', 'Train loss: 0.007137', 'Train acc: 0.998333')\n",
      "('Epoch: 1811/2000', 'Iteration: 9060', 'Validation loss: 0.469279', 'Validation acc: 0.931667')\n",
      "('Epoch: 1812/2000', 'Iteration: 9065', 'Train loss: 0.015525', 'Train acc: 0.993333')\n",
      "('Epoch: 1813/2000', 'Iteration: 9070', 'Train loss: 0.005631', 'Train acc: 1.000000')\n",
      "('Epoch: 1813/2000', 'Iteration: 9070', 'Validation loss: 0.469626', 'Validation acc: 0.931667')\n",
      "('Epoch: 1814/2000', 'Iteration: 9075', 'Train loss: 0.010195', 'Train acc: 0.998333')\n",
      "('Epoch: 1815/2000', 'Iteration: 9080', 'Train loss: 0.010958', 'Train acc: 0.996667')\n",
      "('Epoch: 1815/2000', 'Iteration: 9080', 'Validation loss: 0.466419', 'Validation acc: 0.926667')\n",
      "('Epoch: 1816/2000', 'Iteration: 9085', 'Train loss: 0.008002', 'Train acc: 1.000000')\n",
      "('Epoch: 1817/2000', 'Iteration: 9090', 'Train loss: 0.005058', 'Train acc: 1.000000')\n",
      "('Epoch: 1817/2000', 'Iteration: 9090', 'Validation loss: 0.463759', 'Validation acc: 0.930000')\n",
      "('Epoch: 1818/2000', 'Iteration: 9095', 'Train loss: 0.006767', 'Train acc: 1.000000')\n",
      "('Epoch: 1819/2000', 'Iteration: 9100', 'Train loss: 0.005738', 'Train acc: 1.000000')\n",
      "('Epoch: 1819/2000', 'Iteration: 9100', 'Validation loss: 0.466692', 'Validation acc: 0.926667')\n",
      "('Epoch: 1820/2000', 'Iteration: 9105', 'Train loss: 0.007138', 'Train acc: 0.998333')\n",
      "('Epoch: 1821/2000', 'Iteration: 9110', 'Train loss: 0.024196', 'Train acc: 0.993333')\n",
      "('Epoch: 1821/2000', 'Iteration: 9110', 'Validation loss: 0.461932', 'Validation acc: 0.926667')\n",
      "('Epoch: 1822/2000', 'Iteration: 9115', 'Train loss: 0.008641', 'Train acc: 0.998333')\n",
      "('Epoch: 1823/2000', 'Iteration: 9120', 'Train loss: 0.006503', 'Train acc: 1.000000')\n",
      "('Epoch: 1823/2000', 'Iteration: 9120', 'Validation loss: 0.457183', 'Validation acc: 0.930000')\n",
      "('Epoch: 1824/2000', 'Iteration: 9125', 'Train loss: 0.010999', 'Train acc: 0.996667')\n",
      "('Epoch: 1825/2000', 'Iteration: 9130', 'Train loss: 0.009055', 'Train acc: 0.998333')\n",
      "('Epoch: 1825/2000', 'Iteration: 9130', 'Validation loss: 0.483874', 'Validation acc: 0.926667')\n",
      "('Epoch: 1826/2000', 'Iteration: 9135', 'Train loss: 0.008438', 'Train acc: 0.998333')\n",
      "('Epoch: 1827/2000', 'Iteration: 9140', 'Train loss: 0.011947', 'Train acc: 0.995000')\n",
      "('Epoch: 1827/2000', 'Iteration: 9140', 'Validation loss: 0.470613', 'Validation acc: 0.933333')\n",
      "('Epoch: 1828/2000', 'Iteration: 9145', 'Train loss: 0.007876', 'Train acc: 0.998333')\n",
      "('Epoch: 1829/2000', 'Iteration: 9150', 'Train loss: 0.010584', 'Train acc: 0.996667')\n",
      "('Epoch: 1829/2000', 'Iteration: 9150', 'Validation loss: 0.473589', 'Validation acc: 0.930000')\n",
      "('Epoch: 1830/2000', 'Iteration: 9155', 'Train loss: 0.014106', 'Train acc: 0.995000')\n",
      "('Epoch: 1831/2000', 'Iteration: 9160', 'Train loss: 0.015195', 'Train acc: 0.996667')\n",
      "('Epoch: 1831/2000', 'Iteration: 9160', 'Validation loss: 0.464093', 'Validation acc: 0.926667')\n",
      "('Epoch: 1832/2000', 'Iteration: 9165', 'Train loss: 0.007841', 'Train acc: 0.998333')\n",
      "('Epoch: 1833/2000', 'Iteration: 9170', 'Train loss: 0.010953', 'Train acc: 0.995000')\n",
      "('Epoch: 1833/2000', 'Iteration: 9170', 'Validation loss: 0.469636', 'Validation acc: 0.930000')\n",
      "('Epoch: 1834/2000', 'Iteration: 9175', 'Train loss: 0.012708', 'Train acc: 0.998333')\n",
      "('Epoch: 1835/2000', 'Iteration: 9180', 'Train loss: 0.012097', 'Train acc: 0.995000')\n",
      "('Epoch: 1835/2000', 'Iteration: 9180', 'Validation loss: 0.466979', 'Validation acc: 0.930000')\n",
      "('Epoch: 1836/2000', 'Iteration: 9185', 'Train loss: 0.009173', 'Train acc: 0.998333')\n",
      "('Epoch: 1837/2000', 'Iteration: 9190', 'Train loss: 0.006827', 'Train acc: 1.000000')\n",
      "('Epoch: 1837/2000', 'Iteration: 9190', 'Validation loss: 0.460765', 'Validation acc: 0.931667')\n",
      "('Epoch: 1838/2000', 'Iteration: 9195', 'Train loss: 0.010001', 'Train acc: 1.000000')\n",
      "('Epoch: 1839/2000', 'Iteration: 9200', 'Train loss: 0.010132', 'Train acc: 0.998333')\n",
      "('Epoch: 1839/2000', 'Iteration: 9200', 'Validation loss: 0.469680', 'Validation acc: 0.928333')\n",
      "('Epoch: 1840/2000', 'Iteration: 9205', 'Train loss: 0.005115', 'Train acc: 1.000000')\n",
      "('Epoch: 1841/2000', 'Iteration: 9210', 'Train loss: 0.007773', 'Train acc: 0.998333')\n",
      "('Epoch: 1841/2000', 'Iteration: 9210', 'Validation loss: 0.465715', 'Validation acc: 0.931667')\n",
      "('Epoch: 1842/2000', 'Iteration: 9215', 'Train loss: 0.008076', 'Train acc: 0.998333')\n",
      "('Epoch: 1843/2000', 'Iteration: 9220', 'Train loss: 0.006115', 'Train acc: 1.000000')\n",
      "('Epoch: 1843/2000', 'Iteration: 9220', 'Validation loss: 0.479564', 'Validation acc: 0.930000')\n",
      "('Epoch: 1844/2000', 'Iteration: 9225', 'Train loss: 0.011648', 'Train acc: 0.998333')\n",
      "('Epoch: 1845/2000', 'Iteration: 9230', 'Train loss: 0.010073', 'Train acc: 0.996667')\n",
      "('Epoch: 1845/2000', 'Iteration: 9230', 'Validation loss: 0.462966', 'Validation acc: 0.930000')\n",
      "('Epoch: 1846/2000', 'Iteration: 9235', 'Train loss: 0.007343', 'Train acc: 0.996667')\n",
      "('Epoch: 1847/2000', 'Iteration: 9240', 'Train loss: 0.013876', 'Train acc: 0.995000')\n",
      "('Epoch: 1847/2000', 'Iteration: 9240', 'Validation loss: 0.468509', 'Validation acc: 0.930000')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 1848/2000', 'Iteration: 9245', 'Train loss: 0.008939', 'Train acc: 0.998333')\n",
      "('Epoch: 1849/2000', 'Iteration: 9250', 'Train loss: 0.005994', 'Train acc: 1.000000')\n",
      "('Epoch: 1849/2000', 'Iteration: 9250', 'Validation loss: 0.467573', 'Validation acc: 0.931667')\n",
      "('Epoch: 1850/2000', 'Iteration: 9255', 'Train loss: 0.005923', 'Train acc: 1.000000')\n",
      "('Epoch: 1851/2000', 'Iteration: 9260', 'Train loss: 0.009547', 'Train acc: 0.998333')\n",
      "('Epoch: 1851/2000', 'Iteration: 9260', 'Validation loss: 0.464413', 'Validation acc: 0.923333')\n",
      "('Epoch: 1852/2000', 'Iteration: 9265', 'Train loss: 0.006365', 'Train acc: 1.000000')\n",
      "('Epoch: 1853/2000', 'Iteration: 9270', 'Train loss: 0.005868', 'Train acc: 1.000000')\n",
      "('Epoch: 1853/2000', 'Iteration: 9270', 'Validation loss: 0.460498', 'Validation acc: 0.933333')\n",
      "('Epoch: 1854/2000', 'Iteration: 9275', 'Train loss: 0.012259', 'Train acc: 0.995000')\n",
      "('Epoch: 1855/2000', 'Iteration: 9280', 'Train loss: 0.009033', 'Train acc: 0.996667')\n",
      "('Epoch: 1855/2000', 'Iteration: 9280', 'Validation loss: 0.460870', 'Validation acc: 0.928333')\n",
      "('Epoch: 1856/2000', 'Iteration: 9285', 'Train loss: 0.004420', 'Train acc: 1.000000')\n",
      "('Epoch: 1857/2000', 'Iteration: 9290', 'Train loss: 0.007528', 'Train acc: 1.000000')\n",
      "('Epoch: 1857/2000', 'Iteration: 9290', 'Validation loss: 0.468690', 'Validation acc: 0.930000')\n",
      "('Epoch: 1858/2000', 'Iteration: 9295', 'Train loss: 0.007214', 'Train acc: 0.998333')\n",
      "('Epoch: 1859/2000', 'Iteration: 9300', 'Train loss: 0.007486', 'Train acc: 0.998333')\n",
      "('Epoch: 1859/2000', 'Iteration: 9300', 'Validation loss: 0.458114', 'Validation acc: 0.926667')\n",
      "('Epoch: 1860/2000', 'Iteration: 9305', 'Train loss: 0.007589', 'Train acc: 0.998333')\n",
      "('Epoch: 1861/2000', 'Iteration: 9310', 'Train loss: 0.006643', 'Train acc: 0.998333')\n",
      "('Epoch: 1861/2000', 'Iteration: 9310', 'Validation loss: 0.460849', 'Validation acc: 0.928333')\n",
      "('Epoch: 1862/2000', 'Iteration: 9315', 'Train loss: 0.011006', 'Train acc: 0.996667')\n",
      "('Epoch: 1863/2000', 'Iteration: 9320', 'Train loss: 0.006468', 'Train acc: 0.998333')\n",
      "('Epoch: 1863/2000', 'Iteration: 9320', 'Validation loss: 0.459433', 'Validation acc: 0.926667')\n",
      "('Epoch: 1864/2000', 'Iteration: 9325', 'Train loss: 0.008434', 'Train acc: 0.998333')\n",
      "('Epoch: 1865/2000', 'Iteration: 9330', 'Train loss: 0.004199', 'Train acc: 1.000000')\n",
      "('Epoch: 1865/2000', 'Iteration: 9330', 'Validation loss: 0.466932', 'Validation acc: 0.930000')\n",
      "('Epoch: 1866/2000', 'Iteration: 9335', 'Train loss: 0.005388', 'Train acc: 1.000000')\n",
      "('Epoch: 1867/2000', 'Iteration: 9340', 'Train loss: 0.013716', 'Train acc: 0.995000')\n",
      "('Epoch: 1867/2000', 'Iteration: 9340', 'Validation loss: 0.467340', 'Validation acc: 0.926667')\n",
      "('Epoch: 1868/2000', 'Iteration: 9345', 'Train loss: 0.007196', 'Train acc: 0.998333')\n",
      "('Epoch: 1869/2000', 'Iteration: 9350', 'Train loss: 0.006801', 'Train acc: 0.998333')\n",
      "('Epoch: 1869/2000', 'Iteration: 9350', 'Validation loss: 0.471328', 'Validation acc: 0.926667')\n",
      "('Epoch: 1870/2000', 'Iteration: 9355', 'Train loss: 0.009379', 'Train acc: 0.998333')\n",
      "('Epoch: 1871/2000', 'Iteration: 9360', 'Train loss: 0.005552', 'Train acc: 1.000000')\n",
      "('Epoch: 1871/2000', 'Iteration: 9360', 'Validation loss: 0.464673', 'Validation acc: 0.926667')\n",
      "('Epoch: 1872/2000', 'Iteration: 9365', 'Train loss: 0.006052', 'Train acc: 1.000000')\n",
      "('Epoch: 1873/2000', 'Iteration: 9370', 'Train loss: 0.007070', 'Train acc: 1.000000')\n",
      "('Epoch: 1873/2000', 'Iteration: 9370', 'Validation loss: 0.462093', 'Validation acc: 0.930000')\n",
      "('Epoch: 1874/2000', 'Iteration: 9375', 'Train loss: 0.007645', 'Train acc: 0.998333')\n",
      "('Epoch: 1875/2000', 'Iteration: 9380', 'Train loss: 0.009310', 'Train acc: 0.996667')\n",
      "('Epoch: 1875/2000', 'Iteration: 9380', 'Validation loss: 0.466022', 'Validation acc: 0.930000')\n",
      "('Epoch: 1876/2000', 'Iteration: 9385', 'Train loss: 0.005162', 'Train acc: 1.000000')\n",
      "('Epoch: 1877/2000', 'Iteration: 9390', 'Train loss: 0.009027', 'Train acc: 1.000000')\n",
      "('Epoch: 1877/2000', 'Iteration: 9390', 'Validation loss: 0.465808', 'Validation acc: 0.930000')\n",
      "('Epoch: 1878/2000', 'Iteration: 9395', 'Train loss: 0.013982', 'Train acc: 0.996667')\n",
      "('Epoch: 1879/2000', 'Iteration: 9400', 'Train loss: 0.004386', 'Train acc: 1.000000')\n",
      "('Epoch: 1879/2000', 'Iteration: 9400', 'Validation loss: 0.473870', 'Validation acc: 0.926667')\n",
      "('Epoch: 1880/2000', 'Iteration: 9405', 'Train loss: 0.004387', 'Train acc: 1.000000')\n",
      "('Epoch: 1881/2000', 'Iteration: 9410', 'Train loss: 0.007746', 'Train acc: 0.998333')\n",
      "('Epoch: 1881/2000', 'Iteration: 9410', 'Validation loss: 0.486447', 'Validation acc: 0.926667')\n",
      "('Epoch: 1882/2000', 'Iteration: 9415', 'Train loss: 0.006440', 'Train acc: 0.998333')\n",
      "('Epoch: 1883/2000', 'Iteration: 9420', 'Train loss: 0.005118', 'Train acc: 1.000000')\n",
      "('Epoch: 1883/2000', 'Iteration: 9420', 'Validation loss: 0.477015', 'Validation acc: 0.933333')\n",
      "('Epoch: 1884/2000', 'Iteration: 9425', 'Train loss: 0.005834', 'Train acc: 0.998333')\n",
      "('Epoch: 1885/2000', 'Iteration: 9430', 'Train loss: 0.008754', 'Train acc: 0.998333')\n",
      "('Epoch: 1885/2000', 'Iteration: 9430', 'Validation loss: 0.471897', 'Validation acc: 0.930000')\n",
      "('Epoch: 1886/2000', 'Iteration: 9435', 'Train loss: 0.007706', 'Train acc: 0.998333')\n",
      "('Epoch: 1887/2000', 'Iteration: 9440', 'Train loss: 0.004293', 'Train acc: 1.000000')\n",
      "('Epoch: 1887/2000', 'Iteration: 9440', 'Validation loss: 0.476075', 'Validation acc: 0.928333')\n",
      "('Epoch: 1888/2000', 'Iteration: 9445', 'Train loss: 0.008159', 'Train acc: 0.998333')\n",
      "('Epoch: 1889/2000', 'Iteration: 9450', 'Train loss: 0.008227', 'Train acc: 1.000000')\n",
      "('Epoch: 1889/2000', 'Iteration: 9450', 'Validation loss: 0.477968', 'Validation acc: 0.931667')\n",
      "('Epoch: 1890/2000', 'Iteration: 9455', 'Train loss: 0.005776', 'Train acc: 0.998333')\n",
      "('Epoch: 1891/2000', 'Iteration: 9460', 'Train loss: 0.012166', 'Train acc: 0.993333')\n",
      "('Epoch: 1891/2000', 'Iteration: 9460', 'Validation loss: 0.476403', 'Validation acc: 0.930000')\n",
      "('Epoch: 1892/2000', 'Iteration: 9465', 'Train loss: 0.005128', 'Train acc: 1.000000')\n",
      "('Epoch: 1893/2000', 'Iteration: 9470', 'Train loss: 0.007240', 'Train acc: 0.998333')\n",
      "('Epoch: 1893/2000', 'Iteration: 9470', 'Validation loss: 0.477325', 'Validation acc: 0.931667')\n",
      "('Epoch: 1894/2000', 'Iteration: 9475', 'Train loss: 0.014767', 'Train acc: 0.993333')\n",
      "('Epoch: 1895/2000', 'Iteration: 9480', 'Train loss: 0.009569', 'Train acc: 0.998333')\n",
      "('Epoch: 1895/2000', 'Iteration: 9480', 'Validation loss: 0.475331', 'Validation acc: 0.928333')\n",
      "('Epoch: 1896/2000', 'Iteration: 9485', 'Train loss: 0.015343', 'Train acc: 0.993333')\n",
      "('Epoch: 1897/2000', 'Iteration: 9490', 'Train loss: 0.006929', 'Train acc: 0.998333')\n",
      "('Epoch: 1897/2000', 'Iteration: 9490', 'Validation loss: 0.479354', 'Validation acc: 0.930000')\n",
      "('Epoch: 1898/2000', 'Iteration: 9495', 'Train loss: 0.006476', 'Train acc: 0.996667')\n",
      "('Epoch: 1899/2000', 'Iteration: 9500', 'Train loss: 0.006772', 'Train acc: 1.000000')\n",
      "('Epoch: 1899/2000', 'Iteration: 9500', 'Validation loss: 0.474815', 'Validation acc: 0.926667')\n",
      "('Epoch: 1900/2000', 'Iteration: 9505', 'Train loss: 0.006534', 'Train acc: 1.000000')\n",
      "('Epoch: 1901/2000', 'Iteration: 9510', 'Train loss: 0.007131', 'Train acc: 0.996667')\n",
      "('Epoch: 1901/2000', 'Iteration: 9510', 'Validation loss: 0.477666', 'Validation acc: 0.925000')\n",
      "('Epoch: 1902/2000', 'Iteration: 9515', 'Train loss: 0.008614', 'Train acc: 0.998333')\n",
      "('Epoch: 1903/2000', 'Iteration: 9520', 'Train loss: 0.010635', 'Train acc: 0.998333')\n",
      "('Epoch: 1903/2000', 'Iteration: 9520', 'Validation loss: 0.473899', 'Validation acc: 0.926667')\n",
      "('Epoch: 1904/2000', 'Iteration: 9525', 'Train loss: 0.007567', 'Train acc: 0.998333')\n",
      "('Epoch: 1905/2000', 'Iteration: 9530', 'Train loss: 0.013050', 'Train acc: 0.996667')\n",
      "('Epoch: 1905/2000', 'Iteration: 9530', 'Validation loss: 0.479190', 'Validation acc: 0.930000')\n",
      "('Epoch: 1906/2000', 'Iteration: 9535', 'Train loss: 0.009712', 'Train acc: 0.998333')\n",
      "('Epoch: 1907/2000', 'Iteration: 9540', 'Train loss: 0.007545', 'Train acc: 0.998333')\n",
      "('Epoch: 1907/2000', 'Iteration: 9540', 'Validation loss: 0.485262', 'Validation acc: 0.926667')\n",
      "('Epoch: 1908/2000', 'Iteration: 9545', 'Train loss: 0.009194', 'Train acc: 0.998333')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 1909/2000', 'Iteration: 9550', 'Train loss: 0.011889', 'Train acc: 0.995000')\n",
      "('Epoch: 1909/2000', 'Iteration: 9550', 'Validation loss: 0.473021', 'Validation acc: 0.931667')\n",
      "('Epoch: 1910/2000', 'Iteration: 9555', 'Train loss: 0.006483', 'Train acc: 1.000000')\n",
      "('Epoch: 1911/2000', 'Iteration: 9560', 'Train loss: 0.010871', 'Train acc: 0.995000')\n",
      "('Epoch: 1911/2000', 'Iteration: 9560', 'Validation loss: 0.472782', 'Validation acc: 0.928333')\n",
      "('Epoch: 1912/2000', 'Iteration: 9565', 'Train loss: 0.009105', 'Train acc: 0.998333')\n",
      "('Epoch: 1913/2000', 'Iteration: 9570', 'Train loss: 0.007164', 'Train acc: 1.000000')\n",
      "('Epoch: 1913/2000', 'Iteration: 9570', 'Validation loss: 0.479448', 'Validation acc: 0.933333')\n",
      "('Epoch: 1914/2000', 'Iteration: 9575', 'Train loss: 0.008619', 'Train acc: 0.996667')\n",
      "('Epoch: 1915/2000', 'Iteration: 9580', 'Train loss: 0.004814', 'Train acc: 1.000000')\n",
      "('Epoch: 1915/2000', 'Iteration: 9580', 'Validation loss: 0.481084', 'Validation acc: 0.930000')\n",
      "('Epoch: 1916/2000', 'Iteration: 9585', 'Train loss: 0.010082', 'Train acc: 0.995000')\n",
      "('Epoch: 1917/2000', 'Iteration: 9590', 'Train loss: 0.010821', 'Train acc: 0.996667')\n",
      "('Epoch: 1917/2000', 'Iteration: 9590', 'Validation loss: 0.470199', 'Validation acc: 0.928333')\n",
      "('Epoch: 1918/2000', 'Iteration: 9595', 'Train loss: 0.007593', 'Train acc: 0.998333')\n",
      "('Epoch: 1919/2000', 'Iteration: 9600', 'Train loss: 0.009376', 'Train acc: 0.998333')\n",
      "('Epoch: 1919/2000', 'Iteration: 9600', 'Validation loss: 0.472401', 'Validation acc: 0.928333')\n",
      "('Epoch: 1920/2000', 'Iteration: 9605', 'Train loss: 0.005787', 'Train acc: 1.000000')\n",
      "('Epoch: 1921/2000', 'Iteration: 9610', 'Train loss: 0.008049', 'Train acc: 0.998333')\n",
      "('Epoch: 1921/2000', 'Iteration: 9610', 'Validation loss: 0.474479', 'Validation acc: 0.930000')\n",
      "('Epoch: 1922/2000', 'Iteration: 9615', 'Train loss: 0.006673', 'Train acc: 0.998333')\n",
      "('Epoch: 1923/2000', 'Iteration: 9620', 'Train loss: 0.007686', 'Train acc: 0.998333')\n",
      "('Epoch: 1923/2000', 'Iteration: 9620', 'Validation loss: 0.477718', 'Validation acc: 0.931667')\n",
      "('Epoch: 1924/2000', 'Iteration: 9625', 'Train loss: 0.010647', 'Train acc: 0.998333')\n",
      "('Epoch: 1925/2000', 'Iteration: 9630', 'Train loss: 0.006971', 'Train acc: 1.000000')\n",
      "('Epoch: 1925/2000', 'Iteration: 9630', 'Validation loss: 0.476183', 'Validation acc: 0.925000')\n",
      "('Epoch: 1926/2000', 'Iteration: 9635', 'Train loss: 0.006445', 'Train acc: 0.998333')\n",
      "('Epoch: 1927/2000', 'Iteration: 9640', 'Train loss: 0.006560', 'Train acc: 0.998333')\n",
      "('Epoch: 1927/2000', 'Iteration: 9640', 'Validation loss: 0.482058', 'Validation acc: 0.930000')\n",
      "('Epoch: 1928/2000', 'Iteration: 9645', 'Train loss: 0.004602', 'Train acc: 1.000000')\n",
      "('Epoch: 1929/2000', 'Iteration: 9650', 'Train loss: 0.007176', 'Train acc: 1.000000')\n",
      "('Epoch: 1929/2000', 'Iteration: 9650', 'Validation loss: 0.484835', 'Validation acc: 0.928333')\n",
      "('Epoch: 1930/2000', 'Iteration: 9655', 'Train loss: 0.006862', 'Train acc: 1.000000')\n",
      "('Epoch: 1931/2000', 'Iteration: 9660', 'Train loss: 0.005314', 'Train acc: 1.000000')\n",
      "('Epoch: 1931/2000', 'Iteration: 9660', 'Validation loss: 0.480747', 'Validation acc: 0.928333')\n",
      "('Epoch: 1932/2000', 'Iteration: 9665', 'Train loss: 0.008666', 'Train acc: 0.998333')\n",
      "('Epoch: 1933/2000', 'Iteration: 9670', 'Train loss: 0.008263', 'Train acc: 0.998333')\n",
      "('Epoch: 1933/2000', 'Iteration: 9670', 'Validation loss: 0.481193', 'Validation acc: 0.930000')\n",
      "('Epoch: 1934/2000', 'Iteration: 9675', 'Train loss: 0.006377', 'Train acc: 0.998333')\n",
      "('Epoch: 1935/2000', 'Iteration: 9680', 'Train loss: 0.009147', 'Train acc: 0.996667')\n",
      "('Epoch: 1935/2000', 'Iteration: 9680', 'Validation loss: 0.482878', 'Validation acc: 0.930000')\n",
      "('Epoch: 1936/2000', 'Iteration: 9685', 'Train loss: 0.009903', 'Train acc: 0.996667')\n",
      "('Epoch: 1937/2000', 'Iteration: 9690', 'Train loss: 0.004354', 'Train acc: 0.998333')\n",
      "('Epoch: 1937/2000', 'Iteration: 9690', 'Validation loss: 0.472996', 'Validation acc: 0.931667')\n",
      "('Epoch: 1938/2000', 'Iteration: 9695', 'Train loss: 0.010412', 'Train acc: 0.998333')\n",
      "('Epoch: 1939/2000', 'Iteration: 9700', 'Train loss: 0.004577', 'Train acc: 1.000000')\n",
      "('Epoch: 1939/2000', 'Iteration: 9700', 'Validation loss: 0.475077', 'Validation acc: 0.928333')\n",
      "('Epoch: 1940/2000', 'Iteration: 9705', 'Train loss: 0.005623', 'Train acc: 1.000000')\n",
      "('Epoch: 1941/2000', 'Iteration: 9710', 'Train loss: 0.007785', 'Train acc: 0.998333')\n",
      "('Epoch: 1941/2000', 'Iteration: 9710', 'Validation loss: 0.473581', 'Validation acc: 0.930000')\n",
      "('Epoch: 1942/2000', 'Iteration: 9715', 'Train loss: 0.006123', 'Train acc: 0.998333')\n",
      "('Epoch: 1943/2000', 'Iteration: 9720', 'Train loss: 0.007548', 'Train acc: 0.998333')\n",
      "('Epoch: 1943/2000', 'Iteration: 9720', 'Validation loss: 0.488558', 'Validation acc: 0.928333')\n",
      "('Epoch: 1944/2000', 'Iteration: 9725', 'Train loss: 0.006869', 'Train acc: 0.998333')\n",
      "('Epoch: 1945/2000', 'Iteration: 9730', 'Train loss: 0.006039', 'Train acc: 1.000000')\n",
      "('Epoch: 1945/2000', 'Iteration: 9730', 'Validation loss: 0.478279', 'Validation acc: 0.926667')\n",
      "('Epoch: 1946/2000', 'Iteration: 9735', 'Train loss: 0.008191', 'Train acc: 0.998333')\n",
      "('Epoch: 1947/2000', 'Iteration: 9740', 'Train loss: 0.006353', 'Train acc: 0.998333')\n",
      "('Epoch: 1947/2000', 'Iteration: 9740', 'Validation loss: 0.479682', 'Validation acc: 0.926667')\n",
      "('Epoch: 1948/2000', 'Iteration: 9745', 'Train loss: 0.006983', 'Train acc: 0.998333')\n",
      "('Epoch: 1949/2000', 'Iteration: 9750', 'Train loss: 0.012256', 'Train acc: 0.996667')\n",
      "('Epoch: 1949/2000', 'Iteration: 9750', 'Validation loss: 0.485987', 'Validation acc: 0.926667')\n",
      "('Epoch: 1950/2000', 'Iteration: 9755', 'Train loss: 0.009925', 'Train acc: 0.998333')\n",
      "('Epoch: 1951/2000', 'Iteration: 9760', 'Train loss: 0.004896', 'Train acc: 0.998333')\n",
      "('Epoch: 1951/2000', 'Iteration: 9760', 'Validation loss: 0.487626', 'Validation acc: 0.923333')\n",
      "('Epoch: 1952/2000', 'Iteration: 9765', 'Train loss: 0.006925', 'Train acc: 0.998333')\n",
      "('Epoch: 1953/2000', 'Iteration: 9770', 'Train loss: 0.010344', 'Train acc: 0.996667')\n",
      "('Epoch: 1953/2000', 'Iteration: 9770', 'Validation loss: 0.491878', 'Validation acc: 0.926667')\n",
      "('Epoch: 1954/2000', 'Iteration: 9775', 'Train loss: 0.007818', 'Train acc: 0.998333')\n",
      "('Epoch: 1955/2000', 'Iteration: 9780', 'Train loss: 0.005804', 'Train acc: 1.000000')\n",
      "('Epoch: 1955/2000', 'Iteration: 9780', 'Validation loss: 0.492280', 'Validation acc: 0.928333')\n",
      "('Epoch: 1956/2000', 'Iteration: 9785', 'Train loss: 0.007448', 'Train acc: 0.996667')\n",
      "('Epoch: 1957/2000', 'Iteration: 9790', 'Train loss: 0.004776', 'Train acc: 1.000000')\n",
      "('Epoch: 1957/2000', 'Iteration: 9790', 'Validation loss: 0.485368', 'Validation acc: 0.925000')\n",
      "('Epoch: 1958/2000', 'Iteration: 9795', 'Train loss: 0.006019', 'Train acc: 1.000000')\n",
      "('Epoch: 1959/2000', 'Iteration: 9800', 'Train loss: 0.006121', 'Train acc: 1.000000')\n",
      "('Epoch: 1959/2000', 'Iteration: 9800', 'Validation loss: 0.478375', 'Validation acc: 0.928333')\n",
      "('Epoch: 1960/2000', 'Iteration: 9805', 'Train loss: 0.004405', 'Train acc: 1.000000')\n",
      "('Epoch: 1961/2000', 'Iteration: 9810', 'Train loss: 0.005462', 'Train acc: 1.000000')\n",
      "('Epoch: 1961/2000', 'Iteration: 9810', 'Validation loss: 0.488594', 'Validation acc: 0.926667')\n",
      "('Epoch: 1962/2000', 'Iteration: 9815', 'Train loss: 0.004901', 'Train acc: 1.000000')\n",
      "('Epoch: 1963/2000', 'Iteration: 9820', 'Train loss: 0.005345', 'Train acc: 1.000000')\n",
      "('Epoch: 1963/2000', 'Iteration: 9820', 'Validation loss: 0.492155', 'Validation acc: 0.923333')\n",
      "('Epoch: 1964/2000', 'Iteration: 9825', 'Train loss: 0.007888', 'Train acc: 0.998333')\n",
      "('Epoch: 1965/2000', 'Iteration: 9830', 'Train loss: 0.007448', 'Train acc: 0.996667')\n",
      "('Epoch: 1965/2000', 'Iteration: 9830', 'Validation loss: 0.471204', 'Validation acc: 0.926667')\n",
      "('Epoch: 1966/2000', 'Iteration: 9835', 'Train loss: 0.006562', 'Train acc: 1.000000')\n",
      "('Epoch: 1967/2000', 'Iteration: 9840', 'Train loss: 0.006362', 'Train acc: 1.000000')\n",
      "('Epoch: 1967/2000', 'Iteration: 9840', 'Validation loss: 0.471610', 'Validation acc: 0.928333')\n",
      "('Epoch: 1968/2000', 'Iteration: 9845', 'Train loss: 0.009204', 'Train acc: 0.998333')\n",
      "('Epoch: 1969/2000', 'Iteration: 9850', 'Train loss: 0.007743', 'Train acc: 0.998333')\n",
      "('Epoch: 1969/2000', 'Iteration: 9850', 'Validation loss: 0.481835', 'Validation acc: 0.930000')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 1970/2000', 'Iteration: 9855', 'Train loss: 0.010738', 'Train acc: 0.996667')\n",
      "('Epoch: 1971/2000', 'Iteration: 9860', 'Train loss: 0.006808', 'Train acc: 1.000000')\n",
      "('Epoch: 1971/2000', 'Iteration: 9860', 'Validation loss: 0.475649', 'Validation acc: 0.933333')\n",
      "('Epoch: 1972/2000', 'Iteration: 9865', 'Train loss: 0.005512', 'Train acc: 0.998333')\n",
      "('Epoch: 1973/2000', 'Iteration: 9870', 'Train loss: 0.007143', 'Train acc: 0.996667')\n",
      "('Epoch: 1973/2000', 'Iteration: 9870', 'Validation loss: 0.490755', 'Validation acc: 0.926667')\n",
      "('Epoch: 1974/2000', 'Iteration: 9875', 'Train loss: 0.010707', 'Train acc: 0.996667')\n",
      "('Epoch: 1975/2000', 'Iteration: 9880', 'Train loss: 0.010894', 'Train acc: 0.996667')\n",
      "('Epoch: 1975/2000', 'Iteration: 9880', 'Validation loss: 0.474101', 'Validation acc: 0.930000')\n",
      "('Epoch: 1976/2000', 'Iteration: 9885', 'Train loss: 0.005401', 'Train acc: 0.998333')\n",
      "('Epoch: 1977/2000', 'Iteration: 9890', 'Train loss: 0.007529', 'Train acc: 1.000000')\n",
      "('Epoch: 1977/2000', 'Iteration: 9890', 'Validation loss: 0.480929', 'Validation acc: 0.928333')\n",
      "('Epoch: 1978/2000', 'Iteration: 9895', 'Train loss: 0.005127', 'Train acc: 1.000000')\n",
      "('Epoch: 1979/2000', 'Iteration: 9900', 'Train loss: 0.003809', 'Train acc: 1.000000')\n",
      "('Epoch: 1979/2000', 'Iteration: 9900', 'Validation loss: 0.479814', 'Validation acc: 0.933333')\n",
      "('Epoch: 1980/2000', 'Iteration: 9905', 'Train loss: 0.008657', 'Train acc: 0.996667')\n",
      "('Epoch: 1981/2000', 'Iteration: 9910', 'Train loss: 0.004461', 'Train acc: 1.000000')\n",
      "('Epoch: 1981/2000', 'Iteration: 9910', 'Validation loss: 0.485458', 'Validation acc: 0.931667')\n",
      "('Epoch: 1982/2000', 'Iteration: 9915', 'Train loss: 0.008311', 'Train acc: 0.998333')\n",
      "('Epoch: 1983/2000', 'Iteration: 9920', 'Train loss: 0.008507', 'Train acc: 0.996667')\n",
      "('Epoch: 1983/2000', 'Iteration: 9920', 'Validation loss: 0.477120', 'Validation acc: 0.930000')\n",
      "('Epoch: 1984/2000', 'Iteration: 9925', 'Train loss: 0.008013', 'Train acc: 0.998333')\n",
      "('Epoch: 1985/2000', 'Iteration: 9930', 'Train loss: 0.008796', 'Train acc: 0.998333')\n",
      "('Epoch: 1985/2000', 'Iteration: 9930', 'Validation loss: 0.479553', 'Validation acc: 0.930000')\n",
      "('Epoch: 1986/2000', 'Iteration: 9935', 'Train loss: 0.006035', 'Train acc: 0.996667')\n",
      "('Epoch: 1987/2000', 'Iteration: 9940', 'Train loss: 0.010735', 'Train acc: 0.998333')\n",
      "('Epoch: 1987/2000', 'Iteration: 9940', 'Validation loss: 0.488355', 'Validation acc: 0.930000')\n",
      "('Epoch: 1988/2000', 'Iteration: 9945', 'Train loss: 0.010086', 'Train acc: 0.996667')\n",
      "('Epoch: 1989/2000', 'Iteration: 9950', 'Train loss: 0.005525', 'Train acc: 1.000000')\n",
      "('Epoch: 1989/2000', 'Iteration: 9950', 'Validation loss: 0.484004', 'Validation acc: 0.931667')\n",
      "('Epoch: 1990/2000', 'Iteration: 9955', 'Train loss: 0.012422', 'Train acc: 0.998333')\n",
      "('Epoch: 1991/2000', 'Iteration: 9960', 'Train loss: 0.006795', 'Train acc: 0.998333')\n",
      "('Epoch: 1991/2000', 'Iteration: 9960', 'Validation loss: 0.486459', 'Validation acc: 0.933333')\n",
      "('Epoch: 1992/2000', 'Iteration: 9965', 'Train loss: 0.007055', 'Train acc: 0.998333')\n",
      "('Epoch: 1993/2000', 'Iteration: 9970', 'Train loss: 0.005354', 'Train acc: 1.000000')\n",
      "('Epoch: 1993/2000', 'Iteration: 9970', 'Validation loss: 0.486291', 'Validation acc: 0.933333')\n",
      "('Epoch: 1994/2000', 'Iteration: 9975', 'Train loss: 0.005926', 'Train acc: 1.000000')\n",
      "('Epoch: 1995/2000', 'Iteration: 9980', 'Train loss: 0.005864', 'Train acc: 1.000000')\n",
      "('Epoch: 1995/2000', 'Iteration: 9980', 'Validation loss: 0.487987', 'Validation acc: 0.931667')\n",
      "('Epoch: 1996/2000', 'Iteration: 9985', 'Train loss: 0.011100', 'Train acc: 0.998333')\n",
      "('Epoch: 1997/2000', 'Iteration: 9990', 'Train loss: 0.007570', 'Train acc: 0.998333')\n",
      "('Epoch: 1997/2000', 'Iteration: 9990', 'Validation loss: 0.482478', 'Validation acc: 0.930000')\n",
      "('Epoch: 1998/2000', 'Iteration: 9995', 'Train loss: 0.005449', 'Train acc: 1.000000')\n",
      "('Epoch: 1999/2000', 'Iteration: 10000', 'Train loss: 0.011616', 'Train acc: 0.996667')\n",
      "('Epoch: 1999/2000', 'Iteration: 10000', 'Validation loss: 0.478122', 'Validation acc: 0.930000')\n"
     ]
    }
   ],
   "source": [
    "validation_acc = []\n",
    "validation_loss = []\n",
    "\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "   \n",
    "    # Loop over epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Loop over batches\n",
    "        for x,y in get_batches(X_tr, y_tr, batch_size):\n",
    "            \n",
    "            # Feed dictionary\n",
    "            feed = {inputs_ : x, labels_ : y, keep_prob_ : 0.5, learning_rate_ : learning_rate}\n",
    "            \n",
    "            # Loss\n",
    "            loss, _ , acc = sess.run([cost, optimizer, accuracy], feed_dict = feed)\n",
    "            train_acc.append(acc)\n",
    "            train_loss.append(loss)\n",
    "            \n",
    "            # Print at each 5 iters\n",
    "            if (iteration % 5 == 0):\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {:d}\".format(iteration),\n",
    "                      \"Train loss: {:6f}\".format(loss),\n",
    "                      \"Train acc: {:.6f}\".format(acc))\n",
    "            \n",
    "            # Compute validation loss at every 10 iterations\n",
    "            if (iteration%10 == 0):                \n",
    "                val_acc_ = []\n",
    "                val_loss_ = []\n",
    "                \n",
    "                for x_v, y_v in get_batches(X_vld, y_vld, batch_size):\n",
    "                    # Feed\n",
    "                    feed = {inputs_ : x_v, labels_ : y_v, keep_prob_ : 1.0}  \n",
    "                    \n",
    "                    # Loss\n",
    "                    loss_v, acc_v = sess.run([cost, accuracy], feed_dict = feed)                    \n",
    "                    val_acc_.append(acc_v)\n",
    "                    val_loss_.append(loss_v)\n",
    "                \n",
    "                # Print info\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {:d}\".format(iteration),\n",
    "                      \"Validation loss: {:6f}\".format(np.mean(val_loss_)),\n",
    "                      \"Validation acc: {:.6f}\".format(np.mean(val_acc_)))\n",
    "                \n",
    "                # Store\n",
    "                validation_acc.append(np.mean(val_acc_))\n",
    "                validation_loss.append(np.mean(val_loss_))\n",
    "            \n",
    "            # Iterate \n",
    "            iteration += 1\n",
    "    \n",
    "    saver.save(sess,\"checkpoints-cnn/har.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAF3CAYAAABKeVdaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbdbb9952d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training and test loss\n",
    "t = np.arange(iteration-1)\n",
    "\n",
    "plt.figure(figsize = (6,6))\n",
    "plt.plot(t, np.array(train_loss), 'r-', t[t % 10 == 0], np.array(validation_loss), 'b*')\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAF3CAYAAABKeVdaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFBRERCRFQUFERFIUFEREJEVBQUREUv4/Ousr78QX8N0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbdbac22910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Accuracies\n",
    "plt.figure(figsize = (6,6))\n",
    "\n",
    "plt.plot(t, np.array(train_acc), 'r-', t[t % 10 == 0], validation_acc, 'b*')\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"Accuray\")\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints-cnn/har.ckpt\n",
      "Test accuracy: 0.716667\n",
      "(array([ 1.  ,  0.95,  1.  ,  0.93,  0.98,  0.91,  1.  ,  0.4 ,  0.48,\n",
      "        1.  ,  0.23,  1.  ,  0.64,  1.  ,  1.  ,  0.5 ]), array([ 1.  ,  1.  ,  0.93,  1.  ,  0.95,  0.98,  0.94,  0.88,  0.78,\n",
      "        0.72,  0.09,  0.03,  0.94,  0.94,  1.  ,  1.  ]), array([ 1.  ,  0.98,  0.97,  0.97,  0.96,  0.94,  0.97,  0.55,  0.6 ,\n",
      "        0.84,  0.13,  0.05,  0.76,  0.97,  1.  ,  0.67]), array([20, 21, 15, 14, 55, 41, 16, 32, 36, 36, 70, 74, 72, 34, 32, 32]))\n",
      "Accuracy: 0.716666666667\n",
      "F1 score: 0.716666666667\n",
      "Recall: 0.716666666667\n",
      "Precision: 0.716666666667\n",
      "\n",
      " clasification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        20\n",
      "          1       0.95      1.00      0.98        21\n",
      "          2       1.00      0.93      0.97        15\n",
      "          3       0.93      1.00      0.97        14\n",
      "          4       0.98      0.95      0.96        55\n",
      "          5       0.91      0.98      0.94        41\n",
      "          6       1.00      0.94      0.97        16\n",
      "          7       0.40      0.88      0.55        32\n",
      "          8       0.48      0.78      0.60        36\n",
      "          9       1.00      0.72      0.84        36\n",
      "         10       0.23      0.09      0.13        70\n",
      "         11       1.00      0.03      0.05        74\n",
      "         12       0.64      0.94      0.76        72\n",
      "         13       1.00      0.94      0.97        34\n",
      "         14       1.00      1.00      1.00        32\n",
      "         15       0.50      1.00      0.67        32\n",
      "\n",
      "avg / total       0.77      0.72      0.66       600\n",
      "\n",
      "\n",
      " confussion matrix:\n",
      "[[20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 21  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1 14  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 14  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1 52  2  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1 40  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 15  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 28  0  0  0  0  0  0  0  4]\n",
      " [ 0  0  0  0  0  0  0  0 28  0  0  0  0  0  0  8]\n",
      " [ 0  0  0  0  0  0  0  0  0 26  0  0  4  0  0  6]\n",
      " [ 0  0  0  0  0  0  0 42  0  0  6  0 12  0  0 10]\n",
      " [ 0  0  0  0  0  0  0  0 30  0 20  2 22  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0 68  0  0  4]\n",
      " [ 0  0  0  0  0  2  0  0  0  0  0  0  0 32  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 32  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 32]]\n",
      "Confusion matrix, without normalization\n",
      "[[20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 21  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1 14  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 14  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1 52  2  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1 40  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 15  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 28  0  0  0  0  0  0  0  4]\n",
      " [ 0  0  0  0  0  0  0  0 28  0  0  0  0  0  0  8]\n",
      " [ 0  0  0  0  0  0  0  0  0 26  0  0  4  0  0  6]\n",
      " [ 0  0  0  0  0  0  0 42  0  0  6  0 12  0  0 10]\n",
      " [ 0  0  0  0  0  0  0  0 30  0 20  2 22  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0 68  0  0  4]\n",
      " [ 0  0  0  0  0  2  0  0  0  0  0  0  0 32  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 32  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 32]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYfAlqQmDL0lNGHxJauJ/Acz2XLpusNoKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbdba567950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1AAAALWCAYAAABIhXJAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbdbb910110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_acc = []\n",
    "with graph.as_default():\n",
    "    prediction=tf.argmax(logits,1)\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # Restore\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints-cnn'))\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for x_t, y_t in get_batches(X_test, y_test, batch_size):\n",
    "        feed = {inputs_: x_t,\n",
    "                labels_: y_t,\n",
    "                keep_prob_: 1}\n",
    "        batch_acc, batch_y_pred = sess.run([accuracy, prediction], feed_dict=feed)\n",
    "        y_pred.extend(batch_y_pred)\n",
    "        y_true.extend(np.where(r==1)[0][0] for r in y_t )\n",
    "        \n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.6f}\".format(np.mean(test_acc)))\n",
    "#     print y_true\n",
    "#     print y_pred\n",
    "    sk_class_labels = [i for i in range(NUM_CLASS)]\n",
    "    cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    print precision_recall_fscore_support(y_true, y_pred, average=None, labels=sk_class_labels)\n",
    "    print 'Accuracy:', accuracy_score(y_true, y_pred)\n",
    "    print 'F1 score:', f1_score(y_true, y_pred, average='micro')\n",
    "    print 'Recall:', recall_score(y_true, y_pred, average='micro')\n",
    "    print 'Precision:', precision_score(y_true, y_pred, average='micro')\n",
    "    print '\\n clasification report:\\n', classification_report(y_true,y_pred)\n",
    "    print '\\n confussion matrix:\\n',cnf_matrix\n",
    "    fig, ax = plt.subplots() \n",
    "    fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "    fig_size[0] = 20\n",
    "    fig_size[1] = 10\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, class_label_vn, title='Confusion matrix')\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
